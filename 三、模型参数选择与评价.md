# 模型评价

## 指标&评分

### 判定系数$R^2$

**代码**

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html?highlight=r2_score#sklearn.metrics.r2_score

签名：

```python
sklearn.metrics.r2_score(y_true, y_pred, *, 
                         sample_weight=None, 
                         multioutput='uniform_average')
```

样例：

```python
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
```

参数：

​	y_true：数组，目标变量真实值。

​	y_pred：数组，目标变量预测值。

​	sample_wright：权重。以数组形式给出。

​	multioutput：定义输出分数的类型。默认’uniform_average’ 也即每个样本点权重相同。

返回值：

​	返回计算出的$R^2$。



### PR曲线

对于二分类问题，不同分类器的表现各有千秋。通常来说，一些分类器更擅长正确分类正类（或者说更倾向于将数据分成正类），而另一些会擅长正确分类负类（或者说更倾向于将数据分成负类）。为了综合衡量不同的分类器的优劣，考虑用某种方式同时表现出分类器在正负类的表现。PR曲线便是一种可行的方式。

（为什么不能单纯考虑对某一类的分类效果，或者仅仅考察准确率呢？一个极端的例子是，正负类的样本数量差距很大，而分类器将全部样本一股脑的视为了更多的哪一类，此时的准确率依然很高，但实际上并没有有效分类。）

为了描述PR曲线，需要先介绍一些概念。对于一个二分类问题，有以下四个值：

> TP: Ture Positive 把正的判断为正的数目。
> FN: False Negative 把正的错判为负的数目。
> FP: False Positive 把负的错判为正的数目。
> TN: True Negative 把负的判为负的数目。

进而定义出

>精确率(Precision)：预测为正的样本中正确的比例。P= TP / (TP+FP)
>召回率(Recall)：正样本被预测正确的比例。R= TP / (TP+FN)

一个分类器的输出，实际上应该是样本属于类别的概率，我们根据确定的阈值将其判别为正类或者负类。比如用50%作为阈值，概率大于50%的就认为是正例，小于50%的就是负例。每选定一个阈值，则就会有一个确定的分类结果，从而有确定的精确率P与召回率R。以阈值为自变量，点对(R,P)为因变量画参数函数图，便得到了PR曲线。

![](D:\OneDrive\files\数据分析\常用算法描述图片\PR曲线.png)

一般情况下，将recall设置为横坐标，precision设置为纵坐标。在阈值设置极高的情况下，全部样本都被认为是负类，此时召回率为0，精确率也为0，但随着阈值略微提高，即便仍有大部分样本被认为是负类，但少数被分为正类的样本大概率是正确识别的，所以曲线会迅速上升到(0,1)附近；在阈值设置极低的情况下，全部样本都被认为是正类，此时召回率为1，精确率小于1（更准确的说，会变成正类占总体的比例），为曲线的右下角端点，通常由于样本比例不均衡，可能会非常接近(1,0)。在阈值的降低过程中，正负样本都会越来越多的被判断为正类，因此召回率R会单调增加，而精确率P会震荡。

![](D:\OneDrive\files\数据分析\常用算法描述图片\PR曲线2.png)

当某个分类器PR曲线完全包络另一分类器的PR曲线，那就说明其性能全面超越后者。不过大多数情况下，PR曲线都会交叉，此时考虑用P=R的平衡点来衡量优劣，将这个思路状转化成指标$F1=2/(\frac1P+\frac1R)$，也即准确率和召回率的调和平均数，F1越大分类越好；此外，也可以用曲线的线下面积来衡量。

### ROC曲线与AUC值

ROC的全名叫做Receiver Operating Characteristic（受试者工作特征曲线 ），又称为感受性曲线（sensitivity curve）。得此名的原因在于曲线上各点反映着相同的感受性，它们都是对同一信号刺激的反应，只不过是在几种不同的判定标准下所得的结果而已。其主要分析工具是一个画在二维平面上的曲线——ROC 曲线。ROC曲线以真正例率TPR为纵轴，以假正例率FPR为横轴，在不同的阈值下获得坐标点，并连接各个坐标点，得到ROC曲线。

继续使用前述的TP等概念：

![](D:\OneDrive\files\数据分析\常用算法描述图片\二分类混淆矩阵.png)

定义

> 精确率(Precision)：预测为正的样本中预测为正的比例。P = TP / (TP+FP)
> 召回率(Recall)：实际为正的样本中预测为正的比例。R = TP / (TP+FN)
> 真正率(True Positive Rate)：实际为正的样本中预测为正的比例。TPR = TP / (TP+FN) = R
> 假正率(False Positive Rate)：实际为负的样本中预测为正的比例。FPR = FP / (TN+FP)

与PR曲线思路相仿，ROC曲线是点对 (FPR,TPR) 随着判别阈值的变化曲线。当阈值极大时，一切样本都是负类，此时真假正率都是0；当阈值极小时，一切样本都是正类，此时真假正率都是1。ROC曲线便是从 (0,0) 到 (1,1) 的曲线。

![](D:\OneDrive\files\数据分析\常用算法描述图片\ROC曲线.png)

值得一提的是，这条曲线一定会在左上半部分，因为直线y=x表示正例、负例被认为是正类的概率相同，这实际上就是瞎猜时的效果，而但凡有一点用的分类器都应该获得更高的真正率和更低的假正率，因此曲线应该向左上移动。当曲线能达到 (0,1) 点时，分类效果将是完美的。

通常来说，不同分类器的ROC曲线也会交叉，为了衡量分类器综合效果（也即，要不同阈值下综合表现的更好），使用线下面积AUC (Area Under Curve) 来作为评价指标。



## 算法

### 交叉验证

**思路**

为了评估模型性能，常常将数据集划分成两部分，一部分训练模型，一部分用来验证，分别称为训练集与测试集。在测试集效果好才是真的好。这里所说的效果好，就是模型得分高，“得分”通常用预测/分类准确率、判定系数等来衡量。

上述的划分过程中，依然存在“碰巧”选到了好的数据，因而效果好，这样的可能。更加稳妥的办法是，将数据分成很多部分，每一轮使用一段数据做测试集，其他数据做训练集，得到很多正确率，用这些正确率的平均值衡量模型的有效性。这就称为交叉验证（cross validation）。将样本均分为$n$份，称为进行了$n$轮验证。

交叉验证的一种特例是，每次只取一个样本来测试，其他全部样本训练，训练的轮数与样本总数相同。这称为LOO（leave-one-out）交叉验证。

**代码**

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html

签名：

```python
sklearn.model_selection.cross_val_score(estimator, X, y=None, *, 
                                        groups=None, 
                                        scoring=None, 
                                        cv=None, 
                                        n_jobs=None, 
                                        verbose=0, 
                                        fit_params=None, 
                                        pre_dispatch='2*n_jobs', 
                                        error_score=nan
                                       )
```

样例：

```python
>>> from sklearn import datasets, linear_model
>>> from sklearn.model_selection import cross_val_score
>>> diabetes = datasets.load_diabetes()
>>> X = diabetes.data[:150]
>>> y = diabetes.target[:150]
>>> lasso = linear_model.Lasso()
>>> print(cross_val_score(lasso, X, y, cv=3))
[0.33150734 0.08022311 0.03531764]
```

参数：

​	estimator：所使用的模型对象。

​	X：特征集。

​	y：标签集。

​	cv：交叉验证方式。若给出整数，则是验证的轮数（默认5）；还可以传入交叉验证分组器（CV splitter，sklearn库中的一种对象，按照特定方式产生训练集与测试集）；还可以传入可迭代对象，其内容是训练集与测试集的索引（细节暂不明，等用到了再说）。

​	score：训练效果“得分”的计算方式。默认None，此时使用estimator的默认评分方式。可以传入一个字符串（可行的值参见model evaluation文档）或一个计算得分的函数（函数应该有 `scorer(estimator, X, y)`形式的签名，且只返回一个值作为结果）。

返回值：

​	返回一个列表，其元素是每一轮验证的得分。



# 参数选择

## 验证曲线

**思路**

我们希望直观地看到模型正确性与超参数之间的关系，以便选择合适的超参数。最直观的方法就是，画出模型评分（正确率或者判定系数）关于参数变化的曲线图。这称为验证曲线。

**代码**

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html?highlight=curve#sklearn.model_selection.validation_curve

签名：

```python
sklearn.model_selection.validation_curve(estimator, X, y, *, 
                                         param_name, param_range, 
                                         groups=None, 
                                         cv=None, scoring=None, 
                                         n_jobs=None, 
                                         pre_dispatch='all', 
                                         verbose=0, 
                                         error_score=nan, 
                                         fit_params=None
                                        )
```

样例：

```python
from sklearn.model_selection import validation_curve 
#从0.18版本起，用model_selection代替learning_curve模块 
from sklearn.svm import SVC #这一行不重要，只是给个模型

param_range = np.logspace(-6, -1, 5)
train_scores, test_scores = validation_curve(
    SVC(),X,y,
    param_name="gamma",
    param_range=param_range,
    scoring="accuracy",
)
```

参数：

​	estimator：所使用的模型对象。

​	X：特征集。

​	y：标签集。对于无监督方法，可以留空。

​	param_name：模型中变化的超参数名称，通过字符串形式给出。

​	param_range：模型中变化的超参数取值，以一维数组形式给出。

​	cv：交叉验证方式。若给出整数，则是验证的轮数（默认5）；还可以传入交叉验证分组器（CV splitter，sklearn库中的一种对象，按照特定方式产生训练集与测试集）；还可以传入可迭代对象，其内容是训练集与测试集的索引（细节暂不明，等用到了再说）。

​	score：训练效果“得分”的计算方式。默认None，此时使用estimator的默认评分方式。可以传入一个字符串（可行的值参见model evaluation文档）或一个计算得分的函数（函数应该有 `scorer(estimator, X, y)`形式的签名，且只返回一个值作为结果）。

返回值：

​	依次返回训练集和测试集的模型评分(train_scores, test_scores)，各自以数组形式给出。数组的规模是(n_ticks, n_cv_folds)，前者是变化的超参数的个数，后者是交叉验证的轮数。通常取每行的均值或中位数作为某一超参数下的正确率代表。

## 学习曲线

**思路**

一般来说，随着训练模型的数据量提升，模型效果会改善。但这种改善是有上限的，当达到上限，效果还不理想时，就说明要更换模型了。

为了“看到”这个上限，我们绘制模型得分关于数据规模（样本量）的图像，这就称为学习曲线。

**代码**

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html

签名：

```python
sklearn.model_selection.learning_curve(estimator, X, y, *, 
                                       groups=None, 
                                       train_sizes=array([0.1, 0.33, 0.55, 0.78, 1.]), 
                                       cv=None, 
                                       scoring=None, 
                                       exploit_incremental_learning=False, 
                                       n_jobs=None, 
                                       pre_dispatch='all', 
                                       verbose=0, 
                                       shuffle=False, 
                                       random_state=None, 
                                       error_score=nan, 
                                       return_times=False, 
                                       fit_params=None
                                      )
```

参数：

​	estimator：所使用的模型对象。

​	X：特征集。

​	y：标签集。对于无监督方法，可以留空。

​	cv：交叉验证方式。若给出整数，则是验证的轮数（默认5）；还可以传入交叉验证分组器（CV splitter，sklearn库中的一种对象，按照特定方式产生训练集与测试集）；还可以传入可迭代对象，其内容是训练集与测试集的索引（细节暂不明，等用到了再说）。

​	train_sizes：用来训练的样本量的取值。以数组形式给出。如果元素是整数，那就是每次训练用的样本数目；如果元素是0-1之间的小数，那就代表所用数据占总数据的比例。默认值*np.linspace(0.1, 1.0, 5)*也即*array([0.1, 0.33, 0.55, 0.78, 1.])*。

​	score：训练效果“得分”的计算方式。默认None，此时使用estimator的默认评分方式。可以传入一个字符串（可行的值参见model evaluation文档）或一个计算得分的函数（函数应该有 `scorer(estimator, X, y)`形式的签名，且只返回一个值作为结果）。

​	return_times：是否要计算训练/评分用时。默认否。如果是，返回值的内容会增加。

返回值：

​	依次返回以下内容（实际上是返回它们组成的列表）

​	train_sizes_abs：每次训练所用的实际样本数目。相当于画图时的自变量取值。

​	train_scores：每次训练的训练集得分。

​	test_scores：每次训练的测试集得分。

​	fit_times：训练用时。仅在`return_times=True`时给出。

​	score_times：评分用时。仅在`return_times=True`时给出。

## 网格搜索

**思路**

实际的模型通常需要多个参数，如果一一通过曲线图来选定参数，会很繁琐。实际上，多个参数的多个可能取值组成了高维的参数网络，这个网络中的每个点都是可以选定的参数组合。我们希望从参数网络中直接选出最优取值。这实际上是一个多元函数求极值点的问题。

**代码**

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

签名：

```python
class sklearn.model_selection.GridSearchCV(estimator, 
                                     param_grid, *,
                                     scoring=None, 
                                     n_jobs=None, refit=True, 
                                     cv=None, 
                                     verbose=0, pre_dispatch='2*n_jobs', 
                                     error_score=nan, 
                                     return_train_score=False)
```

样例：

```python
X, y = make_hastie_10_2(n_samples=8000, random_state=42)

# The scorers can be either one of the predefined metric strings or a scorer
# callable, like the one returned by make_scorer
scoring = {"AUC": "roc_auc", "Accuracy": make_scorer(accuracy_score)}

# Setting refit='AUC', refits an estimator on the whole dataset with the
# parameter setting that has the best cross-validated AUC score.
gs = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid={"min_samples_split": range(2, 403, 10)},
    scoring=scoring,
    refit="AUC",
    return_train_score=True,
)
gs.fit(X, y)
results = gs.cv_results_
```



参数：

​	estimator：所使用的模型对象。

​	param_grid：参数网络。以字典形式给出，其中每个元素的键是参数名，值是有可取到的参数值组成的列表。

​	cv：交叉验证方式。若给出整数，则是验证的轮数（默认5）；还可以传入交叉验证分组器（CV splitter，sklearn库中的一种对象，按照特定方式产生训练集与测试集）；还可以传入可迭代对象，其内容是训练集与测试集的索引（细节暂不明，等用到了再说）。

​	scoring：训练效果“得分”的计算方式。默认None，此时使用estimator的默认评分方式。可以传入一个字符串（可行的值参见[model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)文档）或一个计算得分的函数（只有一个返回值）。还可以同时使用多个评分方式。

​	return_train_score：是否要返回训练得分。默认否。如果是，属性cv_results_的内容会增加。

​	verbose：显示信息的详细程度。默认0，不显示。

​	refit：是否要用选出的最优参数拟合一个模型。如果是，之后可以用best_estimator_引用这个模型。默认是。

方法：

​	fit(X, y)：训练模型。

​	predict(*X*)：对新的X使用模型，计算拟合值。仅当refit为真，且使用的模型对象有predict方法时有效。

​	transform(*X*)：对新的X使用模型，进行变换。仅当refit为真，且使用的模型对象有transform方法时有效。

​	score(*X*, *y=None*)：对给定的数据，计算模型评分。

属性：

​	best_params_：最优参数。以字典形式给出。

​	best_score_：最优参数所得到的模型评分。

​	best_estimator_：最好的参数所拟合出的结果。

​	cv_results_：保存了结果的字典。其元素（键）包括使用的参数，以及测试的结果，对应的值为每次测试结果组成的列表，同索引的说明是同一次测试。如下例

```python
{
'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],
                             mask = [False False False False]...)
'param_gamma': masked_array(data = [-- -- 0.1 0.2],
                            mask = [ True  True False False]...),
'param_degree': masked_array(data = [2.0 3.0 -- --],
                             mask = [False False  True  True]...),
'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],
'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],
'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],
'std_test_score'     : [0.01, 0.10, 0.05, 0.08],
'rank_test_score'    : [2, 4, 3, 1],
'split0_train_score' : [0.80, 0.92, 0.70, 0.93],
'split1_train_score' : [0.82, 0.55, 0.70, 0.87],
'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],
'std_train_score'    : [0.01, 0.19, 0.00, 0.03],
'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],
'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],
'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],
'std_score_time'     : [0.00, 0.00, 0.00, 0.01],
'params'             : [{'kernel': 'poly', 'degree': 2}, ...],
}
```

其中的时间单位是秒。