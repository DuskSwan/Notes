# 零、说明

1. 以下笔记纯粹叙述各种算法，包括它们的假设、核心思路、性质、计算方法、应用场景等。

1. 以下笔记不涉及学科讲解。默认读者拥有线性代数、概率论、数理统计的基本知识。

1. 代码部分，用到的库包括但不限于numpy，pandas，sklearn等。默认读者拥有Python的基本知识，以及了解这些库。

   

# 一、概念描述

## 宏观概念

统计学：收集数据，提取数据中的信息，对所研究的问题作出推断。要得出的结论通常是估计参数或者检验假设。又可分为参数统计与非参数统计。参数统计依赖“模型”，先假定数据服从某个模型，然后估计这个模型中的参数，或者对模型的有效性作出检验，因此统计学可以成为“建模”的手段。非参数统计不对数据满足的模型做出假设，只考虑对数据做出某些推断。

机器学习：机器学习本质上是在构筑一个固定的“算法”，这个算法可以针对一些数据算出一些结论。具体来说，机器学习所做的事情是让机器从数据中确定一些参数，进而确定算法，进而处理新的数据。这样的算法可以来自一个统计学中的模型，也可以来自纯粹穷举，或者是什么其他思路。至于机器学习与统计的关系，应该说有所交集，但不相同。

数据科学：研究数据的科学。研究手段既涵盖了统计的方法（推断、检验），又涵盖了机器的方法（设计算法），当然还有一些别的（比如具体领域内的专业知识）。可以看作是统计与机器学习的应用。

## 数据描述

为了叙述上的统一，在此规定一些概念（以下面所示的数据集和模型为例，假设我们认为数据符合线性回归模型）

| 编号  | 属性x1   | 属性x2   | ...  | 属性xp   | 目标y |
| ----- | -------- | -------- | ---- | -------- | ----- |
| 个体1 | $x_{11}$ | $x_{12}$ | ...  | $x_{1p}$ | $y_1$ |
| 个体2 | $x_{21}$ | $x_{22}$ | ...  | $x_{2p}$ | $y_2$ |
| ...   | ...      | ...      | ...  | ...      | ...   |
| 个体n | $x_{n1}$ | $x_{n2}$ | ...  | $x_{np}$ | $y_n$ |

$$
y=f(\beta,\theta,x_1,x_2,...,x_p)
$$

数据集：记录数据的表格。通常每行记录一个个体的信息，每列记录不同个体的同属性数据。不包含目标列的数据可以用矩阵$X=(x_{ij})$记。

样本：数据集的一行是一个样本。

特征：数据集的每一列是一个特征。通常将其中一项认为是受其他特征影响的因变量，其他的是自变量。有时，仅仅把自变量列看作特征。

标签：作为预测目标的特征是标签，或者叫目标数组、因变量，可以用$y=(y_1,\cdots,y_n)'$记。

## 训练模型涉及的概念

模型：自变量与因变量的关系即模型，模型本质上是函数，但未必有表达式。在上例中是$f$。

超参数：在选择模型类时就要确定的参数，比如是否中心化等。在上例中是$\theta$。这些参数由于各种原因，不适合根据数据来确定，需要人为指定。

参数：模型中待定的变量，需要根据数据求出一些值作为参数，使得模型的预测效果最好。在上例中是$\beta$。在确定$\beta$之前，模型其实是模型空间中的众多函数，确定了$\beta$之后，模型才是一个确定可用的函数。二者容易混淆，要根据语境判断。



# 二、分析过程

数据分析的大致过程是数据预处理→选择模型→训练模型→评价模型，如果模型比较好就可以使用，否则重新选择模型。有时我们对数据服从的模型没有头绪，可以先进行数据感知，查看数据的分布特点，再做进一步打算。

数据预处理，指的是讲原始数据处理成便于后续操作的过程。其中有的是为了让后续操作具有可能性，比如缺失值处理、特征降维、离散特征连续化等；有的则是为了让模型具有更好的效果，比如去噪、归一化等。

数据感知，即用各种指标反应数据的特点。通常包括画散点图查看样本分布、计算协方差矩阵、计算特征与标签的相关来反应特征重要程度等等。

选择模型顾名思义，就是选择合适的模型类型，并确定超参数。

训练模型，指的是根据已知的数据，求出模型的参数使得模型达到最有效果。

评价模型，也可称为模型诊断，指将一些数据带入求解出的模型算出一些指标，用指标衡量这个模型的好坏。这一步通常和训练模型结合进行，先将数据划分为训练集与测试集两部分，用训练集的数据求解参数，再用测试集的数据带入模型计算评价指标。还有交叉验证等更加复杂的方案。

在训练神经网络时，情况要复杂一些。由于训练神经网络需要的数据量非常大，通常要将全部数据分为很多小部分，每一部分称为一个batch，每次只向神经网络中输入一个batch，这才能够运行。如果全部的数据还不够多，就将这些数据重复使用，每一次使用全部数据称为一个epoch。实际上模型运行的次数是epoch_n * batch_n 这么多次，每次计算了batch_size条样本。