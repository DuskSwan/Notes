[toc]

# 迁移学习

迁移学习的核心思路是，将某个领域或任务上学习到的知识或模式应用到不同但相关的领域或问题中。这里的“领域(domain)”，是比任务更高一级的存在，同一领域（比如图书情感分析领域）中存在多个任务（针对不同的书来分析）。

一种具体操作是，将一个任务上训练出来的网络或其一部分，作为另一个任务的模型（初始条件）或者模型的一部分，这称为fine-tuning。这样做的好处是，减少了训练的成本，也可以在新任务的数据量较小（因而不足训练）的时候，更快得到一个可用的模型。





# 元学习

这里记录了一些参考。

[元学习: 学习如何学习【译】](https://wei-tianhao.github.io/blog/2019/09/17/meta-learning.html )

## 一、目标

为了说明“元学习”到底要做什么，先给出一个问题：

我们手里有很多已知的数据集。每一个数据集中有很多图片，比如一号数据集中的图片是猫和狗，二号数据集中的图片是杯子和飞机，三号数据集是书、瓶子和台灯……

利用传统的机器学习方法，我们可以为每个数据集都训练一个分类器。比如用一号数据集训练出的分类器，能识别一张图片的内容是猫还是狗；用二号数据集训练出的分类器，能识别一张图片的内容是杯子还是飞机……

翻译成数学语言就是，对$k$号数据集$D^{(k)}=\{(x^{(k)}_i,y^{(k)}_i)\}$，可以得到模型$f^{(k)}_\theta$，使得模型对于该数据集上数据的预测$ \hat y=f^{(k)}_\theta(x)$有最小的误差，其中$\theta$是模型的参数。

现在，对于前$n-1$个数据集，我们都如此建立了模型$f^{(1)}_\theta,f^{(2)}_\theta,\cdots,f^{(n-1)}_\theta$。但是第$n$个数据集却让我们犯了难：这个数据集的样本量很小，直接训练的模型效果很差；图片内容又和之前的数据集都不相同，不能直接套用前面训练好的模型。

这下该怎么得到效果好的模型$f^{(n)}_\theta$呢？

更进一步，即使我们反复调参，终于得到了效果比较好的$f^{(n)}_\theta$，但是保不齐以后还会遇到同样样本量小且目标新奇的第$n'$数据集，到那时，又该如何得到效果好的模型$f^{(n')}_\theta$呢？如果每次都要动手反复调参，岂不是太麻烦了吗？



这个问题在现实中也有实例：一个孩子经过反复学习，可以分辨猫和狗；经过反复学习，可以分辨鸟和鱼。忽然有一天他见到了牛和羊，如果不让他反复学习，只是看了几眼就离开，那么他下次还能分辨牛和羊吗？他要怎么在有限的几次观察中找到合适的分辨依据呢？

对人类来说，这并不是个困难的问题。人在学习时，会本能地联系之前学过的知识。即使这个孩子没有足够的机会发觉牛和羊的本质区别，他也可以根据辨别猫狗、鱼鸟时的经验来判断——耳朵形状不同、体型大小不同，那么就不是同一种动物。

基于这样“先验”的知识，这个孩子在面对新的未知的任务时，也能快速、准确地完成。



借鉴人类思考的这种“从过往学习中提取先验知识”的思路，为了一劳永逸地解决上述小样本、多任务的问题，我们考虑做到这样的事情：

如果有一个“模型产生器”$F$，它能根据一个已知的数据集$D$、和对应的任务目标$T$（比如辨别猫和狗），返回一个可以完成任务的模型$f$，$f$可以应用到数据集$D$或者新的未知数据上，进行$ \hat y=f(x)$的预测——即使数据集$D$是样本量小且目标新奇的也依然有效——那就解决了上述的问题。只要有确定的$D,T$，那么$f=F(D,T)$就是我们想要得到的模型。

这个“模型产生器”$F$，实际上也是一个“模型”。传统的机器学习所给出的模型$f$，描述的是特征$x$与标签$y$之间的函数关系，训练模型的目的是确定其参数$\theta$；而“模型产生器”$F$，描述的是任务$ D,T$与解决方案$f$之间的关系，它也有需要被训练确定的参数$\omega$。$F$是制造模型的模型，可以命名为meta-模型，也即元模型。$F$的参数$\omega$就是我们想求出的“先验知识”，确定了参数$\omega$，$F$才能根据新的任务$D,T$给出可靠的模型$f$。

传统的机器学习，学习的内容是特征$x$与标签$y$之间的函数关系；而产生$F$的过程，学习的内容是任务$ D,T$与解决方案$f$之间的函数关系，也即是在学习如何”学习得到$f$“。所以这一过程是”学习的学习“，可以命名为meta-学习，也即元学习。



总的来说，元学习的目标是得到一个模型$F$（或者说是参数$\omega$），$F$会针对特定的任务$T$，在给定的数据集$D$上训练得出一个描述数据的模型$f$。而且，即使样本少（few-shot）、任务类型多（multi-task），计算速度和准确度也依然较高。



## 二、思路

在传统的机器学习中，我们会使用一个特定的模型（比如回归模型、决策树），模型的结构是确定的，参数是待定的。确定的参数$\theta$会带来一个确定的模型$f_\theta$，而我们要做到就是根据数据找到最优的$\theta$。

在元学习中，目标是根据数据集和任务得到模型$f_\theta$，这实际上就是要根据数据集和任务得到参数$\theta$。而产生$f_\theta$或者$\theta$的元模型$F$也会有自己的参数$\omega$，它实际上代表了有关“学习”的知识，可以称为元参数或者元知识。正如机器学习的目标是确定参数$\theta$，元学习的目标就是确定元参数$\omega$。

事实上，元参数就是超参数——$F$的目标就是自行确定“机器学习时需要人为确定的参数”。后文将不再分辨这两种称呼，元参数与超参数都用$\omega$表示。

>  举个例子：
>
>  有一些均分区间$[0,2\pi]$的点$x_i$，还有一些在$[0,2]$上均匀分布的$a_k,b_k$，生成了$y_i^{(k)}=a_k\sin(x_i+b_k)$。
>
>  现在令数据集为$D_1=\{(x^{(1)}_i,y^{(1)}_i)\},D_2=\{(x^{(2)}_i,y^{(2)}_i)\},\cdots$，对于每个数据集，我们都希望拟合出一个$f_{\hat a_k,\hat b_k}=\hat a_k \sin(x+\hat b_k)$来描述之。这实际上就是求$\hat a_k,\hat b_k$，使得损失函数最小，是个优化问题。通常采取梯度下降法解决这样的优化问题，而梯度下降法是迭代产生$\hat a_k,\hat b_k$的，此时唯一需要指定的参数就是，迭代的初始值$\hat a^{(0)},\hat b^{(0)}$。
>
>  我们希望找到一组良好的初始值$\hat a^{(0)},\hat b^{(0)}$，它在处理任何一个数据集$D_k$时，都能很快收敛到目标解。这样的$\hat a^{(0)},\hat b^{(0)}$就是该问题中的元参数$\omega$。



### 概念叙述

假设有$n$个数据集$D_k=\{(x_i^{(k)},y_i^{(k)})\}(k=1,2,\cdots,n)$（为了帮助理解，不妨认为每个数据集都是图片集，$x_i$是图片，$y_i$是图片的标签，也即图片内容，比如猫、狗），每个数据集对应一项任务$T_k$（比如识别一张图片是不是猫，是不是狗）。对于元学习来说，每个$(D_k,T_k)$是输入，针对这组数据和任务得到的模型是输出。

$T_k$通常可以根据$D_k$的内容理解得到，并不参与运算，我们重点关注实实在在的数据集$D_k$。$D_k$可以称为一个任务的实例（instance），又可以称为元任务（meta-task），也记为$\tau_k$。

为了实现元学习，我们把每个$D_k$划分为支撑集$S_k$与询问集$Q_k$（可将此二者当做元任务内部的训练集、测试集理解）。设每个$S_k$中的类别数目都为$N$，每一类中都包含$K$个样本，$Q$中的类别数目为$N'$。该元任务学习参数的过程即称为“N-way K-shot 问题”，或者简称为“N-K建模”。通常$N'<N$，$K$也会比较小。



### 机器学习思路

一切机器学习的问题，都可以看作是这样的问题：特征与标签组成的对$(\boldsymbol{x},y)$满足某种关系，而我们已知了很多样本$(\boldsymbol{x}_i,y_i)$，希望得到一个函数$h(\boldsymbol{x})=y$来描述特征与标签之间的关系。

为了解决这一问题，我们需要先选择一个模型，这个模型会有很多参数等待确定。这其中，有一些是训练之前就要指定的超参数$\omega$，另一些是只指定的形式、具体数值由数据决定的普通参数$\theta$。参数（经过训练）确定了之后，我们就得到了如上所述的函数$h(\boldsymbol{x})=y$，显然$h$也受参数$\omega,\theta$决定，所以把它写成$h_{\omega,\theta}(\boldsymbol{x})=y$。

对于任一个企图描述$(\boldsymbol{x},y)$之间关系的函数$h$，令$\hat y=h(\boldsymbol{x})$作为$h$的预测结果。针对一系列输入$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots$，自然得到一系列预测值$\hat y_1,\hat y_2,\cdots$。定义损失函数$\mathscr{l}(y,\hat y)=\mathscr{l}(\boldsymbol{x},y,h)$来描述预测的准确程度，并且记变量$(\boldsymbol{x},y)$取值$(\boldsymbol{x},y)$的概率为$P(\boldsymbol{x},y)$，那么函数$h$预测时的损失的期望值（称为风险期望，expected risk，可以认为是预测误差的度量）为
$$
R(h)
=\int \mathscr{l}(\boldsymbol{x},y,h)\rm d P(\boldsymbol{x},y)
=E \mathscr{l}(\boldsymbol{x},y,h)
$$
由于$P(\boldsymbol{x},y)$未知，我们定义经验风险期望来代替它：
$$
R_n(h)
=\frac1n \sum_{i=1}^n \mathscr{l}(\boldsymbol{x}_i,y_i,h)
$$
这个值越小，就说明函数$h$越正确。

用$\mathcal{H}$表示我们选定的模型、所能构造出的一切$h$组成的集合，称为假设空间。由此定义出以下函数
$$
\begin{align}
h^* &= \operatorname{argmin}\limits_h R(h) \\
h^*_\mathcal{H} &= \operatorname{argmin}\limits_{h\in\mathcal{H}} R(h) \\
h_n &= \operatorname{argmin}\limits_{h\in\mathcal{H}} R_n(h) \\
\end{align}
$$
这三者分别表示全局最优解、假设空间中的最优解、假设空间中的经验函数最优解，我们的目标是得到$h^*$，但实际能求出来的是$h_n$。

由于我们已经选定了模型，不同的$h$实际上只取决于不同的参数$\omega,\theta$，所以风险期望又可以表示为$R_n(h)=R_n(\omega,\theta)$，也即任何选定的参数都能计算出对应的风险期望。让风险期望最小的参数$\omega,\theta$就是良好的、应该采用的$\omega,\theta$。

通常，我们会固定$\omega$，求解使得$R_n(\omega,\theta)$最小的$\theta$。令$\theta$的一切可以取到的值组成的集合为$\Theta$，那么，类比$h$，可以定义$\theta_n = \operatorname{argmin}\limits_{\theta\in\Theta} R_n(\omega,\theta) $，这就是我们可以求出、也是将要得到的结果。



### 元学习思路

元学习的目标，是得到一个元模型$F_\omega$（或者说是参数$\omega$，毕竟$F_\omega$由$\omega$决定），$F_\omega$针对特定的任务$\tau$，能给出模型$f_\theta$（或者说是参数$\theta$，毕竟$f_\theta$由$\theta$决定）。即使任务$\tau$所包含的样本少（few-shot）、可能出现的任务类型多（multi-task），$f_\theta$的训练速度和准确度也依然较高。

实际上，“$f_\theta$”的“长相”无法改变，众多的$f_\theta$只是由参数空间$\Theta$中的无数$\theta$所决定的。所以我们的元模型$F_\omega$，其输出值是不同的$\theta$。而$F_\omega$的参数$\omega$，就是在计算$\theta$时、不发生改变的“设定”。这实际上就是机器学习过程中，模型的超参数。在机器学习中，需要人为指定超参数，编程者就承担着$F_\omega$的角色。

作为编程者，我们在机器学习中都做了什么？面对单一的任务中，我们每选定一个超参数$\omega$，都能通过最小化$R_n(\omega,\theta)$的方式求出一个$\omega$所对应的、最好的$\theta_\omega$。那么，使得$R_n(\omega,\theta_\omega)$最小的$\omega^*$就是一切$\omega$中最好的。

如果面对的是很多任务$D_1,D_2,\cdots,D_n$（或者记为$ \tau_1,\tau_2,\cdots,\tau_n$），我们要为它们设置相同的$\omega$，综合考虑$\omega$在这么多任务上的表现，问，怎么求出最好的$\omega$？

一个朴素的思路是，不断尝试各式各样的$\omega$，直到每个任务上的综合表现都不错——这也就是调参了。

现在，我们要让“调参”自动化。这里的（超）参数，除了数值型参数，还包括网络结构、优化器种类等等。



为了让机器理解参数的效果好坏，首先需要描述某个$\omega$在某个任务上$\tau_k$上的表现，我们依然定义损失函数$l$来度量参数$\omega,\theta$下模型的好坏。但是不同于之前说过的、描述预测准确程度的损失函数$\mathscr{l}(y,\hat y)=\mathscr{l}(\boldsymbol{x},y,h)=\mathscr{l}(\boldsymbol{x},y,\theta)$，现在，损失函数要描述的是参数$\omega,\theta$所决定的模型应用到任务$\tau_k$上的表现。所以，损失函数应该有形式$l(\tau_k,\theta,\omega)$。

假设总数据集为$D=\{(x_i,y_i)\}$，将其分为了训练集$D^{tr}$和测试集$D^{te}$。训练集$D^{tr}$和测试集$D^{te}$又各自分为很多小集合，称为“元任务”（meta-task），可以用$\tau^{tr}_i,\tau^{te}_j$来表示。为了便于编程，我们在划分时令这些元任务中的样本量都相同。在每一个元任务中，再按照一定比例将其划分为支撑集$S$与询问集$Q$。

对任一个$\omega$，在每个训练任务$\tau^{tr}_i$中，理论上可以计算出最优的$\theta^*_\omega$（为了衡量最优，需要在任务中再定义训练集与测试集，这就是支撑集与询问集存在的意义）。不过，实际上算出的只是$\theta^*_\omega$的近似，用$\theta_\omega$记。那么在这个任务中，$\omega$的效果可以用损失函数$l(\tau^{tr}_i,\theta_\omega,\omega)$来衡量。

同样定义经验风险期望
$$
R_n(\omega)
=\frac1n \sum_{i=1}^n l(\tau^{tr}_i,\theta_\omega,\omega)
$$
来衡量“平均损失”，那么，使得该值最小的$\omega^*$就是最好的$\omega$。

当然，为了避免过拟合，需要在测试集（由测试任务组成）上考察$\omega$的效果。在测试集上的效果好，才是可靠的。这一点与传统的机器学习相同。



## 三、算法

根据求解$\omega$的方法，可以将元学习算法分为基于优化、基于度量、基于模型，这三类。



### 基于优化的元学习

这一类算法的代表是MAML（Model-agnostic Meta-learning），它通过梯度下降法迭代的求出使得平均损失$R_n(\omega)$最小的$\omega$。不过，这仍是一个宽松的概念，采用下述思路的方法都可称为是MAML，还可以具体出各种各样的实例。

MAML的算法流程如下：

---

1. 将样本集$D$分为若干训练任务$\tau^{tr}_i$和测试任务$\tau^{te}_j$，每个任务内部又分支撑集和询问集；

1. 选定一个将要产生的$f$的模型结构（网络结构），并随机初始化一个meta网络的参数为$\omega_0$（这个网络将要应用到新的任务上）；

1. 针对每一个训练任务$\tau^{tr}_k$，执行以下操作：

   a. 初始化一个训练网络，其参数的初始值$\theta^{(k)}_0=\omega_j$；

   b. 计算参数$\omega,\theta_i^{(k)}$下，模型在任务$\tau^{tr}_k$的支撑集$S_k$上的损失函数$l(\tau_k^{tr},\theta_{i}^{(k)},\omega_j)$；

   b. 基于设定好的学习率$\alpha$，通过梯度下降法做出更新$\theta_{i+1}^{(k)}=\theta_{i}^{(k)}-\alpha\nabla_\theta l(\tau_k^{tr},\theta_{i}^{(k)},\omega_j)$；

   c. 重复这样的更新$m$次，得到$\theta_m^{(k)}$；

1. 在每个任务$\tau^{tr}_k$的询问集$S_k$上，计算损失函数并求和，得到$ \sum\limits_{k}l(\tau_k^{tr},\theta_{m}^{(k)},\omega_j)$；

1. 基于设定好的meta学习率$\beta$，通过梯度下降法做出更新$\omega_{j+1}=\omega_j-\beta \nabla_\omega\sum\limits_{k}l(\tau_k^{tr},\theta_{m}^{(k)},\omega_j)$；

1. 重复这样的更新，直到满足结束条件。此时的$\omega_j$就是效果好的超参数$\omega$，以后处理新的任务时就用$\omega_j$作初始化参数。

---

可以看到，MAML旨在通过两层迭代，先得到比较好的$\theta$，再进而得到比较好的$\omega$。这一算法与选用的模型无关，因而可以应用在各种模型上，进而可以完成各种领域的任务。

实际中，MAML所改变的$\omega$主要是模型初始化参数，很像预训练。最终达到的效果是预训练后的模型在每个新任务上只需要较少次数的迭代就能达到好的效果。

在MAML的基础上，有改进模型FOMAML（First-order MAML）、iMAML（implicit MAML）等，此外还有Reptile等方法。它们的核心思路都是梯度下降法，相比MAML的主要变化是“梯度”的计算思路，可以简单认为是更新用的算式不同。

比如FOMAML（一阶MAML），改进之处见：https://zhuanlan.zhihu.com/p/508512375

Repile同样每次在一个训练任务上得到一个最佳模型与最佳参数，但与MAML不同，它不是根据最佳模型的loss对$\omega$的梯度来更新$\omega$，而是让$\omega$向着当前的最佳参数前进一个小距离，来进行更新。详见：https://towardsdatascience.com/paper-repro-deep-metalearning-using-maml-and-reptile-fd1df1cc81b0。MAML与Reptile的代码实例可见https://github.com/AdrienLE/ANIML。

另外，还可以利用LSTM网络，通过梯度的某种函数迭代产生被更新的值（而不是靠当前参数减去梯度来更新）。



### 基于度量的元学习

基于度量的方法，希望通过某种方式度量支撑样本与询问样本之间的差异，最好的$\omega$会使得询问样本与支撑样本之间的“差别”最小。

为了实现上述思路，首先需要一个特征提取函数（也可称嵌入函数，总之就是把原数据变换成其他样子的函数，通常是降维用的）$f$，它将样本映射为向量，之后只需要定义向量之间的距离，即可度量其相似性。

假设度量函数为$M_\omega$，嵌入函数为$f_\omega$，支撑样本与询问样本分别是$x^S,x^Q$，可以用他们的某种（损失）函数$l(x^S,x^Q,f_\omega,M_\omega)$来描述支撑样本与询问样本之间的差异大小，使得其期望最小的
$$
\omega^*=\arg\min_\omega E_{x^S,x^Q\in D^{tr} } \  l(x^S,x^Q,f_\omega,M_\omega)
$$
就是最优的$\omega$。其中度量函数和嵌入函数都有下标$\omega$，是因为“如何选择合适的嵌入与度量”，可以看做是一个参数，由$\omega$控制。

孪生网络（Siamese Net）是度量学习的一个代表，它用来处理N-way 1-shot问题。

假设有一个任务$\tau$，其中有$N$个支撑样本，各自成为一类，还有一个询问样本，我们希望知道询问样本属于支撑样本中的哪一类（可以以人脸识别为例理解这一问题）。考虑构造一个网络$f_\omega$来处理支撑样本与询问样本，得到$z^S=f_\omega(x^S)$或者$z^Q=f_\omega(x^Q)$，再算出询问样本到每个支撑样本的距离$M_\omega(z^S,z^Q)$。那么，当$x^S$与$x^Q$属于同一类时，距离$M_\omega(z^S,z^Q)$应该比较小；反之距离较大。

因此，应该定义这样的损失函数：当$x^S$与$x^Q$属于同一类时，损失函数与距离$M_\omega(z^S,z^Q)$都是越小越好，所以二者正相关；反之负相关。在最初的孪生网络中，定义的损失函数是能量对比函数（contrastive energy function）：
$$
l(x^S,x^Q,f_\omega,M_\omega)
=\frac{1}{2N} \sum\limits_{x^S} 
I_{\{x^S与x^Q同类\}} d^2 +
I_{\{x^S与x^Q不同类\}} \max(m-d^2,0)^2 
$$
其中$f_\omega$可以是CNN、LSTM等网络；$d=M_\omega(z^S,z^Q)$就是距离，最初取欧氏距离（2-范数），后续有曼哈顿距离（1-范数）等改进；$m$则是阈值参数。通过最小化损失函数来得到最优的$\omega$（事实上就是训练出一个$f_\omega$，距离$M_\omega$通常人为选定）。

确定$\omega$之后，对于新的未知样本，可以计算出它到各个已知类别的距离，即可判定它属于最近的类别。

同样的思路还可以推广，比如三元组网络（Triplet Net）：它同时询问两个样本的类别，每次输入一个三元组$(x^S,x^{Q_1},x^{Q_2})$，第一分量$x^S$是基准（anchor），认为它是正例，我们希望$x^{Q_1}$也是正例，而$x^{Q_2}$是负例。所以$M_\omega(z^S,z^{Q_1})$越小、$M_\omega(z^S,z^{Q_2})$越大，就越理想，损失函数也应该越小，于是定义损失函数为$l=E_{\{x^S\in S\}}\max(0,M_\omega(z^S,z^{Q_1})-M_\omega(z^S,z^{Q_2})+\alpha)$，其中$\alpha$是控制类与类间距的参数。同理通过最小化损失函数来确定$\omega$。

拓展$f_\omega$与$M_\omega$可以采用的形式，可以得到各种各样的、基于度量的学习方法。比如匹配网络（Match Net），元型网络（Prototypical Net），关系网络（Relation Net）等。



### 基于模型的元学习

基于模型的方法，希望可以利用从历史数据中提取出的信息，通常用于处理序列型数据。它将”长期记忆“看作要学习的元知识，用一个“记忆模块”来保存这些元知识，并在读入新数据后更新。

假设支撑集$S$由一系列$(x^S_i,y^S_i)$组成，这些样本将用来更新记忆模块，更新方式为$f_\omega$，最终得到一个模型$g_\theta$（或曰其参数$\theta$），$\hat y^Q=g_\theta(x^Q)$即是询问样本$x^Q$的预测值。使得$l(S,x^Q,\theta,\omega)$最小的$\omega$就是最优的参数。

根据长期记忆被传给模型的方式不同，基于模型的方法又可分为形式化的（meta-representation）、参数化（meta-parameter）的两类方法。

#### 形式化

形式化的方法以记忆增强神经网络（Memory Augment Neural Network, MANN）为代表。记忆增强神经网络的结构参考了神经图灵机（Neural Turing Machine, NTM），主要包括控制器（controller network）和记忆模块（memory module）两部分，控制器是一个神经网络，可以接受数据并向记忆模块中写入信息、或者从记忆模块中读取信息；记忆模块是一个矩阵。它的工作流程是，控制器读入数据后，从数据中提取信息，利用提取出的信息与记忆模块储存的信息作用，得到输出，然后再更新记忆模块。

由于向记忆矩阵读写信息的方式是固定的，该网络的参数实际上就是控制器的参数。

对于任务$D$，其中有$n$个支撑样本$x_1^S,...,x_n^S$，和一个询问样本$x^Q$，以及它们对应的标签$y_j^S$。以序列的形式，依次输入$(x_1^S,\text{null}),(x_2^S,y_1^S),(x_3^S,y_2^S),...$来训练模型（错位的目的，是延缓样本的标签进入模型，以延长样本在记忆模块中储存的时间），最终得到一个分类器。

针对第$t$个支撑集，将记忆模块矩阵记为$M_t$，设它是$n\times m$阶，其中$n$是支撑样本数量，它的每一行$M_t(j)$储存了样本$x^S_j$的信息。

再定义一个$n$维行向量$w_t$为读取时的权重向量，令$r_t=w_tM_t=\sum\limits_{j=1}^nw_t(j)M_t(j)$，其中$w_t(j)$是第$j$分量，$M_t(j)$是第$j$行。它就是从记忆模块中读取出的信息，是一个$m$维行向量。为了得到权重，需要控制器网络先根据新输入算出键值向量$k_t$，再计算$k_t$与$M_t(j)$的余弦距离$ \displaystyle\frac{k_t\cdot M_t(j)}{\|k_t\|\|M_t(j)\|}$，全部的$n$个余弦距离以softmax函数转化成$n$个权重，就是权重向量$w_t$。

而写入新信息到记忆模块时，MANN采用一种LRUA原则，也即将新信息更新到最少（Least）最近（Recent）使用（Used）的位置（Access）。这一过程比较繁琐，此处暂且略过。

> MANN写入记忆的操作与NTM不同，NTM的更新方式如下：
>
> 在更新记忆模块时，有擦除和写入两个操作。擦除需要借助擦向量$e_t$，它是一个$m$维向量，分量仅为$0$或$1$；写入需要借助写入向量$a_t$。通过式
>
> $$
> \overline M_t(i)=M_{t-1}(i)[1-w_t(i)e_t] \\
> M_t(i)=\overline M_{t}(i)+w_t(i)a_t
> $$
>
> 来更新记忆模块。其中$w_t(i)$是第$i$分量，$M_t(i)$是第$i$列；上面两式中的向量运算都是按分量运算，仍得到向量（可看做Python中向量的直接运算）。

#### 参数化

参数化的方法以MetaNet为代表，其核心思路是，由两套权重参与模型计算，其中慢权重（slow weights）是针对任务间的联系更新的参数，由梯度下降法确定；而快权重（fast weights）是针对的是每个任务更新的参数，面对新任务时快速计算得出。

与MANN类似，MetaNet也有一个外部的记忆模块。此外，主要由元学习器（meta learner）和基本学习器（base learner）组成，基本学习器就是针对特定任务的模型，元学习器是面对所有任务的模型。

为了描述该方法的流程，首先定义以下符号：

>$b$：基本学习器。它有参数$W,W^*$，前者是梯度下降法更新的慢权值，后者是根据每次的任务确定的快权值。
>
>$u$：动态表征函数/嵌入函数。它有参数$Q,Q^*$，前者是梯度下降法更新的慢权值，后者是根据每次的任务确定的快权值。该函数会快速从样本中提取信息，将$x_i$对应的输出记为$r_i$。
>
>$m,d$：两个快权重生成器，分别由参数$Z,G$决定，生成$W^*,Q^*$。
>
>$\theta$：慢权值的合集，包括了$W,Q,Z,G$。
>
>$M$：记忆模块。一个矩阵，行数$n$是支撑集样本数。
>
>$R$：记忆模块的索引矩阵。

算法流程如下：

![](img/MetaNet算法流程.webp)

解释如下：

1. 表征函数的学习：

   将随机采样的支持集数据输入到表征（嵌入）函数$u$中，为了得到数据集的嵌入，利用表征损失 $loss_{emb}$来捕获表示学习目标，并将梯度作为meta information获取。其中损失函数为
   $$
   \mathcal{L}_i=loss_{emb}\left(u(Q,x_i'),y_i'\right)
   $$
   它的具体计算是随机抽取$T$对支持集样本的来计算嵌入损失
   $$
   \mathcal{L}_i=loss_{emb}\left( u(Q,x_{1,i}'),u(Q,x_{2,i}'),l_i \right)
   $$
   其中$l_i$是辅助标签
   $$
   l_i=\begin{cases}
   1, & y_{1,i}'=y_{2,i}' \\
   0, & y_{1,i}'\neq y_{2,i}' 
   \end{cases}
   $$
   针对每一对输入，通过反向传播得到其损失梯度信息$\nabla_i=\nabla_Q\mathcal{L}_i$，通过快权值生成函数$d$得到快权值$Q^*=d(G,\{\nabla\}_{i=1}^T)$。这样一来，就确定了$u$。 

1. 快权值的生成：

   对每个支持集样本，输入到基本学习器$b$中，就可以计算出预测的标签。将它和实际标签通过交叉熵等方式计算损失函数
   $$
   \mathcal{L}_i=loss_{task}\left(b(W,x_i'),y_i'\right)
   $$
   就生成了基本学习器$b$需要的meta formation，即支持集的损失梯度信息$\nabla_i=\nabla_W\mathcal{L}_i$。函数$m$从损失梯度$\nabla_i$通过映射$W_i^*=m(Z,\nabla_i)$学到快权值，分别存入记忆模块$M$的每一行。

1. 建立支持集的索引：

   利用参数为$Q,Q^*$的表征函数$u$为支持样本建立索引（有快权值的嵌入），并存入$R$的每一行。
   $$
   r_i'=u(Q,Q^*,x_i')
   $$

1. 建立训练集的索引：

   与上一步类似，利用参数为$Q,Q^*$的表征函数$u$为询问样本建立索引$r_i=u(Q,Q^*,x_i)$

1. 对快权值的读取：

   如果参数$W_i^*$存储在$M$中且索引$R$已经建立，用attention函数（这里的attention用余弦相似度计算存储索引和输入索引）在之前建立的所有支持集的索引$R$和每一个训练集的索引计算一个相似分数$a_i=attention(R,r_i)$，然后经过归一化后用于读取$M$得到最终的快权值
   $$
   W_i^*=softmax(a_i)^T M
   $$

1. 询问样本的预测：

   基本学习器$b$有了慢权值$w$和快权值$W^*$后，询问样本上的预测为
   $$
   \hat y_i=b(x_i,W_i,W_i^*)
   $$
   另外，这里的输入也可以用询问样本的嵌入$r_i$代替。最终询问样本的损失为
   $$
   loss_{train}(\hat y_i,y_i)
   $$
   整个网络的训练参数是$\theta=\{W,Q,Z,G\}$，用这个损失来更新参数$\theta=\theta-\nabla_\theta loss_{train}$。

   

# 持续学习

主要参考《An Introduction to Lifelong Supervised Learning》（作者Shagun Sodhani等）。

## 思路

考虑这样两种需求：1、模型建立之后，还会不断产生新的数据，用更多数据训练，理应产生更好的模型，那么如何利用新的数据修改已经建立好的模型呢？2、对于多任务模型，在让模型学习新任务的知识时，往往会忘记之前学习的知识，如何解决这样的灾难性遗忘？

以上两个问题以指向了同一个解决方案——找到合适的建模方法，能让模型在保有以往学习到的知识的前提下，输入新数据，学习到新知识。这就是持续学习希望做到的事情。

现在用数学的语言来描述：有一些分类任务，第$t$个任务（不妨就用$t$来记）的目标是将样本归入类别集合$C^{(t)}$中。假设当前已知的任务数为$T$，这些$C^{(t)}$便组成了总共的类别集合$\mathcal{Y}^{(T)}=\bigcup_{t=1}^TC^{(t)}$。同时，每个任务有样本集$D^{(t)}=\{(x_i,y_i)_{i=s}^{s+n_t}\}$，共$n_t$个样本。将所有任务的自变量组成的集合记为$\mathcal{X}$。与机器学习类似，我们的目的仍是找到合适的函数$f_T:\mathcal X \rightarrow \mathcal Y^{(T)}$，使得总经验风险最小，也即
$$
\hat R_T(f) = \frac1T \sum_{t=1}^T \frac1{n_t} 
\sum_{(x_i,y_i)\in D^{(t)}} L(f(x_i),y_i) \\
\hat f_T = \arg\min\hat R_T(f)
$$
其中$L(\hat y,y)$是损失函数。不过，值得注意的是，此时随着任务的逐渐到来，任务总数$T$是变化的，这意味着$\mathcal Y^T$要相应变化，所以$f_T$也应该有更新的能力。这是持续学习与普通机器学习不一样的地方。

根据给出任务的不同形式，持续学习又可以分为领域增量、任务增量、类别增量这三种。

领域增量学习：这种情况指的是，所有任务都有着相同的$C^{(t)}$，而不同之处只是$D^{(t)}$。在该模型下，输出并不需要区分输出所在的空间，使用“单头”的输出层，不需要识别任务的具体编号。

任务增量学习：这种情况指的是，所有任务都有着不相交的$C^{(t)}$，在输出时，要考虑当前任务的输出空间是哪个，也即使用“多头”的输出层。评估时，会用所有任务上的平均表现来衡量。

类别增量学习：这种情况所有任务都有着不相交的$C^{(t)}$，但在输出时，会同时考虑全部的类别，也即依然采用“单头”的输出层。