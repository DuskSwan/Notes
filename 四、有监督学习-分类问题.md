已知的数据被分为了若干个类别，我们希望判断新的样本属于已知的哪一类，这样的问题称为判别分析，或者分类问题。

[TOC]

# Logistic回归

**思路**

经典的Logistic回归只能用于二分类，将两个类别用$0$和$1$标记，我们用函数$\hat y=1+\frac1{e^{-W^Tx}}=1+e^{W^Tx}$来计算“样本$x$属于$1$类的概率”，显然，对$\hat y$做四舍五入就可以预测出$x$属于的类别了。其中$W^Tx$表示对$x$进行一次线性变换。只消求出合适的$W$，就得到了具体的模型。

**代码**

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
```

在实例化时，有以下初始化参数

+ `penalty`：正则化项，也称为惩罚项，可选参数为l1和l2，默认为l2。如果在调参时主要目的是解决过拟合，一般会选择l2正则化。但是当预测结果不好时，可选用l1正则化。
+ `C`：正则化系数$\lambda$的倒数，float类型，默认为1.0，必须是正浮点数类型，数值越小则反应正则化越强。
+ `fit_intercept`：是否拟合截距，默认为True，布尔类型。
+ `class_weight`：类别权重，可以是字典或者字符串，默认值为`None`也就是不考虑权重。如果选择`balanced`，会根据训练样本类别占比分配类别权重，某种类型的样本量越多，则权重越低，样本量越少，则权重越高。
+ `random_state`：随机种子，默认为None，仅在优化方法为sag或liblinear时有效。
+ `solver`：逻辑回归的优化方法。 liblinear：使用开源的liblinear库实现，使用坐标轴下降法来迭代优化损失函数。 lbfgs：拟牛顿法的一种，利用损失函数二阶导数也即Hessian矩阵来迭代优化损失函数。 newton-cg：利用损失函数二阶导数也即Hessian矩阵来迭代优化损失函数。 sag：随机平均梯度下降，与普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度。
+ `max_iter`：算法收敛的最大迭代次数，默认为100。仅在正则优化算法为newton-cg、sag和lbfgs时有效。
+ `multi_class`：面对多分类问题的分类方式，默认为'auto'

方法

+ fit(X,y)

# K近邻

**思路**

对于每个待预测样本，检视其距离最近的k个已知样本，由这些已知样本的标签投票决定待预测样本的标签，K近邻的思路就是如此简单。

要实现这一算法，有三个核心问题：1、如何计算“邻近”程度；2、如何选择k值；3、如何快速找到最近邻点。

第一个问题实际上就是采用何种方式度量距离，常见的距离有而是距离、曼哈顿距离、马氏距离、余弦相似度等等。对第二个问题，常常采用交叉验证等方法评估模型性能后确认。对第三个问题，在数据量较小时，可以遍历得到，数据量较大时，可以参考二分法的思路，将全空间划分为多个部分，快速比较。

# 距离判别

**思路**

假设已知的组别是$\pi_1,\pi_2,...,\pi_k$，对于新得到的样本$x$，计算它到每个类的距离$d(x,\pi_i)$，将它归于距离最近的一类。这个距离又通常定义为$x$到类均值$\mu_i$的（平方）马氏距离$(x-\mu_i)'\Sigma(x-\mu_i)$，其中$\Sigma$是这一类的协方差矩阵。

在实践中，考虑到样本量，既可以选择全部样本的协方差矩阵$S_p$，又可以选择第$i$类样本的协方差矩阵$S_i$来作为$\Sigma$的估计。一般而言，样本量小选择前者，样本量大选择后者。

# 贝叶斯判别

利用到了先验的信息，即可称之为贝叶斯判别。

**最大后验概率法**

对于归入不同类别的概率，我们往往先验的认识（比如，相对于一个小的类，样本更容易被归入很大的类）。所以，我们考虑在此基础上的，被归入不同类的概率。

假设已知的组别是$\pi_1,\pi_2,...,\pi_k$，每个组有概率密度$f_i(x)$（这要求我们对于每个类的分布有着假定，也即这是个参数化的方法）。已知任一样本被归入类$\pi_i$的概率为$p_i,i=1,2,...,k$。现在，新样本$x$已知时，它属于类$\pi_i$的后验概率为
$$
P(\text{归入}\pi_i|x)=\frac{p_if_i(x)}{\displaystyle\sum_{j=1}^k p_jf_j(x)}
$$
使得这个值最大的$\pi_i$当然就是$x$应该被归入的类。

**最小期望误判代价法**

除了先验的归类概率，将类别误判所造成的代价也会各不相同（比如将正常人诊断为有病还好，将病人诊断为正常则很糟）。因此我们希望误判代价大的情况尽量不发生。

假设已知的组别是$\pi_1,\pi_2,...,\pi_k$，每个组有概率密度$f_i(x)$（这要求我们对于每个类的分布有着假定，也即这是个参数化的方法）。任一样本被归入类$\pi_i$的概率为$p_i,i=1,2,...,k$。此外，我们还规定了将第类$\pi_i$误判为类$\pi_l$的“代价”为$c(l|i)$（$l=i$时当然代价是0）。

一旦我们给出判别规则“若$x\in R_i$，则归入类$\pi_i$”，就可以计算出将来自$\pi_i$的样本$x$判为$\pi_l$的条件概率为$P(l|i)=\int_{R_l}f_i(x){\rm d}{x}$，所以误判代价的均值就是$ \displaystyle\sum_{l=1}^k c(l|i) P(l|i)$，进而总误判代价的期望值为
$$
ECM=E\left[\sum_{l=1}^k c(l|i) P(l|i)\right]
=\sum_{i=1}^k p_i\sum_{\substack {l=1\\l\neq i}}^k c(l|i) P(l|i)
$$
我们需要选择合适的$R_1,...,R_k$来使得上式最小。可以证明，这样的判别规则是，使得$\sum_{\substack {j=1 \\ j \neq l}}^k p_j c(l|j) f_j(x)$最小的类$\pi_l$，就是$x$应该归入的。这个指标$\sum_{\substack {j=1 \\ j \neq l}}^k p_j c(l|j) f_j(x)$可以看作是归入第$\pi_l$类的平均代价，当然是代价越小越好。

# 决策树

**思路**

对于一些被划入$C$个类别的样本（其余特征则都是离散变量），我们希望按照一系列分类标准，将所有$C$个类别分开，这一系列分类标准呈现树的形状，每次判断后都走入一条分支，最终的“叶子结点”就是归属的类别。下图是一个例子：

![](.\img\决策树.jpeg)

假设每个样本$x_i=(x_{i1},...,x_{ip})$有$p$个特征，每个样本属于$k$个类别中的一种。最开始，全部样本同属于根节点。在每个节点$G$处（不妨设此节点处有$m$个样本），希望选择某个特征作为分类依据，将样本按照该特征的值分割为多个子节点（为了衡量选择的优劣，需要考量节点的混乱度）。每次分割都可能使得节点更混乱，穷尽所有可能的分类依据，使得混乱度增加最少的那个分类依据，就是好的分类依据。

> 所谓的“混乱”，指的是这样的概念：对于一个节点（样本空间），其中的样本（变量）可能会属于多个类别（样本点），如果所有样本都是同一类，那么任抽一个样本，它的类别归属是唯一的，归类结果确定；如果这些样本大多属于一类，剩下的零散分布在其它类中，那么任抽一个样本，我们可以说它大概率属于最多的那一类，归类结果有倾向；如果抽取的样本等概率地落入多个类别，那么归类的结果最混乱、最难以预测。显见，这三种局面的不确定程度是逐渐增大的。为了衡量结果的不确定程度，我们希望用某个函数$I(G)$来描述节点$G$的不确定性，用$I(G|X_j,f)$来描述选取第$j$特征、依分割标准$f$分割之后，新产生的子节点的不确定性总和，二者的差$I(G|X_j,f)-I(G)$就是“混乱度的增量”。
>
> 这种“不确定”，其含义与混乱、均匀、不纯同质；相对的则是确定、有序、不均、纯粹。我们的目标是将各类别区分开，所以越有序、越不混乱更好，也即混乱度增加越小越好。数学形式的函数会写在“细节”部分。

依次（此处应该可以有多种顺序，比如按照节点产生的顺序，按照树的前序遍历等等）按照上面的方式划分树的节点，直到满足某个标准（比如叶子结点总数够多，树的深度够大，叶子的样本量够小，叶子的纯度够高等等）才停止。现在，全部的数据可以用树状结构来储存，每个叶子节点中样本类别的众数，就是该叶子节点被判为的类别。对于一个新样本，只要判断它应该属于哪个叶子节点，就能判别它的类别。

**细节与改进**

1、不确定性的度量函数

假设在节点$G$处，有$m$个样本，分别属于$C$个类别$c_1,c_2,...c_C$。每个样本有$p$个特征，用$X_1,X_2,...,X_p$来表示。对于第$j$个特征，它有$m_j$个可能的取值，对于连续型特征，样本取值通常不会重复，$m_j$通常达到$m$；而对于离散型变量，其类别数通常小于样本数，也即$m_j<m$。将$X_j$可以取的值记为$x_1^{(j)},x_2^{(j)},...,x_{m_j}^{(j)}$。

用$p(c_i|G)$记录属于$G$处属于类$c_i$的样本占$G$ 处总样本数的比例（频率），这实际上是对$x\in c_i$这件事的发生率的估计。显然有$\sum\limits_{i=1}^C p(c_i|G)=1$。

有以下几种方法定义节点划分前后的”不确定性“（或者叫不纯度、混乱度、均匀度）：

（1）信息熵

用
$$
\begin{align}
I(G) = -\sum_{i=1}^{C} p(c_i|G)\log_2(p(c_i|G)) \\
\end{align}
$$
衡量节点$G$分割前的不确定性，称为（信息）熵不纯度。这实际上就是用频率替换概率代入了信息熵的公式。

显见，当节点中的类别均匀分布，信息熵取最大值$\log_2C$；当节点中仅有一个类别，信息熵取最小值$0$。值得注意的是，式中默认了$0\log_20=0$，这实际上是一个极限过程。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\sum_{k=1}^K\frac{m_k}m I(G_k)
$$
来表示。用信息熵的下降值$ I(G)-I(G|X_j,f)$来衡量分割的好坏（可以证明分割一定会使信息熵下降），下降值越大，则分割后越有序，则越好。该下降值称为信息增益（InfoGain)。

实践中，信息增益倾向于将节点分割得很细、每个叶子节点都是单一类的（纯粹的），这会导致过拟合问题。对此的一个改进是，用“分裂信息量”来限制分裂出的节点数。

假设节点$G$分裂成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则定义分裂的信息量为
$$
\text{SplitInfo}=-\sum_{k=1}^K \frac{m_k}{m} \log_2 (\frac{m_k}{m})
$$
可以看出，$K$越大，这个值也会越大。用信息增益率
$$
\text{InfoGainRatio}
=\frac{\text{InfoGain}}{\text{SplitInfo}}
=\frac{ I(G)-\sum\limits_{k=1}^K\frac{m_k}m I(G_k)}{-\sum\limits_{k=1}^K \frac{m_k}{m} \log_2 (\frac{m_k}{m})}
$$
代替信息增益作为衡量分割优劣的依据，就能避免分割出过多的叶子结点。

（2）Gini指数

节点$G$处的Gini指数为
$$
I(G)=\operatorname{Gini}(G)
=\sum_{i=1}^C p(c_i|G) \big(1-p(c_i|G)\big)
=1-\sum_{i=1}^C \left[p(c_i|G)\right]^2
$$
显见，当节点中的类别均匀分布，Gini指数取最大值$1-\frac1C$；当节点中仅有一个类别，Gini指数取最小值$0$。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\operatorname{Gini}_{\text{split}}(G)
=\sum_{k=1}^K\frac{m_k}m \text{Gini}(G_k)
$$
来表示。由于Gini指数是小于$1$的，加权平均数一定会比原来更小，所以变化量是负值。我们于是用Gini指数的下降值$ I(G)-I(G|X_j,f)$来衡量分割的好坏，下降值越大，则分割后越有序，则越好。

（3）误分率

节点$G$处的误分率定义为
$$
I(G)=\text{Error}(G)
=1-\max\limits_{1\leqslant i\leqslant C} \{ p(c_i|G) \}
$$
可见，一个节点中如果某一类占的比重很大，误分率就会比较小；如果各个类比较均匀，误分率就比较大。所以这也是衡量混乱度的指标。

假设依据特征$X_j$、按照某种规则$f$将$G$分割成了$K$个子节点$G_1,G_2,...,G_K$，其样本容量分别是$m_1,m_2,...,m_K$，则它们的总混乱度用加权平均数
$$
I(G|X_j,f)
=\operatorname{Error}_{\text{split}}(G)
=\sum_{k=1}^K\frac{m_k}m \text{Error}(G_k)
$$
来表示。

（4）方差

前述的度量都针对离散型目标（这是自然的，因为决策树的目的就是分类）。但实际上，即使目标是连续的，我们也可以通过划分连续变量的分布区间来使之离散化，用一个代表性的值（比如均值）来作为这个区间所有样本的目标特征（标签、响应变量）。从这个角度看，决策树也可以用于做回归，此时可以称之为回归树。

针对连续型目标，无法使用前述的度量，所以我们考虑用方差衡量节点分裂前后的混乱度。

具体来说，假设在节点$G$处，有$m$个样本，它们的目标特征分别是$y_1,y_2,...,y_m$（一般来说不会出现重复）。那么节点$G$处的混乱程度就是$(y_1,y_2,...,y_m)$的方差$I(G)=V(y)=\displaystyle\frac{1}{m-1}\sum_{i=1}^m(y_i-\bar y)^2$。

针对连续型目标，应该选择一个特征的一个取值作为阈值，将节点分裂成两个子节点。分裂后的“总方差”是与上同理的加权平均值还是直接加和？暂时不知。我认为两种方式的效果应该是相同的。总之，分裂之后的“总方差”应该小于分裂前大方差，减小值越多，则这个分裂点选择越优。

2、划分规则

对一个节点$G$，选定特征$X_j$，如何选择哪些样本进入哪些子节点、总共多少个子节点？划分的具体规则到底是什么呢？

最初提出的决策树算法ID3（Iterative Dichotomiser 3）只能处理离散型特征与离散型标签（目标、响应变量）。选定特征$X_j$（它有$m_j$个可能的取值）之后，样本们的$X_j$取值为$x_1^{(j)},x_2^{(j)},...x_{m_j}^{(j)}$的各自划入一个子节点，共$m_j$个子节点。使用信息熵来度量混乱度。

接下来出现了算法C4.5，它在面对离散型特征时，依然采取如上的“一值一类”的拆分规则；在面对连续型特征时，则从$X_j$可能取到的值中选一个作为分裂阈值，如果选了$x_i^{(j)}$，则将$X_j\leqslant x_i^{(j)}$的样本归入一个节点，其余的$X_j > x_i^{(j)}$的样本放入另一个节点，仅有两个子节点。现在，C4.5既可以处理离散型特征，又可以处理连续型特征了，但是目标依然只能是离散的（分类变量）。此外，C4.5还用信息增益率代替了信息增益来衡量分裂的优劣。

新的方案是CART（Classification And Regression Tree），不论是按照离散型特征还是连续型特征，它都只分裂为两个节点。连续型特征的处理方案与C4.5相同；针对离散型特征，则只分为$X_j= x_i^{(j)}$和$X_j\neq x_i^{(j)}$这两组。此外，CART针对离散型目标使用Gini指数，针对连续型目标使用方差，来度量混乱度。

3、优化方法-剪枝

为了避免决策树太过复杂，产生过拟合，我们希望减除一些不必要的分裂。主要有预剪枝和后剪枝两种思路。

预剪枝会设定一个“混乱度下降阈值”，如果某个节点在分裂后的混乱度下降值不超过阈值，则认为这个分裂没必要进行，直接将该节点视为叶子结点即可。

后剪枝通过指标——整体损失函数，来衡量树的优劣。越小的整体损失意味着树越好。先建立完整的树，考虑是否要进行一些剪枝操作。如果剪枝之后的整体损失函数变小，就说明这次剪枝可取，否则不可取。遍历全部的剪枝方案，最终可以让整体损失函数达到（局部）最小。

假设一棵树$T$共有$|T|$个节点，记为$G_1,G_2,...,G_{|T|}$，它们所蕴含的样本数分别是$n_1,n_2,...,n_{|T|}$，选定某种混乱度度量函数$I(G)$，那么这棵树的整体损失函数（也可称复杂性代价）定义为
$$
R_\alpha(T)=\text{Cost}_\alpha(T)
=\sum_{t=1}^{|T|} n_t I(G_t) +\alpha|T|
$$
其中的$\alpha$是复杂度控制参数，$\alpha$越大，越倾向于选择节点数少的树。

**代码**

1、分类树

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

```python
#签名
class sklearn.tree.DecisionTreeClassifier(*, 
	criterion='gini', 
	splitter='best', 
	max_depth=None, 
	min_samples_split=2, 
	min_samples_leaf=1, 
	min_weight_fraction_leaf=0.0, 
	max_features=None, 
	random_state=None, 
	max_leaf_nodes=None, 
	min_impurity_decrease=0.0, 
	class_weight=None, 
	ccp_alpha=0.0)
#样例
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
```



超参数：

​	criterion：不纯度度量。默认'gini'即使用Gini指数度量。还可以选择'entropy'即信息熵。

​	max_depth：树的最大深度。如果不指定，则会一直计算直到叶子节点全部纯净、或者内部样本量足够小。

​	min_samples_leaf：叶子节点的最小样本数。如果是整数则直接指定；如果是小数，则视为是占全体样本的比例（也即用`ceil(min_samples_leaf * n_samples)`作为最小样本数）

​	min_impurity_decrease：不纯度下降的最大值。用于在预剪枝中筛选优秀子树。

​	ccp_alpha：整体损失函数的参数$\alpha$。这一参数需要sklearn版本0.22才有。

属性：

方法：

​	fit(X,Y)：训练。其中X应该是记录了特征的二维数组，Y是记录了标签的一维数组。

其他操作：

```python
#绘制分类树
>>> tree.plot_tree(clf) 

#以文本形式输出树
>>> r = export_text(clf, feature_names=df['feature_names'])
>>> print(r)
|--- petal width (cm) <= 0.80
|   |--- class: 0
|--- petal width (cm) >  0.80
|   |--- petal width (cm) <= 1.75
|   |   |--- class: 1
|   |--- petal width (cm) >  1.75
|   |   |--- class: 2
```

2、回归树

签名：

```Python
class sklearn.tree.DecisionTreeClassifier(criterion='mse'
                                          ,splitter="random"
                                          ,max_depth=None
                                          ,min_samples_split=2
                                          ,min_samples_leaf=1
                                          ,min.weight_fracton_leaf=0.0
                                          ,mac_features=None
                                          ,random_state=None
                                          ,max_leaf_nodes=None
                                          ,min_impurity_decrease=0.0
                                          ,min_impurity_split=None
                                          ,presort=False
                                         )

```

超参数：

（大多数都和决策树含义相同）

​	criterion：回归树衡量分枝质量的指标，支持的标准有三种。"mse"使用均方误差；"friedman_mse"使用费尔德曼均方误差，这种指标是用费里德曼对潜在分支中的问题改进后的均方误差；输入 "mae"使用绝对平均误差MAE（mean absolute error）

方法：

​	fit(X,Y)：训练。其中X应该是记录了特征的二维数组，Y是记录了标签的一维数组。

​	score(*X*, *y*)：针对测试集X、和真实的标签y，计算判定系数。

​	

# 随机森林

**思路**

如果只有一棵决策树，难免出现过拟合或者欠拟合的情况，结果的偏差在所难免。但如果有很多棵相互无关的决策树，各自独立地做出判断，投票得出判别结果，就大大降低了出差错的风险。这样由多棵树共同组成的分类模型，就成为森林。而为了让决策树之间互不相关，每棵树只随机抽取一部分样本来建立，于是称为随机森林。在实践中，也可以建立在其他各种分类器上（而不是必须依赖决策树）。

**代码**

1、随机森林分类器

在该随机森林中，使用决策树作为基本分类单元。

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=forest#sklearn.ensemble.RandomForestClassifier

签名：

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, 
                                              criterion='gini', 
                                              max_depth=None, 
                                              min_samples_split=2, 
                                              min_samples_leaf=1, 
                                              min_weight_fraction_leaf=0.0, 
                                              max_features='auto', 
                                              max_leaf_nodes=None, 
                                              min_impurity_decrease=0.0, 
                                              bootstrap=True, 
                                              oob_score=False, 
                                              n_jobs=None, 
                                              random_state=None, 
                                              verbose=0, 
                                              warm_start=False, 
                                              class_weight=None, 
                                              ccp_alpha=0.0, 
                                              max_samples=None)
```

超参数：

​	n_estimators：森林中决策树的数量。

​	criterion：不纯度度量。可以是Gini指数'gini'或者信息熵'entropy'。

​	max_depth：决策树的最大深度。

​	max_features：建立每棵树所采用的最大特征值数。传入整数直接表明数量；浮点数表示占全部特征的比例；None表示使用全部特征；“auto”等价于“sqrt”表示使用全部特征数的平方根；“log2”表示使用全部特征数的以二为底的对数。

​	min_samples_split：待分裂节点的最小样本数。样本数小于该节点的不会再分裂。

​	min_samples_leaf：决策树的叶子节点的最小样本数。如果是整数则直接指定；如果是小数，则视为是占全体样本的比例（也即用`ceil(min_samples_leaf * n_samples)`作为最小样本数）

​	max_leaf_nodes：决策树的叶子节点的最大数目。用于在后剪枝中控制树的复杂度。

​	min_impurity_decrease：不纯度下降的最大值。用于在预剪枝中筛选优秀子树。

​	bootstrap：是否抽样。如果是，则每次抽样建立决策树；如果否，则每次用全部数据建立决策树。

​	oob_score：仅在需要抽样（bootstrap=True）时生效。是否要使用抽样样本之外的样本评估泛化得分。

​	random_state：在涉及的随机函数中充当随机种子。如果传入一个整型，就拿它当做随机种子。此外也可以传入一个numpy.random.RandomState对象。

​	ccp_alpha：整体损失函数的参数$\alpha$。这一参数需要sklearn版本0.22才有。

​	max_samples：仅在需要抽样（bootstrap=True）时生效。设定每棵树在抽样时的样本数。默认等于全部样本数；传入整数则直接使用；传入小数则视为占全部样本数的比例。这一参数需要sklearn版本0.22才有。



属性：

​	estimators_：由树组成的列表。每棵树是一个DecisionTreeClassifier对象。

​	classes_：目标特征。也即类别组成的数组。

​	oob_score_：该属性仅在oob_score=True时产生。是使用抽样样本之外的样本估计获得的训练数据集的分数。



方法：

​	fit(*X*, *y*)：根据样本训练模型。

​	apply(*X*)：针对数据集X，返回每棵树对每个样本的拟合结果。以(n_samples, n_estimators)形式的数组给出。

​	predict(*X*)：针对数据集X进行预测。

​	predict_proba(*X*)：针对数据集X，返回每个样本属于各个类别的概率。以(n_samples, n_classes)形式的数组给出。

​	score(*X*, *y*)：针对测试集X、和真实的标签y，计算预测的平均正确率。



# 支持向量机SVM

支持向量机（Support Vector Machine, SVM）通过核函数将低维数据映射成高维，然后确定最优的划分超平面。能够处理在低维空间中难以分割的情况。

**思路**

假设数据集为$D=\{(x_i,y_i)\}$，其中样本量为$n$，$x_i$是$d$维特征，$y_i$仅为$1$或$-1$，表示正类与负类。我们的目标是找到一个超平面（假设$d=2$，那么$x_i$就是平面上的点，此时就是找一条直线）将正负两类样本分开，而且这两类样本之间的“距离”最大。

为了描述两类的“距离”，首先定义间隔。对于一个确定的超平面$w^Tx+b=0$，它关于样本点$(x_i,y_i)$的函数间隔就定义为
$$
\hat \gamma_i=y_i \cdot (w^T x_i+b)
$$
假设超平面的两侧分别是预测的正类与负类，那么预测正确的样本与预测错误的样本，其函数间隔的正负号不同，且函数间隔的数值越大，预测的确信度（也即分类正确或错误的程度）也就越大。我们不妨将$w^T x_i+b>0$一侧的样本点预测为正类，另一侧为负类。此时函数间隔为正的，代表预测正确，为负的代表预测错误。而对于预测正确的样本点，函数间隔的值越大，就说明样本点距离超平面越远。

由于超平面的表达式不唯一，为了消除系数对间隔大小的影响，我们定义几何间隔为
$$
r_i=\frac{\hat \gamma_i}{\|w\|_2}=\frac{y_i \cdot (w^T x_i+b)}{\|w\|_2}
$$
此时，每个样本的几何间隔，就是该样本到目标超平面的垂直距离。现在，我们要找到使得几何间隔尽量大的超平面。

在超平面两侧，都会有距离超平面最近的点，称为支持向量。目标超平面将由这些支持向量决定（而不是全部的样本）。使得训练样本到超平面的最小距离（也即支持向量到目标超平面的距离）最大化，这样的超平面就是目标超平面。用数学语言描述，就是求$w,b$达到
$$
\max\limits_{w,b} \min\limits_{x_i} \frac{y_i \cdot (w^T x_i+b)}{\|w\|_2}
$$
不妨令支持向量满足$y_i(w^T x_i+b)=1$，该问题其实等价于求
$$
\min\limits_{w,b} \frac{1}{2}\|w\|^2_2,\ \text{ s.t. } \ y_i(w^T x_i+b)\geqslant1
$$
这是一个带有线性约束的二次规划问题，可以用牛顿法等数值方法解决。

在以上过程中，都没有出现“升维”的操作。实际上这一步是在算法中，通过修改核函数体现出来的。

**算法**

实际中，往往转换成对偶问题来解决。首先用拉格朗日乘子法将原问题变为无约束的最优化问题：令拉格朗日函数为$L(w,b,\alpha)=\frac{1}{2}\|w\|^2_2-\sum\limits_i \alpha_i(y_i(w^T x_i+b)-1)$，那么问题转化为
$$
\min\limits_{w,b} \max\limits_{\alpha_i\geqslant0} L(w,b,\alpha)
$$
它的对偶问题就是求
$$
\max\limits_{\alpha_i\geqslant0} \min\limits_{w,b} L(w,b,\alpha)
$$
通常对偶问题的极值，会超越原问题的极值，为了确保对偶问题和原问题的解相同，需要满足斯莱特条件（此时对偶问题与原问题的解相同，称为强对偶性）。由于我们的原问题是凸二次规划，已经满足了斯莱特条件。

接下来，还需要满足KKT条件（在强对偶性下，它是最优解的必要条件），才能求出最优解。在这个问题里，KKT条件中的一部分已经天然满足，还需要补充的是
$$
\begin{align}
\frac{\partial L(w,b,\alpha)}{\partial w} & =w-\sum\limits_i\alpha_iy_ix_i=0 \\
\frac{\partial L(w,b,\alpha)}{\partial b} & =-\sum\limits_i\alpha_iy_i=0 
\end{align}
$$
将$w=\sum\limits_i\alpha_iy_ix_i$代入拉格朗日函数，则最优化问题等价于
$$
\begin{align}
\max\limits_{\alpha_i\geqslant0} &&& \sum_{i}^n \alpha_i-\frac12 \sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i^Tx_j), \\
\text{s.t.} &&& \sum_i^n\alpha_iy_i=0,\ \alpha_i\geqslant0.
\end{align}
$$
现在，求解目标转变成了$\alpha_1,...,\alpha_n$。为了解决这个转化后的问题，微软研究院的John C. Platt提出了SMO算法。其基本思路是，初始化全部$\alpha_i$后，选择两个$\alpha_j,\alpha_k$，固定其他变量，关于这两个变量求解最优化问题，获得更新后的$\alpha_j,\alpha_k$。这样反复更新$\alpha_i$们，直到结果收敛。如此便可以求出$\alpha_1,...,\alpha_n$。

这之后，利用$w=\sum\limits_i\alpha_iy_ix_i$可以求出$w$；由支持向量满足$y_i(w^T x_i+b)=1$可以解出$b$。这就得到了原问题的解。

原问题的时间复杂度为$O(d^3)$，$d$是特征维数，适用于特征维数较小的情况；对偶问题时间复杂度为$O(n^3)，$适用于样本数较少的情况。

**改进-核函数**

在一些局面下，原本的样本并不是线性可分的，考虑将它们映射到高维空间来变成线性可分。这称为使用核方法。

在思路中，只需要把样本$x$用升维后的$\phi(x)$代替即可。但我们注意到，计算对偶问题的过程中，涉及$x$的部分实际只有计算内积$x_i^Tx_j$，所以只需要把$x_i^Tx_j$改换形式成$\varphi(x_i,x_j)$即可。

通常用的有如下核函数

| 核函数类型   | 定义                                       | 说明               |
| ------------ | ------------------------------------------ | ------------------ |
| 多项式       | $(x_1^Tx_2+1)^d$                           | 正整数$d$代表次数  |
| 高斯函数     | $\exp(-\frac{\|x_1-x_2\|^2_2}{2\sigma^2})$ | $\sigma>0$         |
| 拉普拉斯函数 | $\exp(-\frac{\|x_1-x_2\|_2}{\sigma})$      | $\sigma>0$         |
| sigmoid      | $\tanh(\beta x_1^T x_2+\theta)$            | $\beta>0,\theta<0$ |

**改进-软间隔**

在实际中，样本未必真的线性可分，即使用核方法强行分开了，也可能是过拟合。我们需要在一定程度上容忍“错判”的样本，来保证结果的可靠性。所以，我们允许一些数据跨越分类超平面，去往错误的类别，只是对它们施加一定的惩罚。这就是“软间隔”（soft margin）的思想。

在计算时，优化目标改变为
$$
\min\limits_{w,b,\xi} &&& \frac{1}{2}\|w\|^2_2+C\sum_i l(y_i(w^T x_i+b)-1), \\
$$
其中的$ l(z)=I_{\{z<0\}}$，也即$y_i(w^T x_i+b)<1$将计入惩罚。

由于$I_{\{z<0\}}$是非凸、非连续的，所以我们希望用其他函数来作为$l(z)$。最常用的软间隔支持向量机采用了hinge损失：$l(z)=max(0,1-z)$。此时令$\xi_i=l(y_i(w^T x_i+b)-1)$，最初的问题就相当于
$$
\begin{align}
\min\limits_{w,b,\xi} &&& \frac{1}{2}\|w\|^2_2+C\sum_i \xi_i, \\
\text{s.t.} &&& y_i(w^T x_i+b)\geqslant1-\xi_i,\ \xi_i\geqslant0,&i=1,2,...,n.
\end{align}
$$
之后的推导方法完全相同，最终得到的对偶问题变为
$$
\begin{align}
\max\limits_{\alpha_i\geqslant0} &&& \sum_{i}^n \alpha_i-\frac12 \sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i^Tx_j), \\
\text{s.t.} &&& \sum_i^n\alpha_iy_i=0,\ 0\leqslant\alpha_i\leqslant C,\ i=1,2,...,n.
\end{align}
$$
同样可使用SMO算法求解。

**支持向量回归**

支持向量回归（Support Vector Regression, SVR）将支持向量机的思路用于回归。对数据集$D=\{(x_i,y_i)\}$，希望得到一个线性模型$ \hat y=f(x)=w^Tx+b$来描述。（实际上通常会使用核方法，得到的是非线性函数$f(x)=w^T\phi(x)+b$）

在进行预测时，允许预测值与真实值之间有$\varepsilon$的误差，也即，只有$f(x)$与$y$之间的差的绝对值大于$\varepsilon$时才计算损失。SVR的目的便是求
$$
\min\limits_{w,b} \frac12 \|w\|^2+C\sum_{i=1}^n l(y-f(x))
$$
其中
$$
l(z)=\begin{cases}
0 & |z|\leqslant\varepsilon \\
|z|-\varepsilon & |z|>\varepsilon
\end{cases}
$$
是损失函数。$C$是正则化参数。

我们注意到，这个求解目标与软间隔支持向量机的形式完全类似，同样是转化成对偶问题后求解。推导可以参考https://zhuanlan.zhihu.com/p/76609851

**代码**

1、SVR

官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR

```Python
#签名
class sklearn.svm.SVR(*, kernel='rbf', 
                      degree=3, 
                      gamma='scale', 
                      coef0=0.0, 
                      tol=0.001, 
                      C=1.0, 
                      epsilon=0.1, 
                      shrinking=True, 
                      cache_size=200, 
                      verbose=False, 
                      max_iter=- 1)
#范例
>>> from sklearn.svm import SVR
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> import numpy as np
>>> n_samples, n_features = 10, 5
>>> rng = np.random.RandomState(0)
>>> y = rng.randn(n_samples)
>>> X = rng.randn(n_samples, n_features)
>>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))
>>> regr.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svr', SVR(epsilon=0.2))])
```



超参数：

​	kernel：核函数。默认使用rbf（径向基函数），还可以是‘linear’, ‘poly’, ‘sigmoid’, ‘precomputed’或者手动给出的可调用对象（callable）。

​	degree：如果使用多项式核函数，该参数用于指定多项式次数。

​	gamma：如果使用径向基函数、多项式函数、Sigmoid函数作为核函数，需要该参数。

​	C：惩罚函数的正则化参数。必须是正值。默认1。

​	epsilon：预测的允许误差限。预测误差小于这个值的样本点，不会计入损失。

​	max_iter：求解时的迭代次数限制。默认为-1也即无限制。

​	tol：迭代的停止精度。默认1e-3。

属性：

​	support_vectors_：支持向量。以矩阵形式给出，每行是一个支持向量。

方法：

​	fit(*X*, *y*, *sample_weight=None*)：训练模型。

​	get_params()：给出系数。

​	predict(*X*)：预测。

​	score(X, y, sample_weight=None)：计算判定系数。

# Boosting

XGBoost是一种集成学习算法，属于三类常用的集成方法（Bagging，Boosting，Stacking）中的Boosting算法的一种。它是一个加法模型，基模型一般选择树模型，但也可以选择逻辑回归模型。

XGBoost属于梯度提升树(GBDT)模型的改进算法，GBDT的基本想法是让新的基模型去拟合前面模型的偏差，从而不断将加法模型的偏差降低。相比于经典的GBDT，XGBoost做了一些改进，从而在效果和性能上有明显的提升。

**代码**

```python
from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators=50, learning_rate=0.1,
                    gamma=0.65, random_state=10,
                    eval_metric=['logloss','auc','error'])

```

从xgboost库加载sklearn接口类`XGBClassifier`，可以构建XGBoost模型，然后再将划分好的训练集`X_trian`，`y_train`带入模型中，调用`fit`方法进行模型训练。

`XGBClassifier`模型主要参数如下：

+ booster：gbtree 树模型做为基分类器（默认），gbliner 线性模型做为基分类器
+ scale_pos_weight：正样本的权重，在二分类任务中，当正负样本比例失衡时，设置正样本的权重，模型效果更好。例如，当正负样本比例为1:10时，scale_pos_weight=10。

+ n_estimatores：基模型的个数
+ early_stopping_rounds：在测试集上，当连续n次迭代，评价分数没有提高后，提前终止训练，防止过拟合。
+ max_depth：树的深度，默认值为6，典型值为3-10，值越大，越容易过拟合；值越小，越容易欠拟合。
+ min_child_weight：最小叶节点样本权重，默认值为1，值越大，越容易欠拟合；值越小，越容易过拟合。
+ subsample：训练每棵树时，使用的数据占全部训练集的比例，默认值为1，典型值为0.5-1，防止过拟合。
+ colsample_bytree：训练每棵树时，使用的特征占全部特征的比例，默认值为1，典型值为0.5-1，防止过拟合。

+ learning_rate：学习率，控制每次迭代更新权重时的步长，默认为0.3，值越小，训练越慢，典型值为0.01-0.2。
+ objective 目标函数： 回归任务：reg：linear (默认)或logistic 。 二分类： binary:logistic（概率）或logitraw（类别）。 多分类：multi：softmax num_class=n（返回类别）softprob num_class=n（返回概率）。rank：pairwise 。
+ eval_metric： 评价指标。针对回归任务(默认rmse)有rmse均方根误差，mae平均绝对误差。 分类任务(默认error)，auc-roc曲线下面积，error错误率（二分类），merror错误率（多分类），logloss负对数似然函数（二分类），mlogloss负对数似然函数（多分类）。
+ gamma：惩罚项系数，指定节点分裂所需的最小损失函数下降值。
+ alpha：L1正则化系数，默认为1。
+ lambda：L2正则化系数，默认为1。