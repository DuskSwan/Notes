真实数据集往往有很多特征，体现在数学上就是每组样本会有很高的维度，这会为数学计算带来很大的难度。为了解决这一问题，我们希望针对每组样本，用低维的统计量$(y_1,y_2,\cdots,y_m)$代替高维的原数据$(x_1,x_2,\cdots,x_p)$。那么如何计算出这样的$(y_1,y_2,\cdots,y_m)$？这就是降维方法要解决的问题。

# 主成分分析

### 思路

降维，也就是针对每组样本，用低维的统计量$(y_1,y_2,\cdots,y_m)$代替高维的原数据$(x_1,x_2,\cdots,x_p)$。对随机变量$x=(x_1,x_2,\cdots,x_p)'$，考虑让每个$y_i$都与$x$有线性关系，也即$y_i=a_i'x$，其中每个$a'_i$是列向量。

考虑这样的条件：首先，为了使得$y_i$保留$x$的信息，应该让$y_i$的方差（在$x$的一切线性组合中）最大化；其次，为了使得主成分之间有可比性，需要限制$a_i$为单位向量；最后，为了避免$y_i$之间信息产生重复，应该使它们两两之间（实际上只需每一个与之前的相比）协方差为$0$。

由于$D(y_i)=D(a'_ix)=a_i'\Sigma a_i$，所以（可以证明）满足以上条件的$a_i$的解就是随机变量$x$的协方差矩阵$\Sigma$的全部单位正交特征向量，而且要按照对应特征值的降序排列。也即，假设$\Sigma$的特征值为$\lambda_1 \geqslant \lambda_2 \geqslant \cdots \geqslant \lambda_p $（可以证明这些特征值都是非负的），那么它们对应的单位正交特征向量$t_1,t_2,\cdots,t_p$就是满足我们需求的$a_1,a_2,\cdots,a_p$（实践中常常只取前几个）。由此得到的$y_i=t_i'x$就称为第$i$主成分。写成矩阵形式，就是
$$
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\y_p
\end{pmatrix} =
\begin{pmatrix}
t_{11} & t_{21} & \cdots & t_{p1} \\
t_{12} & t_{22} & \cdots & t_{p2} \\
\vdots & \vdots & \ddots & \vdots \\
t_{1p} & t_{2p} & \cdots & t_{pp} \\
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\x_p
\end{pmatrix} =
\begin{pmatrix}
t_1' \\ t_2' \\ \vdots \\ t_1' \\
\end{pmatrix} x = T'x
$$
其中$T$是以$t_1,t_2,\cdots,t_p$为列的正交矩阵。

考虑到实际数据的尺度差异会比较大，而分量之间的数值大小差异过大（因而方差也差异过大）时，主成分很大程度上受数值大的分量影响，数值小的分量会被忽略。为了修正，我们先将每个分量（特征）标准化。最常用的标准化方式是正态化，也即$x_i^*=\displaystyle\frac{x_i-\mu_i}{\sqrt{\sigma_{ii}}}$，这时$x^*=(x_1^*,x_2^*,\cdots,x_p^*)'$的协方差矩阵正是原本的$x$的相关矩阵$R=\operatorname{cor}(x)$，用$R$替换$\Sigma$即可计算出改进的主成分$y^*=(y_1^*,y_2^*,\cdots,y_p^*)'$，相应的特征值与特征向量记为$\lambda_1^*,\lambda_2^*,\cdots,\lambda_p^* $与$t_1^*,t_2^*,\cdots,t_p^* $。

### 性质

为了衡量主成分对原本数据的描述效率，需要研究主成分的性质，包括以下内容：

1. （主成分贡献率）各个主成分$y_1,y_2,\cdots,y_p$互不相关，而它们的方差（也即$\lambda_j$）之和恰等于$x$的协方差矩阵$\Sigma$的对角元之和。由此可见，协方差矩阵的迹是变换前后的不变量。所以我们用$  \lambda_i/\displaystyle\sum_{j=1}^p\lambda_j$来衡量第$i$主成分的解释效率，称之为贡献率。

1. （对分量的贡献率）原始数据$x$的分量$x_i$与主成分$y_j$之间的相关系数可以由关系$x_i=\displaystyle\sum_{j=1}^pt_{ij}y_j$得出，计算可得
   $$
   \operatorname{Cov}(x_i,y_j)=\operatorname{Cov}(t_{ij}y_j,y_j)=t_{ij}\lambda_j \\
   \rho(x_i,y_j)=\frac{\operatorname{Cov}(x_i,y_j)}{\sqrt{V(x_i)}\sqrt{V(y_j)}}
   	=\frac{\sqrt{\lambda_j}}{\sqrt{\sigma_{ii}}}t_{ij}
   $$
   又由于$x_i$是$y_1,y_2,\cdots,y_p$的线性组合，所以复相关系数$\rho_{i\cdot1,2,\cdots,p }=1$，复相关系数的平方又是每个分量的相关性系数的平方和，所以$\rho_{i\cdot1,2,\cdots,p }^2=\displaystyle\sum_{j=1}^p\rho^2(x_i,y_j)=1$。这启示我们，如果要考虑前$m$个主成分$y_1,y_2,\cdots,y_m$对$x_i$的反映程度，可以用$\rho_{i\cdot1,2,\cdots,m }^2=\displaystyle\sum_{j=1}^m\rho^2(x_i,y_j)=\displaystyle\sum_{j=1}^m\frac{\lambda_jt_{ij}^2}{\sigma_{ii}}$来衡量，这称为前$m$个主成分对$x_i$的贡献率。

1. （主成分的决定因素）主成分$y_j$与原始变量$x_i$之间有关系$y_j=\displaystyle\sum_{i=1}^pt_{ij}x_i$，称$t_{ij}$为第$j$主成分在第$i$原始变量上的载荷。还可证明$\sigma_{ii}=\displaystyle\sum_{j=1}^pt_{ij}^2\lambda_j$，这说明$x_i$的方差$\sigma_{ii}$是$y_j$的方差$\lambda_j$的加权平均（因$T$是正交矩阵），于是较大的$y_i$与方差较大的$x_i$相关性更强，较小的$y_i$与方差较小的$x_i$相关性更强。当这种效应十分明显时，我们会注意到主成分几乎就是原始数据按方差大小进行的重新排列。

1. （主成分揭露共线性）当$\lambda_j$非常小（接近$0$）时，可以认为$y_j$是常数，这又意味着$\displaystyle\sum_{i=1}^pt_{ij}x_i$是常数，也即$x_i$之间存在共线性。这可以看作一种检查共线性、筛选变量的方法。

1. （相关系数矩阵下的主成分）考虑使用标准化修正后的主成分，上述性质会发生改变，主要体现在新的$x_i$的方差$\sigma_{ii}^*$会变成$1$，上述公式中的$\lambda_j$与$t_j$也相应地用$\lambda_j^*$与$t_j^*$代替。这样做完全等效于使用相关系数矩阵来代替协方差矩阵计算主成分。由于方差的差异被消除了，上述的3.4.点不再体现出来。

### 算法

以上所描述的，均是某个确定总体下的主成分。而实际上我们是在用样本估计总体，只能求出样本的主成分。假设数据矩阵是
$$
X=
\begin{pmatrix}
x_1' \\ x_2' \\ \cdots \\ x_n' 
\end{pmatrix}=
\begin{pmatrix}
x_{11} & x_{12} &\cdots &x_{1p} \\
x_{21} & x_{22} &\cdots &x_{2p} \\
\vdots & \vdots &\ddots &\vdots \\
x_{n1} & x_{n2} &\cdots &x_{np}
\end{pmatrix}
$$
那样本协方差矩阵为$S=\displaystyle\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)'=(s_{ij})$，样本相关矩阵为$\hat R=(r_{ij})$，其中$r_{ij}=\displaystyle\frac{s_{ij}}{\sqrt{s_{ii}}\sqrt{s_{jj}}}$是样本间的相关性系数，$\bar x=\displaystyle\frac1n\sum_{i=1}^nx_i$是样本均值。用$S$、$\hat R$作为$\Sigma$、$R$的估计，即可算出样本的主成分。

### 应用

1、样本降维

这是最直观的使用方式，暂不详述。

2、特征分类

（用相关性矩阵$R$算出的）第$k$主成分$Z_k$与第$i$特征$X_i$的相关性系数为$\sqrt{\lambda_k}a_{ik}$，其中$a_{ik}$是第$k$特征向量的第$i$分量，记其为$\rho_{ik}$. 如果某两个特征$X_i,X_j$对每个主成分的相关性系数都差不多，也即$\rho_{ik}\approx\rho_{jk}(k=1,2,...,m)$，那么可想而知，这两个特征是高度线性相关的。所以，可以用$Q_i=(\rho_{i1},\rho_{i2},...,\rho_{im})$作为第$i$特征$X_i$的一种代表，用点$Q_i$（的散点图等）来对特征进行分类。

3、特征重构

（在对含义加以解释后）使用主成分作为新的特征，可以进行排序和回归等。进行回归的好处在于，主成分消除了共线性。

4、正态性检验

多元正态分布的检验，通常要转化成一元正态分布检验来处理。当样本的各个分量之间存在相关性时，无法直接对各个分量做一元正态分布检验。转化成主成分则可以。

# 因子分析

### 思路

考虑这样的情况：每个特征都不是“本质”的，而是其他一些更底层的变量（因子）的线性组合。也正是因为如此，所以特征之间存在相关性。我们希望找出这些更底层的因子来描述数据。

具体来说，对于一个随机向量$x=(x_1,x_2,...,x_p')$，有均值$\mu=(\mu_1,\mu_2,...,\mu_p)$与协方差矩阵$\Sigma=(\sigma_{ij})$，我们会假定它的分量$x_i$由$m$个因子$f_1,f_2,...,f_m$的线性组合，以及自身的特殊因素$\varepsilon_i$来决定，即
$$
\begin{cases}
x_1&=\mu_1+a_{11}f_1+a_{12}f_2+\cdots+a_{1m}f_m+\varepsilon_1 \\
x_2&=\mu_2+a_{21}f_1+a_{22}f_2+\cdots+a_{2m}f_m+\varepsilon_2 \\
&\vdots& \\
x_p&=\mu_p+a_{p1}f_1+a_{p2}f_2+\cdots+a_{pm}f_m+\varepsilon_p \\
\end{cases}
$$
或者写成矩阵
$$
x=\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_p
\end{pmatrix}
=\begin{pmatrix}
\mu_1 \\ \mu_2 \\ \vdots \\ \mu_p
\end{pmatrix}+
\begin{pmatrix}
a_{11} & \cdots & a_{1m}  \\ 
a_{21} & \cdots & a_{2m}  \\ 
\vdots & \ddots & \vdots  \\ 
a_{p1} & \cdots & a_{pm}  \\ 
\end{pmatrix}
\begin{pmatrix}
f_1  \\ \vdots \\ f_m
\end{pmatrix}+
\begin{pmatrix}
\varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_p
\end{pmatrix}
=\mu+Af+\varepsilon
$$
其中的$f_i$称（公共）因子，$\varepsilon_i$称特殊因子，系数$a_{ij}$称为载荷。并且为了让因子之间的信息不重叠，还假定
$$
\begin{cases}
E(f)=0 \\
E(\varepsilon)=0 \\
V(f)=I \\
V(\varepsilon)=D=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2) \\
\operatorname{Cov}(f,\varepsilon)=0
\end{cases}
$$
建立在以上全部假定基础上的模型，称为正交因子模型。

在实践中，终极目的是针对每组样本$x_i$给出对应的$f$。这通常需要先算出载荷矩阵的$A$，根据载荷的大小，可以对因子做出实际意义上的解释（这一点与主成分分析类似）。

### 性质

1. （A的计算式）由$x=\mu+Af+\varepsilon$可得$\Sigma=V(x)=V(Af)+V(\varepsilon)=AV(f)A'+V(\varepsilon)=AA'+D$，所以只要能对$\Sigma$找到合适的分解，就能得到$A$。与主成分分析同理，为了消除变量尺度的影响，可以对$x$做标准化，标准化后的协方差矩阵就相当于原本的相关系数矩阵$R$。

1. （数据变化的影响）改变$x$的单位，也即将$x$变换成$x^*=Cx$，其中$C=\operatorname{diag}(c_1,c_2,\cdots,c_p),c_i>0$，并不妨害正交因子模型的成立。也即$x^*$依然满足全部假定。（这为上面说的标准化提供了可行性。）
1. （因子旋转的影响）对$\Sigma$的分解方法并不唯一，所有可行的分解之间只相差一个正交变换。假如已经得到一组可行的$A,f$，用正交矩阵$T$做变换$A^*=AT,f^*=T'f$，那么新得到的$A^*,f^*$依然满足正交因子模型的假定，且成立$\Sigma={A^*} {A^*}'+D $。这启示我们可以通过正交变换来选择性质更好的因子。
1. （载荷的意义）由$x=\mu+Af+\varepsilon$可得$\operatorname{Cov}(x,f)=A $，也即$\operatorname{Cov}(x_i,f_j)=a_{ij} $。特别地，如果$x$经过了标准化，就是$\rho(x_i,f_j)=a_{ij} $。
1. （载荷平方和）由$x_i = \mu_i+a_{i1}f_1+a_{i2}f_2+\cdots+a_{im}f_m+\varepsilon_i$可得$V(x_i)=\sigma_{ii}=a_{i1}^2+\cdots+a_{im}^2+\sigma_i^2$，进而有$ \sum\limits_{i=1}^p V(x_i)=\sum\limits_{i=1}^p a_{i1}^2+\cdots+\sum\limits_{i=1}^p a_{im}^2+\sum\limits_{i=1}^p \sigma_{i}^2$。记$h_i^2=\sum\limits_{j=1}^m a_{ij}^2$（行载荷平方和），$g_j^2=\sum\limits_{i=1}^p a_{ij}^2$（列载荷平方和），则$h_i^2$反映了公共因子对分量$x_i$的方差贡献，$g_j^2$反映了第$j$个公共因子$f_j$对所有分量的总方差贡献。

### 算法

为了求解因子模型，需要根据样本求出载荷矩阵$A$和特殊方差矩阵$D=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2)$。假设样本是$x_1,\cdots,x_n$，可知均值$\bar x$、协方差矩阵$S=\frac{1}{n-1}\sum\limits_{i=1}^n (x_i-\bar{x})(x_i-\bar{x})'$与相关系数矩阵$\hat R$。有以下方法估计$A$与$D$，以及计算因子得分$f$：

#### 估计载荷-主成分法

已知协方差矩阵可以做分解$S=\lambda_1t_1t_1'+\lambda_2t_2t_2'+\cdots+\lambda_pt_pt_p'$，其中的$\lambda_i,t_i$是$S$的特征值（可以证明非负）与相应的单位正交特征向量。那么，令$ \hat A =(\sqrt{\lambda_1}t_1,\cdots,\sqrt{\lambda_m}t_m) $，就有$\hat A\hat A'=\lambda_1t_1t_1'+\cdots+\lambda_mt_mt_m'$，再令$ \hat D$是剩余部分$\lambda_{m+1}t_{m+1}t_{m+1}'+\cdots+\lambda_pt_pt_p'$的对角元所构成的对角阵，就有$S\approx\hat A\hat A'+\hat D$，二者只相差了$\lambda_{m+1}t_{m+1}t_{m+1}'+\cdots+\lambda_pt_pt_p'$的非对角元部分。$\hat A,\hat D$就是对$A,D$的估计。

与主成分分析类似，应该选择合适的$m$，使得贡献率$\sum\limits_{i=1}^m \lambda_i / \sum\limits_{i=1}^p \lambda_i$比较大。

#### 估计载荷-迭代主因子法

该方法要求对数据经过了标准化，也即满足关系$R=AA'+D$。

如果我们已经有了对$D$的估计$\hat D$，就可以先得到$R^*=\hat R- \hat D=AA'$，再做分解$R^*=\lambda^*_1{t^*_1}{t^*_1}'+\cdots+\lambda^*_p{t^*_p}{t^*_p}'$。现在令$ \hat A =(\sqrt{\lambda_1^*}t_1^*,\cdots,\sqrt{\lambda_m^*}t_m^*) $，则有$R^*\approx \hat A\hat A'$。

没有先验的估计$ \hat D$时，可以用某个初值$D_0$作为估计，先得到初步的估计$\hat A$，再更新特殊方差为$\sigma_i^2=1-h_i^2=1-\sum\limits_{i=1}^m a_{ij}^2 $，进而得到更新的$D_1=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2)$来作为下一步的$ \hat D $的估计。不断迭代来得到稳定的结果。

一般可以采取的$\sigma_i^2$的初始估计值有：（1）$\hat R^{-1} $的对角元$r^{ii}$（2）取$\sigma_i^2=1-\max\limits_{j\neq i} |r_{ij}|$（3）取$0$。（4）取$1$。

#### 估计载荷-极大似然法

根据对$f,\varepsilon$的假设，可以知道$ f \sim N_m(0,I), \varepsilon\sim N_p(0,D) $，而$x$是$f$与$\varepsilon$的线性函数，于是也服从多元正态分布，即有$x\sim N_p(\mu,\Sigma)$。可以写出$x$的密度函数为$f(x,\mu,\Sigma)$，用$AA'-D$替换掉$\Sigma$，再把样本代入$x$，就得到了似然函数$L(\mu,A,D)$，

使得该函数取最大值的参数就是极大似然估计$(\hat\mu,\hat A,\hat D)$。

可以证明，$\hat\mu=\bar x$，而$\hat A,\hat D$满足方程组
$$
\begin{cases}
\hat\Sigma\hat D^{-1}\hat A=\hat A(I_m+\hat A'\hat D^{-1}\hat A) \\
\hat D=\operatorname{diag}(\hat\Sigma-\hat A\hat a')
\end{cases}
$$
其中$\hat\Sigma=\displaystyle \frac1n \sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)' $。为了求出唯一解，还需要补充条件“$A'D^{-1}A$是对角阵”。现在，完全转变成为解方程问题，可以用数值方法求解。

#### 因子得分-加权最小二乘法

得到载荷矩阵$A$只能满足解释的需求，为了实现降维，还需要计算出因子得分也即$f_1,f_2,\cdots,f_m$的值。

加权最小二乘法，其思路是选取合适的因子得分，使得下式
$$
\sum_{i=1}^p \frac{\left[x_i-(\mu_i+a_{i1}f_1+\cdots+a_{im}f_m)\right]^2}{\sigma_i^2}
$$
达到最小。这样求得的$\hat f=(\hat f_1,\cdots,\hat f_m)'$称为巴特莱特因子得分。

用数学分析的方法可以得到，上述问题的解为
$$
\hat f=(A'D^{-1}A)^{-1}A'D^{-1}(x-\mu)
$$
在实践中，用$ \bar x,\hat A,\hat D$作为$ \mu,A,D$的估计，代入求解。可以证明，该估计是无偏的。

#### 因子得分-回归法/条件期望法

根据对$f,\varepsilon$的假设，可以知道$ \begin{pmatrix} f \\ \varepsilon \end{pmatrix} $服从多元正态分布$N(\bold{0}, \begin{pmatrix} I &\bold{0} \\ \bold{0} & D \end{pmatrix} )$。而$x$是$f$与$\varepsilon$的线性函数，所以$ \begin{pmatrix} f \\ x \end{pmatrix} $也服从多元正态分布，且可以求出分布为$N(\begin{pmatrix} \bold{0} \\ \bold{\mu} \end{pmatrix}, \begin{pmatrix} I & A' \\ A & \Sigma \end{pmatrix} )$。于是我们用$f$的条件数学期望
$$
\hat f=E(f|x)=A'\Sigma^{-1}(x-\mu)=(I+A'D^{-1}A)^{-1}A'D^{-1}(x-\mu)
$$
来作为$f$的估计。该得分也称之为汤姆森因子得分。

在实践中，用$ \bar x,\hat A,\hat D,S$作为$ \mu,A,D,\Sigma$的估计，代入求解。

该估计是有偏的。不过，其有效性严格大于加权最小二乘法。而且，在偏离正态假设的情形下依然可以使用。

### 评价

评价因子模型是否可用，有以下参考标准：

1. （残差矩阵）定义残差矩阵为$S-(\hat A \hat A'+\hat D)$（如果是经过了标准化，则用原数据的相关系数矩阵$\hat R$代替$S$），残差矩阵的元素越小，说明估计值$\hat A,\hat D$越精确，模型的效果也就越好。
1. （解释总方差的累计比例）在性质的载荷平方和部分提到过，$A$的列平方和$g_j^2$是$f_j$对所有分量的总方差贡献，那么$g_j^2$的和$\sum\limits_{j=1}^m g_j^2$也即全部载荷的平方和$\sum\sum a_{ij}^2$就是采用的全部$m$个因子所有分量的总方差贡献。所以可以用因子的总方差贡献除以分量方差之和，也即$\sum\limits_{j=1}^m g_j^2 / \sum\limits_{i=1}^p V(x_i) $ ，来衡量主成分发挥的作用。让该值比较大（达到$0.8$之类的）的$m$就是合适的因子个数。在实践中，用样本协方差矩阵的迹$\tr(S)$来代替$\sum\limits_{i=1}^p V(x_i)$计算。

### 代码

#### Python

官方文档：

​	https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html



样例：

```python
#FactorAnalysis(n_components=None, *, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None, svd_method='randomized', iterated_power=3, rotation=None, random_state=0)

from sklearn.decomposition import FactorAnalysis 
FA=FactorAnalysis()
FA.fit(X)
X_transformed = FA.transform(X)
```



超参数：

​	n_components：所使用的因子维度$m$。

​	noise_variance_init：迭代计算的初始$D_0$，只需给出对角元即可。也即参数内容是长度为$p$的数组，代表每个分量的特殊方差估计值。默认全部为$1$。

​	rotation：旋转的方式，接受字符串‘varimax’或者‘quartimax’，默认值None。如果给了参数，会使用相应方法进行旋转。varimax指的是最大方差旋转法；quartimax可译为四次方最大值法，内容暂不明。



属性：

​	components_：载荷矩阵，每一列是一个分量所对应的载荷，也即$A'$。其形状为(n_components, n_features)的矩阵，对接到上述的理论则是$m\times p$。

​	noise_variance_：特殊方差。也即$D$的对角元组成的向量。



方法：

​	fit(X)：根据数据训练。X是二维数组类的变量。

​	transform(X)：对二维数组X应用训练过的模型，返回降维之后得到的由向量$f_i$作为行的二维数组。

​	score(X)：返回平均对数似然值。这是一个浮点数。

#### R语言

样例：

```R
library(psych)
faml=fa(r=R, nfactors=4, residuals=TRUE, rotate="none", fm="ml") #极大似然法因子分析
faml$loadings
faml$communality

faml.varimax=fa(R, nfactors=4, rotate="varimax", scores=TRUE, fm="ml") #因子旋转
faml.varimax$loadings
```



参数：

​	r：协方差矩阵或者相关系数矩阵。

​	nfactorts：采用的因子数目。

​	rotate：旋转方式。有"none", "varimax", "quartimax", "bentlerT", "equamax", "varimin", "geominT"等等。

​	fm：计算方式。有"pa"（主成分法），"ml"（极大似然估计）等方法。



属性：

​	loadings：因子载荷矩阵。

​	communality：载荷的行平方和，也即共性方差。