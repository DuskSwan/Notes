[TOC]

# 预处理方法

## 1.特征编码

将非数值特征转化成数值特征。

### 整数值编码

依自然数顺序编码。此举保留了特征数据的顺序。具体来说就是把数据集中的列

| 类别 |
| :--: |
|  A   |
|  A   |
|  B   |
|  B   |
|  C   |

替换成

| 类别 |
| :--: |
|  0   |
|  0   |
|  1   |
|  1   |
|  2   |



### One-Hot编码

按自然基编码。此举使得文本特征之间保持等距，也即地位对等，避免引入次序。具体来说就是把

| 类别 |
| :--: |
|  A   |
|  A   |
|  B   |
|  B   |
|  C   |

替换成

| 类别x | 类别y | 类别z |
| :---: | :---: | :---: |
|   0   |   0   |   1   |
|   0   |   0   |   1   |
|   0   |   1   |   0   |
|   0   |   1   |   0   |
|   1   |   0   |   0   |

但此举增加了特征之间的相关性。如上例，将会有$ x+y+z\equiv1 $。

### 哑变量编码

One-Hot编码的改进，将某一指标的编码置为零向量，避免引入相关性。但距离也因此不完全对等。具体来说就是把

| 类别 |
| :--: |
|  A   |
|  A   |
|  B   |
|  B   |
|  C   |

替换成

| 类别x | 类别y |
| :---: | :---: |
|   0   |   0   |
|   0   |   0   |
|   0   |   1   |
|   0   |   1   |
|   1   |   0   |

### WOE编码

WOE叫做证据权重(Weight of Evidence)，是一种有监督的编码方式。将预测类别的集中度的属性作为编码的数值。数据集进行WOE编码后，表示的其实是自变量取某个值的时候对正例比例的一种影响。

WOE公式：
$$
\text{WOE}_i = \text{ln}(\frac{\normalsize{\text{bad}}_i}{\normalsize{\text{bad}}_\text{T}})-\text{ln}(\frac{\normalsize{\text{good}}_i}{\normalsize{\text{good}}_\text{T}})
$$
$\text{good}_i$表示每组中标签为good的数量即没有违约的数量，$\text{good}_T$为good的总数量；bad相同。

从category_encoders库中导入`WOEEncoder`，可以进行WOE编码。

```python
from category_encoders import WOEEncoder
coder = WOEEncoder(cols=feature_names).fit(X,y)
coded_data = coder.transform(X)
```

`WOEEncoder`常用参数：

+ cols：列名列表，对其指定的列进行编码，如果未指定则将对所有列进行编码。
+ drop_invariant：布尔型，是否删除方差为0的列。
+ regularization：正则化，浮点型，正则化的主要目的是防止被零除。

可调用的主要方法：

+ fit： 用于计算训练数据的WOE值，后面用其来转换数据。
+ transform： 按照保存的WOE值转换数据。
+ fit_transform： 不仅计算训练数据的WOE值，还会基于计算出来的WOE值来转换数据。

## 2.缺失值处理

### 删除法

删除数据组（数据集行），或者删除特征（数据集列）。

### 均值填补

使用均值（对连续性数据）或众数（对离散型数据）进行填补。

对此的一个改进是，按照某个依据对数据分组，之后组内按均值或众数填补。

### 随机填补（Bootstrap自助法）

核心要义是选择其他组数据的该特征填入缺失组，选择过程有随机性。

Bootstrap被提出时，是一种估计参数的手段，可以用这种“重抽样”的手段衡量一个估计量的好坏（比如计算其标准差）。现在我们只是用类似的思路去抽值填补，目的是“抽样”而不再是“估计”。

传统（频率派）的Bootstrap即是从已有的样本中等可能地随机抽取值来填补。

贝叶斯Bootstrap仍然从已有的样本中随机抽取值来填补，但此时不再是等可能的，抽取概率是由[0,1]区间上的一次混乱划分决定的。这样的优势在于，重采样所得到的样本频数相对更光滑。具体操作如下

> 设样本量为$n$，某特征有$k$个非缺失值${f_1,f_2...,f_k}$与$n-k$个缺失值。$a_1,a_2...,a_{k-1}$是在$[0,1]$区间均匀混乱的$k-1$个值，它们将$[0,1]$区间划分成了$k$段，每段长度为$p_i(i=1,2,...,k)$。那么在每个空缺位置上，以为$p_i$概率填入$f_i$ 。

近似贝叶斯Bootstrap方法则先从非缺失值${f_1,f_2...,f_k}$中有放回地抽取$k$个建立可重复集合$F$，那么在每个空缺位置上，从$F$中等概率的抽一个值填入。可以计算发现，这样的时间复杂度要更小。

>  参考：
>
>  https://zhuanlan.zhihu.com/p/54201828
>
>  https://blog.csdn.net/hqh45/article/details/42706263
>
>  https://blog.csdn.net/weixin_43948357/article/details/112977055

### 预测填补/基于模型填补

用其他特征预测有缺失值的特征的值。比如建立目标特征与其他特征的回归模型。



## 3.数据标准化

对于一列特征$X=(X_1,X
_2,\cdots,X_n)'$，为了消除数据规模对相关性的影响，或者让数据适应算法，需要控制数据的尺度。

### Z-score标准化/正态标准化

**思路**

标准化方法：

$$
X_i'=\frac{X_i-\bar{X}}{\sigma}
$$
其中$\sigma=\displaystyle\sqrt{\frac1n \sum(X_i-\bar{X})^2}$是对总体标准差的一个估计，这个估计并不无偏，但是比样本标准差（分母n-1）要更准确一些。

有时，为了降低离群值的影响，也使用$s=\frac1n \sum|X_i-\bar{X}|$代替$σ$进行标准化。

该方法适用于样本比较分散，或者特征的最值未知的情况。标准化后的样本近似服从标准正态分布。

**代码**

使用sklearn的preprocessing模块，其中的类StandardScaler可以完成这一任务，见下例

```python
>>> from sklearn.preprocessing import StandardScaler
>>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]
>>> scaler = StandardScaler()
>>> print(scaler.fit(data))
StandardScaler()
>>> print(scaler.mean_) #返回每列的均值
[0.5 0.5]
>>> print(scaler.transform(data))
[[-1. -1.]
 [-1. -1.]
 [ 1.  1.]
 [ 1.  1.]]
>>> print(scaler.transform([[2, 2]])) #以同法转化其他数据
[[3. 3.]]
```

参数矩阵应该是`(n_samples, n_features)`形状的二维数组（其他类型会被自动转化），其每一行被当做一组数据。按照列的均值和方差来标准化。



### Min-Max标准化/归一化

**思路**

标准化方法：

$$
X_i'=\frac{X_i-X_{min}}{X_{max}-X_{min}} \\ 
X_i'=\frac{b-a}{X_{max}-X_{min}} (X_i-X_{min})+a\\
$$
前者是常用形式，将数据放缩到[0,1]区间；后者是通用形式，放缩到[a,b]区间

其不利之处在于：1.新加入数据后要重新计算；2.易受离群值影响。

**代码**

```python
>>> from sklearn.preprocessing import MinMaxScaler
>>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
>>> scaler = MinMaxScaler()
>>> print(scaler.fit(data))
MinMaxScaler()
>>> print(scaler.data_max_)
[ 1. 18.]
>>> print(scaler.transform(data))
[[0.   0.  ]
 [0.25 0.25]
 [0.5  0.5 ]
 [1.   1.  ]]
>>> print(scaler.transform([[2, 2]]))
[[1.5 0. ]]
```

用法与StandardScaler完全类似，接收二维数组并使用fit和transform方法来计算标准化数据。

### 小数定标标准化/科学计数法

标准化方法：

$$
X_i'=\frac{X_i}{10^j}
$$
其中$j$是使得$max{X_i'}<1$的最小整数。这本质上就是对全部数据除以某个10的幂，降低数量级。

### Logistic标准化

标准化方法：

$$
X_i'=\frac{1}{1+e^{-X_i}}
$$

函数$\sigma(x)=\frac1{1+e^{-x}}$将实数映射到[0,1]区间上，较容易将之分成两类。

### 向量单位化

让向量除以自己的范数，所得到的新向量将会有单位范数。

**代码**

使用sklearn的preprocessing模块，其中的函数normalize可以完成这一任务，见下例

```python
X = [[ 1., -1.,  2.],
     [ 2.,  0.,  0.],
     [ 0.,  1., -1.]]

from sklearn.preprocessing import normalize
X_normalized = normalize(X) #计算结果
print(X_normalized) #输出
#[[ 0.40824829 -0.40824829  0.81649658]
# [ 1.          0.          0.        ]
# [ 0.          0.70710678 -0.70710678]]

```

该函数接收二维数组（其他类型的会自动转化成二维数组），返回numpy数组。接收的二维数组须是`(n_samples, n_features)`形状的，其每一行被当做一个向量来做单位化。默认使用二范数。

## 4.（连续）特征离散化

为了提高算法的效率，或者使数据适应算法，有时需要将连续的特征转变为离散的。

离散的方法就是将连续特征的取值范围划分成若干小区间。而问题在于如何寻找分点。我们的目标是，划分完毕的小区间，尽可能地保留原本数据的信息。

### 等距离散化

将连续特征的取值范围均分分成k段。缺点是对离群值很敏感。这是一个无监督的离散化方法。

### 等频离散化

根据连续特征的样本数n，将取值范围划分成k段，使得每一小段都包含n/k个数据点。缺点是可能将相近的值划入不同的区间。这是一个无监督的离散化方法。

### 聚类离散化

就是聚类。这是一个无监督的离散化方法。

### 信息增益离散化

一开始将全部取值视作一个大区间。对区间中的每个取值，计算以它位分点时的“熵”（其实是建立决策树的平方误差这一指标），选择熵最小的作为分点。重复这一过程，再进行一些合并。直到满足终止条件（决策树深度、叶子结点个数等）结束。这是一个无监督的离散化方法。

具体来说，对于特征下的一串数据$x_i$，先把它们排列起来，一开始以整体作为初始的大区间，对于每个数据点$x_{(i)}$，计算出在该点处分裂之后，样本的信息熵
$$
H_i=-\sum p_k log_2(p_k)
$$
其中$k$表示分裂之后的第$k$个小区间，$p_k$是这一特征在该区间取值的概率，实践中用频率$f_k=\frac{\text{第k区间内的样本个数}}{总样本个数}$来代替。较小的$H_i$意味着分裂带来的不确定性较少，所以是的$H_i$最小的$x_{(i)}$就是最佳的分裂点。

### ChiMerge卡方离散化

使用该方法，要求特征同时拥有取值和类别两种信息。一开始将每个取值都作为一段。对于每一对相邻的区间，通过计算卡方统计量来判断是否合并两区间。这是一个有监督的离散化方法。

该方法力求让划分后的区间与类别有较高的一致性。统计学背景可查看$\chi^2$拟合优度检验。

具体来说，假设某一列数据$x_i$总共分为$C$个类，排序得到一列$x_{(i)}$，起始以每个$x_{(i)}$为单独的小区间，对于对于每对相邻的区间，计算
$$
\chi^2=\sum_{i=1}^2 \sum_{j=1}^C \frac{A_{ij}-E_{ij}}{E_{ij}}
$$
其中$A_{ij}$是第$i$区间（由于考虑的是一对，所以一共就两个区间）中第$j$类别的样本数目，$E_{ij}$是第$i$区间中第$j$类别的期望频数（这通常由两区间的平均样本数目代替）。当该统计量小于特定的阈值时，合并两区间。其含义是，当卡方统计量较小，我们认为这两个区间中“类别”的分布有着相同的规律，所以合并。重复此过程直到满足特定的需求。

### 类别属性相互依赖最大化

也即CAIM，全称Class-attribute Interdependence Maximization。使用该方法，要求特征同时拥有取值和类别两种信息。一开始将全部取值视作一个大区间。对区间中的每个取值，计算以它位分点时，类别和区间的相互依赖程度，程度越大说明该分位点越佳。这是一个无监督的离散化方法。

一般来说，该法得到的区间个数会与类别数相近。它力求让划分后的区间与类别有较高的关联性。统计学背景可查看$\chi^2$独立性检验。

具体来说，假设一些数据$x_{ij}$总共分为$C$个类，分布在$[d_0,d_1],(d_1,d_2],\cdots,(d_{k-1},d_k]$这$k$个区间中。我们可以得到一个$C\times k$列联表：

| 类别\区间 | $[d_0,d_1]$    | $(d_1,d_2]$   | $\cdots$ | $(d_{k-1},d_k]$ | 总数         |
| --------- | -------------- | ------------- | -------- | --------------- | ------------ |
| $1$       | $n_{11}$       | $n_{12}$      | $\cdots$ | $n_{k1}$        | $n_{1\cdot}$ |
| $2$       | $n_{21}$       | $n_{22}$      | $\cdots$ | $n_{k2}$        | $n_{2\cdot}$ |
| $\vdots$  | $\vdots$       | $\vdots$      |          | $\vdots$        | $\vdots$     |
| $C$       | $n_{C1}$       | $n_{C2}$      | $\cdots$ | $n_{Ck}$        | $n_{C\cdot}$ |
| 总数      | $n_{\cdot 1 }$ | $n_{\cdot 2}$ | $\cdots$ | $n_{\cdot k}$   | $n$          |

令CAIM统计量为
$$
T=\frac{1}{N} \sum_{j=1}^k \frac{M_j^2}{n_{\cdot j}}
$$
其中$M_j=\max\{n_{1j},n_{2j},\cdots,n_{Cj}\}$是第$j$列的最大值。$T$的取值范围是$(0,1]$，它越大说明当前的区间划分越良好。

最开始，将全部取值视作一个大区间。每次划分一个区间时，以区间中的每个值为候选点，计算切分后的新区间的CAIM统计量，选使得其最大的点为划分点。依次划分所有区间，直到满足某个终止条件。

## 5.离群值检测

### 拉依达准则

假定数据服从正态分布，将$[μ-3σ,μ+3σ]$之外的数据看作是离群值。实际操作中往往将$x_z=\frac{|x-\mu|}{\sigma}>3$的$x$扔掉。

### K邻近算法

对每个样本，计算它与最近的$k$个样本之间的距离，这个距离大于设定的阈值，则认为这个样本离群。

这是一个常用的有效的办法，不过缺点在于，只能检测全局异常值，无法检测局部异常值。

### 局部离群因子算法（LOF, Local Outlier Factor）

用$d(x_1,x_2)$表示样本$x_1$与$x_2$的距离，$d_k(x)$表示样本$x$到其第$k$邻近的样本点的距离，$N_k(x)$表示样本$x$的前$k$个邻近样本点组成的集合。$\operatorname{rd}_k(x_1,x_2)=\operatorname{max}\{d_k(x_2),d(x_1,x_2)\}$称为样本$x_1$到$x_2$的可达距离，当$x_2$离群或者$x_1$与$x_2$的距离很远时这个值比较大。

定义样本的局部可达密度为$ \displaystyle \operatorname{lrd}_k(x)=\left(\frac 1k \sum_{y\in K_k(x)}\operatorname{rd}_k(x,y)\right)^{-1}$，它与$x$到其邻近样本的平均可达距离成反比，这个值小意味着$x$到其临近点的距离较远，或者$x$的临近点都比较离群。

定义样本的局部离群因子为$ \displaystyle\operatorname{lof}_k(x)=\frac1k\sum_{y\in N_k(x)}\frac{\operatorname{lrd}_k(y)}{\operatorname{lrd}_k(x)}$，若x离群，那么分子们都会小而分母会大，通常$\operatorname{lof}_k(x)$大于$1$就可以怀疑$x$是离群点。

## 6.噪声去除

### 小波去噪

处理数字信号的噪声所用的办法。过程是相对信号进行小波分解，然后依据某种阈值，对分解得到的系数进行操作，将操作后的信号部分重构，得到的就是去噪后的信号。

**阈值类型**

有多种方法构造阈值，来判别系数的大小。

VisuShrink阈值：

也叫固定阈值、通用阈值，为$\lambda=\sigma\sqrt{2\log_2N}$。其中$\sigma$是噪声信号的标准差，$N$是信号的长度。通常用$\hat\sigma=\operatorname{median}\limits_k(|w_{J-1,k}|)/0.6745$来估计噪声信号的标准差，其中$w_{J-1,k}$指的是最高分辨率（仅次于原始信号级别）的一级小波分解系数。它是全局统一的阈值，这意味着对于任意分辨率下的系数，阈值都一样。

对它的一个改进是$\lambda_j=\sigma\sqrt{2\log_2N}/(\log_2(j+1))$，改进后的阈值不再是全局的，而是每一级分辨率下的阈值都不同。

**阈值函数**

阈值函数，指的是要如何根据阈值修改小波分解所得的系数。通常我们认为大于阈值的系数来自有用信号，小于的来自噪声。

硬阈值：
$$
\widetilde{w}_{j,k}=\begin{cases}
0, & |w_{j,k}|\leqslant\lambda \\
w_{j,k}, & |w_{j,k}|>\lambda \\
\end{cases}
$$
软阈值：
$$
\widetilde{w}_{j,k}=\begin{cases}
0, & |w_{j,k}|\leqslant\lambda \\
\text{sign}(w_{j,k})(|w_{j,k}|-\lambda), & |w_{j,k}|>\lambda \\
\end{cases}
$$
软硬阈值折中（有参数$a$，显见$a=0$就是硬阈值，为$1$就是软阈值）：
$$
\widetilde{w}_{j,k}=\begin{cases}
0, & |w_{j,k}|\leqslant\lambda \\
\text{sign}(w_{j,k})(|w_{j,k}|-a\lambda), & |w_{j,k}|>\lambda \\
\end{cases}
$$


# 数据感知方法

## 相关性分析

### Pearson相关系数

### Spearman相关系数

### Kendall相关系数

### 方差分析

### 卡方检验

### 岭回归岭迹图

### 代码

```python
from sklearn.feature_selection import SelectKBest

selector = SelectKBest()
selector.fit()

```

Sklearn中的`SelectKBest()`方法可用于特征选择，根据给定的方法，选择出前k个与目标最相关的特征，其主要参数如下：

+ score_func：用于计算相关性的函数，该函数应该接受两个数组X和y，并返回一对数组（`scores`，`pvalue`）或带`scores`的单个数组，`scores`表示得分，得分越高相关性越强，`pvalue`表示检验的P值，值越小表示相关性越强，默认值为`f_classif`，表示计算方差分析的F值，其它取值如下：

  0.`f_classif`：查看连续型特征与离散目标特征相关程度。
  1.`f_regression`：计算相关系数的F值，适用于连续型与连续型的相关性计算
  2.`chi2`：计算卡方统计量，适用于离散型与离散型的相关性计算
  3.`mutual_info_classif`：离散型与离散型的互信息计算
  4.`mutual_info_regression`：连续型与连续型的互信息计算

+ k：需要选择的最佳特征个数，默认值为10。也可以指定'all'表示全部保留。

方法

+ `fit(X,y)`：传入 X (n_samples, n_features)和 y (n_samples,) 来“训练模型”，实际上就是计算并筛选出前k个特征。
+ `transform(X)`：传入特征表格X，返回筛选剩下的新特征表格，其行数n_samples不变，列数n_features变成k。

属性

+ `score_`：得到相关性得分。

+ `pvalues_`：得到检验的P值。这两个都是长度为k的向量。

  