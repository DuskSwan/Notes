交叉熵函数：httns://www.zhihu.com/tardis/zm/art/35709485?source_id=1003



以下均假设样本数为$N$，所谓的损失是针对一个样本的。对于一个batch，总的损失自然是样本损失之和或者平均。

对于回归任务，以下均假设真实标签是$y=(y_1,...,y_n)$，预测结果是$\hat y=(\hat y_1,...,\hat y_n)$，维数为$n$。

对于分类任务，以下均假设真实标签是$k$，预测结果是$\hat k$，共有$c$个类别。更一般的，模型输出的是各个类别的概率$p=(p_1,p_2,...,p_c)$

### L1损失/平均绝对误差(Mean Absolute Error, MAE)

$$
L_1(\hat y,y)=\frac1n\sum_{i=1}^n |\hat y_i -y_i|
$$



### L2损失/均方误差(Mean Squared Error, MSE)

$$
L_2(\hat y,y)=\frac1n\sum_{i=1}^n (\hat y_i -y_i)^2
$$

### smooth L1

$$
smooth_{L_1}=\frac1n\sum_{i=1}^n \text{smooth}(\hat y_i -y_i)\\
\text{smooth}(x)=\begin{cases}
0.5x^2, &|x|<1\\
|x|-0.5, &|x|\ge1
\end{cases}
$$

![img](img/v2-4edbd47a9cd0cf5a4637e84c557603a3_1440w.png)

### 交叉熵损(Cross Entrony Loss)

二分类情况时，每个样本的$y_i$只会是$0$或$1$，模型输出是属于正类的概率$p$，此时第$i$个样本的损失为
$$
L_i = -[y_i \ln(p_i) + (1-y_i) \ln(1-p_i)] =
\cases{
-\ln(p_i), & $y_i=1$ \\
-\ln(1-p_i), & $y_i=0$
}
$$
由于$p_i<0$，这个损失永远是正值。

多分类情况时，假设第$i$个样本的真实类别是$k$，该样本的损失为
$$
L_i = -\frac1c\sum_{i=1}^c y_{ij}\ln(p_{ij}) = -\ln(p_{ik})
$$
其中$y_{ij}$在第$i$样本真实类别为$j$时取$1$，否则取$0$；$p_{ij}$是第$i$样本真实被判断为第$j$类别的概率。
