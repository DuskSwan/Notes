# 损失函数

以下均假设样本数为$N$，所谓的损失是**针对一个样本**的。对于一个batch，总的损失自然是样本损失之和或者平均。

对于回归任务，以下均假设真实标签是$y=(y_1,...,y_n)$，预测结果是$\hat y=(\hat y_1,...,\hat y_n)$，维数为$n$。不过我们注意到，当维数为$1$的时候，一个标签就写作$(y_1)=y$，此时如果用$y_i$表示第$i$个样本，那么“一个batch的loss”和“一个样本的loss”实际上表达式完全相同，基于此，以下给出的表达式也可以看作是针对一批数据的。

对于分类任务，以下均假设真实标签是$k$，预测结果是$\hat k$，共有$c$个类别。更一般的，模型输出的是各个类别的概率$\hat p=(\hat p_1,\hat p_2,...,\hat p_c)$，真实概率则记为$p=(p_1,p_2,...,p_c)$。

## 回归损失

### L1损失/平均绝对误差(Mean Absolute Error, MAE)

$$
L_1(\hat y,y)=\frac1n\sum_{i=1}^n |\hat y_i -y_i|
$$



### L2损失/均方误差(Mean Squared Error, MSE)

$$
L_2(\hat y,y)=\frac1n\sum_{i=1}^n (\hat y_i -y_i)^2
$$

### smooth L1

$$
smooth_{L_1}=\frac1n\sum_{i=1}^n \text{smooth}(\hat y_i -y_i)\\
\text{smooth}(x)=\begin{cases}
0.5x^2, &|x|<1\\
|x|-0.5, &|x|\ge1
\end{cases}
$$

![img](img/v2-4edbd47a9cd0cf5a4637e84c557603a3_1440w.png)

## 分类损失

### 交叉熵损失(Cross Entropy Loss)

该损失函数要求模型输出是属于各个类别的概率$p=(p_1,p_2,...,p_c)$。

二分类情况时，每个样本的$y_i$只会是$0$或$1$，模型输出是属于正类的概率$p$，此时第$i$个样本的损失为

$$
L_i = -[y_i \ln(p_i) + (1-y_i) \ln(1-p_i)] =
\cases{
-\ln(p_i), & $y_i=1$ \\
-\ln(1-p_i), & $y_i=0$
}
$$

由于$p_i<1$，这个损失永远是正值。

多分类情况时，假设第$i$个样本的真实类别是$k$，该样本的损失为
$$
L_i = -\frac1c\sum_{j=1}^c y_{ij}\ln(p_{ij}) \propto -\ln(p_{ik})
$$
其中$y_{ij}$在第$i$样本真实类别为$j$时取$1$，否则取$0$；$p_{ij}$是模型输出的第$i$样本判为第$j$类别的概率。总损失自然是$L = \frac1n\sum_iL_i$。

> 从物理意义上解释，如果说信息熵是消除不确定性的最小代价（或曰使用最优策略的代价），那么交叉熵就是在局面未知时，使用假定局面对应的最优策略时，实际付出的代价。多数情况下，我们并不知道系统的真实分布，如抛硬币例子，真实情况是两面出现的概率相同，但我们不知道这一信息，以为两面不一样，正面出现的概率是$p$，反面则是$1-p=q$，这就是一个非真实的（假定）分布，在此假定下实施最优策略，实际付出的代价就是交叉熵。
>
> 在数学里，交叉熵的定义是$H(p,q)=\sum_x p(x)\ln\frac{1}{q(x)}$，这里的$p,q$均为概率密度函数（离散时则为分布列），这可以看作是在描述分布$p$与$q$的相似度。在离散时应该写作对$x$的取值情况求和，在连续时应该写作对$x$求积分，这里我们不写那么严格了。
>
> 所谓的“假定局面”其实就是预测出来的分布，“真实局面”是数据的真实分布，容易想见，这两个分布越接近，实际付出的代价就越靠近最小代价。所以交叉熵可以用来描述两个分布是否接近，也就能衡量分类模型的预测结果了。

## 生成模型损失

### 相对熵/KL散度

设 p(x)、q(x) 是离散随机变量x的两个分布，则 p 对 q 的相对熵是：
$$
D_{KL}(p||q)=\sum_xp(x)\ln\frac{p(x)}{q(x)}=\int_xp(x)\ln\frac{p(x)}{q(x)}
$$
当分布是离散的时，p和q理解作概率分布；当分布是连续的时，p和q理解作概率密度函数。我们后面可能会写得不那么严格，不注意区分连续和离散，不过这并不影响理解。

> 容易证明信息熵$H(p)$、交叉熵$H(p,q)$以及相对熵$D_{KL}(p||q)$之间有关系：
> $$
> H(p,q)
> =\sum_x p(x)\ln\frac{1}{q(x)} 
> =\sum_x p(x)\ln \frac{1}{p(x)} + \sum_x p(x)\ln \frac{p(x)}{q(x)} 
> =H(p)+D_{KL}(p||q) 
> $$
> 这表明交叉熵等于分布p的熵加上p相对于q的KL散度。按照前述的物理意义，相对熵/KL散度的物理意义就是，使用假定分布q所制定的策略，相对真实分布p所对应的最优策略所多付出的代价。值得注意的是，p与q的相对关系是确定的，因此p与q不对称。

考虑p,q均是一维正态分布N(μ~i~,σ~i~^2^)，此时的KL散度可以计算得到，是
$$
D_{KL}(p||q) 
= \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac12
$$

> 推导过程见：https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/

更简单地，如果q是标准正态分布N(0,1)，代入上式可以直接得到

$$
D_{KL}(p||q) 
= \ln \frac1\sigma + \frac{\sigma^2+\mu^2}{2}-\frac12
=-\frac12 \left( 1+\ln\sigma^2 -\mu^2-\sigma^2 \right)
$$
