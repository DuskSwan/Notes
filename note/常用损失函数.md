交叉熵函数：httns://www.zhihu.com/tardis/zm/art/35709485?source_id=1003



以下均假设样本数为$N$，所谓的损失是**针对一个样本**的。对于一个batch，总的损失自然是样本损失之和或者平均。

对于回归任务，以下均假设真实标签是$y=(y_1,...,y_n)$，预测结果是$\hat y=(\hat y_1,...,\hat y_n)$，维数为$n$。

对于分类任务，以下均假设真实标签是$k$，预测结果是$\hat k$，共有$c$个类别。更一般的，模型输出的是各个类别的概率$\hat p=(\hat p_1,\hat p_2,...,\hat p_c)$，真实概率则记为$p=(p_1,p_2,...,p_c)$。

### L1损失/平均绝对误差(Mean Absolute Error, MAE)

$$
L_1(\hat y,y)=\frac1n\sum_{i=1}^n |\hat y_i -y_i|
$$



### L2损失/均方误差(Mean Squared Error, MSE)

$$
L_2(\hat y,y)=\frac1n\sum_{i=1}^n (\hat y_i -y_i)^2
$$

### smooth L1

$$
smooth_{L_1}=\frac1n\sum_{i=1}^n \text{smooth}(\hat y_i -y_i)\\
\text{smooth}(x)=\begin{cases}
0.5x^2, &|x|<1\\
|x|-0.5, &|x|\ge1
\end{cases}
$$

![img](img/v2-4edbd47a9cd0cf5a4637e84c557603a3_1440w.png)

### 交叉熵损失(Cross Entropy Loss)

二分类情况时，每个样本的$y_i$只会是$0$或$1$，模型输出是属于正类的概率$p$，此时第$i$个样本的损失为
$$
L_i = -[y_i \ln(p_i) + (1-y_i) \ln(1-p_i)] =
\cases{
-\ln(p_i), & $y_i=1$ \\
-\ln(1-p_i), & $y_i=0$
}
$$
由于$p_i<0$，这个损失永远是正值。

多分类情况时，假设第$i$个样本的真实类别是$k$，该样本的损失为
$$
L_i = -\frac1c\sum_{i=1}^c y_{ij}\ln(p_{ij}) \propto -\ln(p_{ik})
$$
其中$y_{ij}$在第$i$样本真实类别为$j$时取$1$，否则取$0$；$p_{ij}$是第$i$样本真实被判断为第$j$类别的概率。

从物理意义上解释，如果说信息熵是消除不确定性的最小代价（或曰使用最优策略的代价），那么交叉熵就是在局面未知时，使用假定局面对应的最优策略时，实际付出的代价。多数情况下，我们并不知道系统的真实分布，如抛硬币例子，真实情况是两面出现的概率相同，但我们不知道这一信息，以为两面不一样，正面出现的概率是$p$，反面则是$1-p=q$，这就是一个非真实分布，在此假定下实施最优策略，实际付出的代价就是交叉熵。

在数学里，交叉熵的定义是$H(p,q)=\sum_x p(x)\ln\frac{1}{q(x)}$，这可以看作是在描述分布$p$与$q$的相似度。

所谓的“假定局面”其实就是预测出来的分布，“真实局面”是数据的真实分布，容易想见，这两个分布越接近，实际付出的代价就越靠近最小代价。所以交叉熵可以用来描述两个分布是否接近，也就能衡量分类模型的预测结果了。

### 相对熵/KL散度

设 p(x)、q(x) 是 离散随机变量 X 中取值的两个概率分布，则 p 对 q 的相对熵是：

![img](D:\GithubRepos\notes_about_datascience\note\img\v2-ca0d22b25022c9b294d5306df43e04f1_1440w.png)

可以证明（此处略过过程），信息熵$H(p)$、交叉熵$H(p,q)$以及相对熵$D_{KL}(p||q)$之间有关系
$$
H(p,q)=H(p)+D_{KL}(p||q)
$$
这表明交叉熵等于P的熵加上P相对于Q的KL散度。按照前述的物理意义，相对熵/KL散度的物理意义就是，使用假定局面q所制定的策略，相对真实局面p所对应的最优策略所多付出的代价。值得注意的是，p与q的相对关系是确定的，因此p与q不对称.

在机器学习中，相对熵的计算公式我没找到。（本来想根据交叉熵类比，但是失败了。）

