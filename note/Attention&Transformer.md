从RNN到Encoder-Decoder，再到Attention：https://zhuanlan.zhihu.com/p/28054589

对于Attention中的权重$a_{ij}$如何计算，给出了细节；之后还给出了attention的更多种类：https://zhuanlan.zhihu.com/p/380892265

上文中关于global attention和local attention说的不够清楚，这个清楚一些：https://blog.csdn.net/weixin_40871455/article/details/85007560

transformer讲解，侧重attention的矩阵乘法：https://zhuanlan.zhihu.com/p/311156298

self-attention的内在原理讲解：https://www.zhihu.com/column/p/410776234

同样是self-attention，内容不多，但有很多好看的无水印图：https://blog.csdn.net/Lamours/article/details/125192046

transformer的另一个讲解，细节很多：https://zhuanlan.zhihu.com/p/338817680

更多关于transformer的图：https://blog.csdn.net/weixin_44305115/article/details/101622645