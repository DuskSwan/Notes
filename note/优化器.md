（本文内容在生成式AI的帮助下完成）

# 优化问题与优化算法

在数学中，一个优化问题通常被定义为寻找某个函数的最小值或最大值。具体来说，这涉及到以下几个主要组成部分：

1. **目标函数**：这是需要被最大化或最小化的函数。在优化术语中，寻找最大值的问题被称为最大化问题，寻找最小值的问题被称为最小化问题。
2. **变量**：这些是影响目标函数值的参数或决策变量。
3. **约束条件**（可选）：这些条件限制了变量的取值范围。约束可以是等式或不等式形式。

优化问题可以形式化为：

- 寻找 $x$ 使得 $f(x)$ 最小（最大可以转化成最小，所以无需考虑）
- 满足给定的约束条件 $g_i(x) \leq 0$（不等式约束）和 $h_j(x) = 0$（等式约束）

在机器学习中，训练模型基本上是一个优化问题，目的是找到模型参数的最佳值，使得损失函数（目标函数）最小化。损失函数衡量的是模型预测值与真实值之间的差距。例如，在监督学习中，常用的损失函数包括均方误差（回归问题）和交叉熵损失（分类问题）。

事实上超参数也是参数，也可以使用一些优化算法寻优，不过这就是参数寻优的内容了。



# 梯度下降法种种

根据数学知识，一个函数（一元或者多元）的梯度方向是其函数值增加的方向，所以让自变量沿着负梯度方向移动一小段，理论上会让函数值减小。在机器学习中，损失函数是模型参数的函数，我们希望迭代移动模型参数来降低损失函数值。

机器学习中梯度下降法的一般流程如下：

---

1. 首先，初始化模型的参数$\theta$，通常用随机值或零开始。初始参数记为 $\theta_0$。

2. 选择一个学习率 $\eta$，这是一个正数，决定了在梯度指示的方向上前进的步长大小。学习率的选择关键影响优化过程的收敛性和速度。

3. 计算损失函数 $L(\theta)$ 相对于参数 $\theta_t$ 的梯度。这个梯度 $\nabla_\theta L(\theta_t)$ 表示了损失函数在当前参数点 $\theta$ 的最陡增长方向。

4. 使用梯度下降的核心更新规则来调整参数，以减少损失函数的值，更新公式为$\theta_{t+1} = \theta_t-\eta \nabla_\theta L(\theta_t)$ 。这个更新步骤是向梯度的反方向，即最陡下降方向，以期望减少损失函数的值。

5. 重复步骤3和4，直到满足收敛条件，比如梯度的绝对值小于一个预设的阈值，或者达到预定的迭代次数。

6. 当达到停止条件（如梯度足够小或已经迭代足够多的次数）时，停止迭代。此时的参数 $\theta$ 被视为损失函数的局部最小值点的估计。

---

我们注意到，在上述流程中，每一步迭代都要计算一次损失函数，而计算损失函数需要比较预测值与真实值的误差，考虑我们手里有很多样本，想要让模型从每个样本中学习到规律，就需要让每个样本都计入损失，因此损失函数考虑的是手头所有样本的平均损失。

应用这个最普通的梯度下降算法，每一轮迭代都是在**整个训练集**上计算的，因此它也叫**批次梯度下降**（BGD, Batch gradient descent）。数据集比较大，可能会面临内存不足问题，而且其收敛速度一般比较慢。

我们自然地思考，能不能不用”全部“样本去计算损失，而是只用一部分？答案是可以的，我们可以每次迭代只用**一个随机样本**的数据，只要迭代的次数足够多，大数定律会保证梯度的期望达到最小。这种策略称为**随机梯度下降**（SGD, stochastic gradient descent）。SGD的问题在于不同样本算出的梯度有较大方差，网络训练不稳定。

在实际应用中，为了提高计算效率和稳定训练，实际使用的是**小批量随机梯度下降**（MBGD, Mini-batch gradient descent），它是前二者的折中方案，每次迭代选取训练集中**一批样本**样本计算。这样可以保证网络训练速度不太慢，也能使训练方向不至于偏离太多，具有一定稳定性。（当使用小批量梯度下降时，通常也使用SGD这个术语。）

对于给定的批次数据，计算每个样本的损失函数，然后求这些损失的平均值。现在梯度是这个平均损失相对于模型参数的导数，实际上也就是每个样本求梯度后取平均。数学表达为：
$$
∇_θJ(θ)=\frac1m\sum_{i=1}^m∇_θL(y_i,f(x_i,θ))
$$


其中 $m$ 是批次中的样本数量（batch size），$L(y_i, f(x_i, \theta))$ 是第 $i$ 个样本的损失，$y_i$ 是真实标签，$f(x_i, \theta)$ 是模型预测，$\theta$ 是模型参数。



# 动量法Momentum


动量（Momentum）是一种用于加速梯度下降算法的技术，特别是在面对高曲率、小但一致的梯度或带有噪声的梯度时，动量的引入有助于梯度下降法摆脱局部最优解，在相关方向上加速学习，同时抑制振荡，从而加快收敛速度。

在标准的梯度下降中，每一步的更新直接由当前步的梯度决定。而在动量方法中，每一步的更新不仅取决于当前步的梯度，还取决于前一步的更新。这使得更新过程能够累积之前的梯度方向，从而在一定程度上克服了局部最优的问题，并加速学习过程。

带有动量的梯度下降法流程如下

---

1. 初始化：选择初始参数 $\theta$ 和初始速度 $v$（通常初始化为0）。

2. 设置参数：选择学习率 $\eta$ 和动量因子 $\gamma$（常见值如 0.9）。

3. 迭代更新：对于每一次迭代，进行如下计算：

   - 计算损失函数关于参数 $\theta_t$ 的梯度 $\nabla_\theta J(\theta_t)$。
   - 更新速度 $v$：$v_{t+1} = \gamma v_{t} - \eta \nabla_\theta J(\theta_t)$
   - 更新参数 $\theta$：$\theta_{t+1} = \theta_t + v_{t+1}$

---

动量方法的关键在于引入了速度变量 $v$，它是之前梯度的指数衰减平均。动量因子 $\gamma$ 决定了历史信息保留的程度。较高的 $\gamma$ 值意味着保留更多的历史梯度。每次更新时，当前梯度与之前累积的梯度共同决定了参数的更新方向。这种方式有助于加速收敛，尤其是在梯度的方向一致时，同时还能在某种程度上减少更新过程中的振荡。

由于动量的引入不涉及梯度计算，在计算梯度$\nabla_\theta J(\theta)$时还是有BGD、SGD、MBGD等几种选择，从而导致SGD-M、MBGD-M等具体的迭代策略。

动量是许多现代优化算法，如Adam和Nesterov加速梯度（NAG），的基础组成部分。这些方法在处理复杂的优化问题时通常比基本的梯度下降表现更优。




# Adam

Adam（Adaptive Moment Estimation）是一种用于深度学习应用中的优化算法，由Diederik P. Kingma和Jimmy Ba在2014年提出。Adam结合了两种扩展的随机梯度下降算法的优点：动量方法（Momentum）和RMSprop。它被广泛用于训练各种神经网络，因为它相对于其他优化方法通常更快收敛，并且对超参数的初始选择不那么敏感。

### Adam的主要特点

1. **自适应学习率**：Adam调整每个参数的学习率，根据参数的估计第一（均值）和第二（未中心化的方差）矩来调整。
2. **计算高效**：与其他自适应学习率方法相比，Adam需要的内存较少，计算上也很高效。
3. **适用范围广**：在许多不同类型的机器学习问题上表现良好，包括具有大数据集或参数多的情况。

### Adam的算法步骤

Adam的更新规则使用两个移动平均值：梯度的一阶矩（均值）和二阶矩（方差）。这些矩被用来调整每个参数的学习率。

1. **初始化**：设置参数 $\theta$ 的初始值；选择一个学习率 $\eta$（通常在0.001左右）；设置两个衰减因子 $\beta_1$ 和 $\beta_2$，通常取值为 0.9 和 0.999；初始化一阶矩向量 $m_0$ 和二阶矩向量 $v_0$ 为零向量；初始化时间步 $t = 0$。

2. **计算梯度**：在每个时间步 $t$，计算损失函数 $J(\theta)$ 关于参数 $\theta$ 的梯度 $g_t$，例如 $g_t = \nabla_\theta J(\theta_t)$。

3. **更新偏差校正的一阶和二阶矩估计**：

   - 更新一阶矩（均值）：$m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$
   - 更新二阶矩（方差）：$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$

4. **计算偏差校正的矩**：为了抵消低估的估计，使用如下偏差校正矩：

   - 偏差校正一阶矩：$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$
   - 偏差校正二阶矩：$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

5. **更新参数**：使用偏差校正的一阶矩和二阶矩来调整每个参数：$θ_{t+1}=θ_t−η⋅\frac{\hat m_t}{\sqrt{v_t}+ϵ}$

   其中 $\epsilon$ 是一个非常小的数（例如 $10^{-8}$），防止除以零。

### 使用场景

Adam因其效率和相对较少的内存需求而受到许多研究者和开发者的青睐。它在处理稀疏梯度（如在自然语言处理和计算机视觉任务中常见）或非常不规则的数据分布上表现尤为出色。由于其自适应特性，它对超参数的初始选择非常鲁棒，使得调优过程较为简单。