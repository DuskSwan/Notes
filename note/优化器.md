（本文内容在生成式AI的帮助下完成）

# 优化问题与优化算法

在数学中，一个优化问题通常被定义为寻找某个函数的最小值或最大值。具体来说，这涉及到以下几个主要组成部分：

1. **目标函数**：这是需要被最大化或最小化的函数。在优化术语中，寻找最大值的问题被称为最大化问题，寻找最小值的问题被称为最小化问题。
2. **变量**：这些是影响目标函数值的参数或决策变量。
3. **约束条件**（可选）：这些条件限制了变量的取值范围。约束可以是等式或不等式形式。

优化问题可以形式化为：

- 寻找 $x$ 使得 $f(x)$ 最小（最大可以转化成最小，所以无需考虑）
- 满足给定的约束条件 $g_i(x) \leq 0$（不等式约束）和 $h_j(x) = 0$（等式约束）

在机器学习中，训练模型基本上是一个优化问题，目的是找到模型参数的最佳值，使得损失函数（目标函数）最小化。损失函数衡量的是模型预测值与真实值之间的差距。例如，在监督学习中，常用的损失函数包括均方误差（回归问题）和交叉熵损失（分类问题）。

事实上超参数也是参数，也可以使用一些优化算法寻优，不过这就是参数寻优的内容了。

# 梯度下降法与批次梯度下降

根据数学知识，一个函数（一元或者多元）的梯度方向是其函数值增加的方向，所以让自变量沿着负梯度方向移动一小段，理论上会让函数值减小。在机器学习中，损失函数是模型参数的函数，我们希望迭代移动模型参数来降低损失函数值。

机器学习中梯度下降法的一般流程如下：

---

- 首先，初始化模型的参数$\theta$，通常用随机值或零开始。初始参数记为 $\theta_0$。
- 选择一个学习率 $\eta$，这是一个正数，决定了在梯度指示的方向上前进的步长大小。学习率的选择关键影响优化过程的收敛性和速度。

- 计算损失函数 $L(\theta)$ 相对于参数 $\theta_t$ 的梯度。这个梯度 $\nabla_\theta L(\theta_t)$ 表示了损失函数在当前参数点 $\theta$ 的最陡增长方向。

- 使用梯度下降的核心更新规则来调整参数，以减少损失函数的值，更新公式为$\theta_{t+1} = \theta_t-\eta \nabla_\theta L(\theta_t)$ 。这个更新步骤是向梯度的反方向，即最陡下降方向，以期望减少损失函数的值。

- 重复步骤3和4，直到满足收敛条件，比如梯度的绝对值小于一个预设的阈值，或者达到预定的迭代次数。

- 当达到停止条件（如梯度足够小或已经迭代足够多的次数）时，停止迭代。此时的参数 $\theta$ 被视为损失函数的局部最小值点的估计。

---

我们注意到，在上述流程中，每一步迭代都要计算一次损失函数，而计算损失函数需要比较一个样本的预测值与真实值的误差，换言之每次迭代需要一个样本的数据（这个样本可以重复使用）。

在实际应用中，为了提高计算效率和稳定训练，实际使用的是**批量随机梯度下降**，这样做的目的是利用小批量数据的统计性质来提高训练的效率和稳定性。每次梯度下降使用的不是一个样本，而是一批（batch）样本。

对于单个样本的优化算法，梯度是基于该单一样本的损失函数计算得到的。要将算法扩展到一个batch，需要调整梯度计算，使其基于整个批次的样本来进行：

对于给定的批次数据，计算每个样本的损失函数，然后求这些损失的平均值。现在梯度是这个平均损失相对于模型参数的导数。数学表达为：
$$
∇_θJ(θ)=\frac1m\sum_{i=1}^m∇_θL(y_i,f(x_i,θ))
$$


其中 $m$ 是批次中的样本数量（batch size），$L(y_i, f(x_i, \theta))$ 是第 $i$ 个样本的损失，$y_i$ 是真实标签，$f(x_i, \theta)$ 是模型预测，$\theta$ 是模型参数。



随机梯度下降法(Stochastic Gradient Descent)



# Adam

Adam（Adaptive Moment Estimation）是一种用于深度学习应用中的优化算法，由Diederik P. Kingma和Jimmy Ba在2014年提出。Adam结合了两种扩展的随机梯度下降算法的优点：动量方法（Momentum）和RMSprop。它被广泛用于训练各种神经网络，因为它相对于其他优化方法通常更快收敛，并且对超参数的初始选择不那么敏感。

### Adam的主要特点

1. **自适应学习率**：Adam调整每个参数的学习率，根据参数的估计第一（均值）和第二（未中心化的方差）矩来调整。
2. **计算高效**：与其他自适应学习率方法相比，Adam需要的内存较少，计算上也很高效。
3. **适用范围广**：在许多不同类型的机器学习问题上表现良好，包括具有大数据集或参数多的情况。

### Adam的算法步骤

Adam的更新规则使用两个移动平均值：梯度的一阶矩（均值）和二阶矩（方差）。这些矩被用来调整每个参数的学习率。

1. **初始化**：设置参数 $\theta$ 的初始值；选择一个学习率 $\eta$（通常在0.001左右）；设置两个衰减因子 $\beta_1$ 和 $\beta_2$，通常取值为 0.9 和 0.999；初始化一阶矩向量 $m_0$ 和二阶矩向量 $v_0$ 为零向量；初始化时间步 $t = 0$。

2. **计算梯度**：在每个时间步 $t$，计算损失函数 $J(\theta)$ 关于参数 $\theta$ 的梯度 $g_t$，例如 $g_t = \nabla_\theta J(\theta_t)$。

3. **更新偏差校正的一阶和二阶矩估计**：

   - 更新一阶矩（均值）：$m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$
   - 更新二阶矩（方差）：$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$

4. **计算偏差校正的矩**：为了抵消低估的估计，使用如下偏差校正矩：

   - 偏差校正一阶矩：$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$
   - 偏差校正二阶矩：$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

5. **更新参数**：使用偏差校正的一阶矩和二阶矩来调整每个参数：$θ_{t+1}=θ_t−η⋅\frac{\hat m_t}{\sqrt{v_t}+ϵ}$

   其中 $\epsilon$ 是一个非常小的数（例如 $10^{-8}$），防止除以零。

### 使用场景

Adam因其效率和相对较少的内存需求而受到许多研究者和开发者的青睐。它在处理稀疏梯度（如在自然语言处理和计算机视觉任务中常见）或非常不规则的数据分布上表现尤为出色。由于其自适应特性，它对超参数的初始选择非常鲁棒，使得调优过程较为简单。