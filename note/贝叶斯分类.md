# 贝叶斯判别/贝叶斯决策论

利用到了先验的信息，即可称之为贝叶斯判别。这一部分内容可谓是纯统计学的知识。

### 贝叶斯定理

为了方便理解和记忆，先给出贝叶斯定理。

假设有随机事件A和B，且B发生的概率$P(B)$不为0，那么就有条件概率
$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$
将A与B发生的概率$P(A),P(B)$称为先验概率。条件概率$P(A|B),P(B|A)$则是后验概率。

这个公式可以这样理解：我们原本知道A的发生概率为$P(A)$，现在观察到了事件B发生，同时我们知道事件B和事件A有一定的关系，B的发生对A的发生有一定的促进或者抑制作用，我们想知道，如果B发生了，那么A的“修正的”发生概率$P(A|B)$是什么呢？

这个修正的发生概率是条件概率，条件概率的计算方式则是P(事件与条件都发生)/P(条件发生)。因此分母就是条件B发生的概率；考虑分子时则将“事件与条件都发生”拆解成了“先发生事件，在此基础上再发生条件”。

更一般的，考虑这样的情况：条件B还是条件B，我们要预测的事件却变成了一系列$\{A_1,A_2,\cdots,A_n\}$。这时对公式进行一点点修改，将条件B的发生概率也用含有$A_i$式子表示：
$$
P(A_i|B) = \frac{P(A_i)P(B|A_i)}{P(B)}
=\frac{P(A_i)P(B|A_i)}{\displaystyle\sum_{j=1}^nP(B|A_j)P(A_j)}
$$
后面的等号成立依赖于全概率公式的成立，这要求$A_1,A_2,\cdots,A_n$互不相容，且$P(B|A_j)$均不为零，也即$B \sub \bigcup_{j=1}^nA_j$。一般会让$\{A_j\}$的性质更强，让它是完备事件组，也即其并是全事件空间。

这带给我们一个启示，假使我们要比较事件$A_i$与事件$A_j$谁发生的概率大，除了可以直接比较先验概率，还可以通过比较$P(A_i)P(B|A_i)$和$P(A_j)P(B|A_j)$的方式来比后验概率，这比先验概率要更加有说服力。

事件和随机变量/向量之间往往可以相互转化。对于连续型随机向量$X,Y$，有着长相相似的结论
$$
f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)} = 
\frac{f_{Y|X}(y|x)f_X(x)}{\int_{-\infty}^\infty f_{Y|X}(y|x)f_X(x)}
$$
其中$f_{X|Y}(x|y)$是条件$Y=y$下$X$的条件（概率）密度，$f_Y(y)$是$Y$的边缘密度，$f(x,y)$是随机向量$(X,Y)$的联合概率密度。

### 最大后验概率法

对于归入不同类别的概率，我们往往先验的认识（比如，相对于一个小的类，样本更容易被归入很大的类）。所以，我们考虑在此基础上的，被归入不同类的概率。

假设已知的组别是$\pi_1,\pi_2,...,\pi_k$，每个组有概率密度$f_i(x)$（这要求我们对于每个类的分布有着假定，也即这是个参数化的方法，在理解时，可以假定每个组服从一个高斯分布）。已知任一样本被归入类$\pi_i$的先验概率为$p_i,i=1,2,...,k$，满足$p_1+\cdots+p_k=1$。现在，新样本$x$已知时，它属于类$\pi_i$的后验概率为
$$
P(\text{归入}\pi_i|x)=\frac{p_if_i(x)}{\displaystyle\sum_{j=1}^k p_jf_j(x)}
$$
使得这个值最大的$\pi_i$当然就是$x$应该被归入的类。

这个式子我们不加证明了，可以类比贝叶斯定理的一般形式来理解，事件$A_j$是样本$x$归于$\pi_j$，条件是样本$x$的取值，$p_j$是先验的$x$归于$\pi_j$的概率也即$P(A_j)$，$f_j(x)$则描述了在组$\pi_j$中取到值$x$的机会大小也即$P(B|A_j)$。

### 最小期望误判代价法

除了先验的归类概率，将类别误判所造成的代价也会各不相同（比如将正常人诊断为有病还好，将病人诊断为正常则很糟）。因此我们希望误判代价大的情况尽量不发生。

假设已知的组别是$\pi_1,\pi_2,...,\pi_k$，每个组有概率密度$f_i(x)$。任一样本被归入类$\pi_i$的概率为$p_i,i=1,2,...,k$，满足$p_1+\cdots+p_k=1$。此外，我们还规定了将第类$\pi_i$误判为类$\pi_l$的“代价”为$c(l|i)$（$l=i$时当然代价是0）。

一旦我们给出判别规则“若$x\in R_i$，则归入类$\pi_i$”，就可以计算出将来自$\pi_i$的样本$x$判为$\pi_l$的条件概率为$P(l|i)=\int_{R_l}f_i(x){\rm d}{x}$，所以误判代价的均值就是$ \displaystyle\sum_{l=1}^k c(l|i) P(l|i)$，进而总误判代价的期望值为
$$
ECM(l|i)=E\left[\sum_{l=1}^k c(l|i) P(l|i)\right]
=\sum_{i=1}^k p_i\sum_{\substack {l=1\\l\neq i}}^k c(l|i) P(l|i)
$$
我们需要选择合适的$R_1,...,R_k$来使得上式最小。可以证明，这样的判别规则是【使得$\sum_{\substack {j=1 \\ j \neq l}}^k p_j c(l|j) f_j(x)$最小的类$\pi_l$，就是$x$应该归入的】。这个指标$\sum_{\substack {j=1 \\ j \neq l}}^k p_j c(l|j) f_j(x)$可以看作是归入第$\pi_l$类的平均代价，这个代价其实就是归于其他类的概率$p_j(x)f_j(x)$与代价$c(l|j)$之积的求和。

记
$$ {align}
R_l 
& = \sum_{\substack {j=1 \\ j \neq l}}^k p_j c(l|j) f_j(x) \\
& = \sum_{j=1}^k p_j c(l|j) f_j(x) \\
& = \sum_{j=1}^k c(l|j)P(c_l|x) \\
& = \sum_{j=1}^k \lambda_{lj} P(c_l|x) \\
$$ {align}
为将$x$归入类$c_l$的条件风险，$x$实际会被归入使$R_l$最小的类$c_l$，这就是最小期望误判代价法。



# 朴素贝叶斯分类

用概率论的眼光看待机器学习，所有的分类其实都是贝叶斯判别。错判代价$\lambda_{ij}$是由损失函数给出的，可以直接假设错判到任何类别的代价都相同，也即
$$
\lambda_{ij} = \begin{cases}
0, & i=j \\
1, & i\neq j
\end{cases}
$$
这样就有$R_i=\sum_{j=1}^k \lambda_{ij}P(c_i|x)=1-P(c_i|x)$（由于$c_i$是完备事件组）。

我们想要将贝叶斯的思想用到有监督分类中去，虽然上述理论很完善，但是现实中遇到一个问题，那就是$P(c_i|x)$求不出来，因而$R_i$求不出来。

一些模型放弃了求$P(c_i|x)$，直接对$R_i$（或者说$P(c_i|x)$）建模来一步到位地求出$x$属于各个类别的代价（或者说概率），比如决策树、支持向量机、神经网络等。

另一些模型却试图近似出$P(c_i|x)$，这就是朴素贝叶斯分类器。

为了求出$P(c_i|x)$，首先考虑用贝叶斯定理将其改写成
$$
P(c|x)=\frac{P(c)P(x|c)}{P(x)}
$$
（我删去了下标$i$以便书写，这不影响其含义）此时$P(x)$是样本$x$是出现率，这可以用样本频率来估计；$P(c)$是类别$c$出现的概率，同样可以用频率估计；只剩下$P(x|c)$没有解决。

若要用频率估计$P(x|c)$，会遇到一个问题，“类别$c$中出现样本$x$”的概率不大，所以采样时很可能遇不到这样的样本，若以频率估计，这个值就是0了，会产生很大误差。

朴素贝叶斯分类器采用了“属性条件独立性假设”，设$x$的每个属性（分量）相互独立地对分类结果产生影响（换言之分量之间没有相关性），这样一来$P(x|c)$可以拆解为$\prod_{j=1}^kP(x_i|c)$。现在就可以用“类别$c$中出现属性$x_i$”的频率来估计$P(x_i|c)$了。

对于离散型变量，用频率估计概率。对于连续性变量则可以假设其服从正态分布，用均值和方差代入，便可直接根据正态分布的分布函数求出概率。

由于对所有类别来说$P(x)$是常数，因此朴素贝叶斯分类器的判别式就是
$$
f(x) = \operatorname*{argmax}\limits_{c} P(c)\prod_{j=1}^kP(x_i|c)
$$

> 这里肯定会有人问，为什么$P(x|c)$不能用频率估计，而$P(x_i|c)$就可以？关键在于$x$是高维的，所在的空间过于庞大，即便每个分量的取值都是有限多，$x$可取的值也是$2^{维数}$级别的。而$x_i$的取值有限多时，是真的很有限啊！出现事件$x_i|c$的机会还是很大的。

> 补充另一个细节，当事件$x_i|c$没有出现时，一个$P(x_i|c)=0$会直接将整体拉成0，这是不合理的。所以在用频率估计概率时实际会加以修正，一个常用策略是拉普拉斯修正：
> $$
> \hat P(c) = \frac{|D_c|+1}{|D|+k}(\neq\frac{|D_c|}{|D|}) \\
> \hat P(x_i|c) = \frac{|D_{c,x_i}|+1}{|D_c|+N_i}(\neq\frac{|D_{c,x_i}|}{|D_c|}) \\
> $$
> 其中$D$是样本总集，$D_c$是类别为$c$的样本集，$k$是类别总数，$D_{c,x_i}$是类别为$c$且第$i$属性取$x_i$的样本集，$N_i$是第$i$属性可能的取值数。