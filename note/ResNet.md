# 可参考内容

论文原文：extension://oikmahiipjniocckomdccmplodldodja/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1512.03385.pdf#=&zoom=110.00000000000001



基本思路的讲解：https://zhuanlan.zhihu.com/p/31852747

我觉得它写的并不那么好，对前因后果的叙述不够详细，等找到更好的再替换他。



一些细节：https://blog.csdn.net/u014665013/article/details/81985082

提到了x与F(x)相加时的处理方式，详细描述了程序实现中的维数变化



torch代码例子：

https://blog.csdn.net/qq_17550379/article/details/80359471

https://bbs.huaweicloud.com/blogs/409936

还没细看



# 思路

ResNet的核心思想是每一层都可以表示为学习残差函数$F(x)$，而不是学习一个直接的映射函数$H(x)$。在理想情况下，如果一个恒等映射是最优的，那么让层去逼近0的残差比去逼近恒等映射要容易得多。因此，ResNet让这些层去拟合残差$F(x)=H(x)−x$，而非直接去拟合输出$H(x)$。这种结构称为“残差块”，它允许信息通过“捷径”直接传递，避免了深度网络中梯度消失的问题。



# 实现

假设我们想要的映射为$H(x)$，我们把网络层设计成学习一个残差映射$F(x):=H(x)−x$。因此，原始的映射就被重写为$F(x)+x$。以下面的结构为例，假设网络是一个简单的全连接层：

![image-20240401210021539](D:\GithubRepos\notes_about_datascience\note\img\image-20240401210021539.png)

实际的forward过程就是
$$
F(x) = W_2\sigma(W_1x+b_1)+b_2 \\
H(x) = \sigma(F(x) + x)
$$
上述例子中的$F(x)$与$x$是向量。但ResNet作为一种思想，可以用在各种网络层上，比如在卷积神经网络中，它们是特征图，$F(x)$与$x$可能是特征图等等。

当$F(x)$与$x$的形状不一致时，直接相加会遇到问题。在ResNet中，这个问题通过引入一个调整维度的操作来解决，这个操作通常在捷径连接（shortcut connection）中实现。

这个调整维度的操作通常有两种方式：

1. 卷积调整: 使用一个卷积层来改变$x$的维度（比如通道数、高度、宽度等），使其与$F(x)$的维度匹配。这个卷积层通常有一个$1 \times 1$的卷积核，步长和$F(x)$的计算过程中使用的步长相匹配，以确保空间维度的一致性。这种方式不仅能调整通道数，还能调整特征图的尺寸。但这样会增加参数，也会增加计算量。
2. 零填充调整: 如果只是通道数不一致，而空间维度（即高度和宽度）相同，另一种较为简单的方式是通过在通道维度上添加额外的零来增加$x$的通道数，使其与$F(x)$匹配。这种方式不引入额外的参数，但仅限于通道数的调整。

这两种方式中，卷积调整更为通用，可以处理通道数和空间维度的不一致问题，而零填充调整则更简单，适用于特定情况。在实际的ResNet实现中，通常采用卷积调整方法。
