# 模型集成方法

简单来说，集成方法是将多个模型以某种结构组合，以提高整体预测性能的方法。最经典的集成方法包括Bagging、Boosting、Stacking（堆叠）三种。

## Bagging

Bagging全称Boostrap Aggregating（直译过来是自助抽样后合计），其基本思路是，先从训练数据中有放回地随机抽样（所谓的自主抽样），获取多份训练数据，然后用这些训练数据训练多个基模型，这些基模型投票（对分类任务是投票，回归任务则取均值）来得到集成模型的输出结果。

该方法可以看作是通过重复抽样来减小预测的方差，适合提高方差大、偏差小的模型性能，比如决策树、神经网络。决策树采取Bagging方式的提升即为随机森林（随机森林还多做了一步，每次从所有属性中随机选择k个，而不是用全部属性）。

此外，Bagging的基模型是并行建立的。

## Boosting

Boosting的基本思路是，串行建立多个基模型，每个基模型建立后进行一次测试，测试中出错的样本，在训练下个基模型时会拥有更高的权重（体现为错误分类时损失更大），而最终建立的一系列基模型，通过加权投票的方式得到最终结果，其中基模型的权重是由他们独立预测时的性能决定的。如何反复调整样本的权重（或者说分布），用调整后的样本再建立新的基模型，直到基模型数量足够多，结果将由基模型加权后给出。

Boosting族的一个重要模型是AdaBoost，全称Adaptive Boosting，直译为自适应的Boosting，其中自适应指的是，它可以根据数据自行确定基模型（弱分类器）的误差上界。它是一个二分类模型，也即标签$y_i\in\{-1,+1\}$。

其流程如下：

- 开始时，所有训练样本的权重都被初始化为相等值。即，如果有N个样本，每个样本的初始权重为1/N。

- AdaBoost算法通过以下步骤迭代地训练弱学习器：

  - 基于当前的数据权重训练一个弱学习器$h_t(x)$。将样本权重记为$D_t=(D_t(1),\cdots,D_t(N))$，其中$D_t(i)$是第$i$样本的权重。

  - 计算这个学习器的加权错误率。针对第学习器$h_t$，错误率的计算公式为$\epsilon_t=\displaystyle \frac{\sum_i D_t(i)I_{[h_t(x_i)\neq y_i]}}{\sum_i D_t(i)}$，其中$I_{[h_t(x_i)\neq y_i]}$表示预测值$h_t(x_i)$等于真实值时取$0$，否则取$1$。（值得注意的是，$\epsilon_t<0.5$才有意义，因为该模型是二分类的，误差大于$0.5$说明还不如随机输出。在算法中，如果$\epsilon_t>0.5$就说明算法失效了。）

  - 根据错误率计算学习器的权重（学习器的重要性）。错误率越低，学习器的权重越高，计算公式为$\alpha_t=\frac12\ln(\frac{1-\epsilon_t}{\epsilon_t})$。（值得注意的是，由于保证$\epsilon_t<0.5$，实际有$\alpha_t>0$。）

  - 更新数据的权重，使那些被当前学习器错误分类的样本在后续迭代中获得更高的权重。对样本$x_i$，如果当前学习器的分类正确，也即$h_t(x_i)=y_i$，那么权重$D_t(i)$应该减小，变成$D_{t+1}(i)=D_t(i)*\exp(-\alpha_t)$；否则权重变大，$D_{t+1}(i)=D_t(i)*\exp(\alpha_t)$（变大/变小由$\alpha_t>0$保证着）。当然，变完以后还要进行归一化，保证$D_{t+1}(i)$们总和为一。

  - 重复上述步骤，直到达到预定的弱学习器数量$T$或者获得一个完美的分类器（误差$\epsilon_t=0$，虽然基本不可能）。

- AdaBoost算法将所有训练过程中产生的弱学习器按照其权重进行组合，形成最终的强学习器$h(x)=\text{sign}(\sum_{t=1}^T\alpha_th_t(x))$。在进行分类决策时，AdaBoost基于所有弱学习器的加权投票结果来确定最终的分类。

原始AdaBoost只能用于二分类任务，不过可以在此基础上修改使得它能够完成多分类与回归。

顺带一提，梯度提升决策树（GBDT，Gradient Boosting Decision  Tree）也有“boost”的成分，但与AdaBoost等早期的Boosting方法不同，GBDT并不直接通过修改样本的权重来关注难以分类的样本，而是通过关注每一步的残差（或梯度）来优化模型。GBDT通过迭代地添加弱学习器（通常是决策树）来减少模型的残差，逐步提升模型的性能。

## Stacking

Stacking的基本思路是，用新的模型来弥补上一个模型的缺陷。其流程是，先将训练集$D$分为$D_1,D_2,\cdots$，用第一个基模型$f_1$在$D_1$上学习样本-标签的知识（用$(D_1,y)$记）后，将其运用到$D_2$上，现在它的输出$f_1(D_2)$作为新一层次的样本，用第二个基模型再来学习（也即让$f_2$学习$(f_1(D_2),y)$），之后再用第三个基模型……如此得到一系列模型，每个样本会经过全部基模型的处理，才最终得到输出。

与前述两种方法相比，Stacking的一大特点是，基模型的类型不必相同，比如，可以让支持向量机做第一层，决策树做第二层。