[toc]



# 宏观概念

## 统计学（一元与多元）

一元统计分析可以认为是研究一个随机变量的统计规律的学科，多元统计分析则是在研究多个随机变量之间相互依赖关系。大体上讲，是收集数据，提取数据中的信息，对所研究的问题作出推断。

Kendall在《多元统计》一书中，把多元统计研究的内容归为以下几类：降维问题、分类问题、变量间相互依赖关系、多元分布统计推断（参数估计与假设检验）、分布的理论基础。

又可分为参数统计与非参数统计。参数统计依赖“模型”，先假定数据服从某个模型，然后估计这个模型中的参数，或者对模型的有效性作出检验，因此统计学可以成为“建模”的手段。非参数统计不对数据满足的模型做出假设，只考虑对数据做出某些推断。

## 数据科学

研究数据的科学。研究手段既涵盖了统计的方法（推断、检验），又涵盖了机器的方法（设计算法），当然还有一些别的（比如具体领域内的专业知识）。可以看作是统计与机器学习的应用。

## 人工智能

人工智能 (Artificial Intelligence, AI) 可以被定义成一个像人类一样理性思考与行动的系统。不过，现在的人工智能范围极其广阔，能够完成特定的任务的机器系统，似乎都可以被冠以人工智能的名字。

机器学习是人工智能领域的一部分。人工智能可以用来做数据分析。事实上，我感觉现在用计算机做啥都叫个人工智能……这个词都被用烂了，它是涵盖范围最广泛的概念，具体关系应该是人工智能$\supset$机器学习$\supset$深度学习$\approx$神经网络。

## 机器学习

机器学习本质上是在构筑一个固定的“算法”，这个算法可以针对一些数据算出一些结论。具体来说，机器学习所做的事情是让机器从数据中确定一些参数，进而确定算法，进而处理新的数据。这样的算法可以来自一个统计学中的模型，也可以来自纯粹穷举，或者是什么其他思路。至于机器学习与统计的关系，应该说有所交集，但不相同。

从不同的角度看，机器学习方法有多种分类。可以分为参数方法和非参数方法；有监督方法和无监督方法；传统机器学习与深度学习，等等。

## 深度学习/神经网络

深度学习是一类以神经网络为架构，对资料进行表征学习的方法，因为神经网络有“层”的概念，越多层数（越深）的网络通常能力越强，所以称为深度学习。



# 对数据的描述与名词解释

数据通常会以表格的形式给出：

| 编号  | 属性x1   | 属性x2   | ...  | 属性xp   | 目标y |
| ----- | -------- | -------- | ---- | -------- | ----- |
| 个体1 | $x_{11}$ | $x_{12}$ | ...  | $x_{1p}$ | $y_1$ |
| 个体2 | $x_{21}$ | $x_{22}$ | ...  | $x_{2p}$ | $y_2$ |
| ...   | ...      | ...      | ...  | ...      | ...   |
| 个体n | $x_{n1}$ | $x_{n2}$ | ...  | $x_{np}$ | $y_n$ |

定义以下概念

- 数据集：记录数据的表格。通常每行记录一个个体的信息，每列记录不同个体的同属性数据。不包含目标列的数据可以用矩阵$X=(x_{ij})$记。

- 样本：数据集的一行是一个样本。可以用$x^{(i)}$来表示第$i$个样本（行向量）。

- 特征：数据集的每一列是一个特征。通常将其中一项认为是受其他特征影响的因变量，其他的是自变量。有时，仅仅把自变量列看作特征。用$x_i$来表示第$i$个特征（列向量）。

- 标签：作为预测目标的特征是标签，或者叫目标数组、因变量，可以用$y=(y_1,\cdots,y_n)'$记。

- 模型：自变量$x=(x_1,...x_p)'$与因变量$y$的关系即模型，模型本质上是函数，但未必能写出表达式。假设以$y=f(x)$记，一个最简单的例子是线性模型$f(x)=Wx+b$.

- 超参数：在选择模型类时就要确定的参数，比如是否中心化等。常记作$\theta$。这些参数由于各种原因，不适合根据数据来确定，需要人为指定。

- 参数：模型中待定的变量，需要根据数据求出一些值作为参数，使得模型的预测效果最好。在上例中是$W$和$b$。在确定参数之前，模型其实是模型空间中的一族函数，确定了参数之后，模型才是一个确定可用的函数。二者容易混淆，要根据语境判断。



# 机器学习的宏观思想

考虑一件事物的众多属性，比如一个城市有人口、GDP、面积、零售业总额、房价这些属性，我们认为其中一些是决定性的自变量，一些是被决定的因变量，如果能掌握这些属性之间的决定关系就好了，这就是“学习”希望做到的事。

用更加数学的语言来描述，仍然以城市为例子，假设我们认为人口、GDP、面积、零售业总额将决定着一座城市的房价水平，“希望找到属性之间的关系”，其实就是想要知道自变量$x=(x_1,x_2,x_3,x_4)=$（人口，GDP面积，零售额）与因变量$y=$房价之间的函数关系$y=f(x)$。最理想的情况是，我们能写出$f$的表达式。一个简单的例子是线性关系$y=a_1x_1+...+a_4x_4$，实际可能比这复杂得多。

那么，具体如何求出$f$呢？一般遵循这样的过程：首先对$f$的形式做出一些假设，比如假设$f$是线性函数$f(x)=a_1x_1+...+a_4x_4$（或者假设是非线性的$y=f(x)=a_1x_1^2+...+a_4x_4^2$，诸如此类吧，形式可以多种多样），确定了形式之后，还需要确定其中的参数$a_i$，这个表达式才是”已知“的。而要确定$a_i$，就得根据”已有“的数据来计算。这些参数统一用符号$\theta$来表示，在这个例子中，就是$\theta=(a_1,a_2,a_3,a_4)$。一个以$\theta$为参数的函数$f(x)$也可以写作$f_\theta(x)$。

我们需要一些已知的$x$与对应的$y$，不妨把我们已经知道的实例（已知样本）记为$(x^{(1)},y^{(1)})$（第一对实例）、$(x^{(2)},y^{(2)})$（第二对实例）、……我们希望求出来的那个$f$自然应该满足$y^{(i)}=f_\theta(x^{(i)})$，就算无法做到精确的相等，至少对于每一组实例，误差$\varepsilon_i=|y^{(i)}-f_\theta(x^{(i)})|$得尽量小吧。

假如一共有$n$个已知的样本，定义损失函数$L(\theta)=\frac1n \sum\varepsilon_i=\frac1n\sum|y^{(i)}-f_\theta(x^{(i)})|$，这个函数描述了在参数$\theta$下、模型$f_\theta$作用在已知的众多$x^{(i)}$上、所得到的结果$f(x^{(i)})$们与真实结果$y^{(i)}$们之平均误差，换言之，$L(\theta)$越小、误差越小、$\theta$越优秀。

于是，求参数$\theta$的过程变成了一个明确的数学问题：求一个使得平均误差在已知数据上最少的$\theta$，也即求
$$
\arg\min\limits_{\theta} L(\theta)= 
\arg\min\limits_{\theta} \frac1n\sum_{i=1}^n|y^{(i)}-f_\theta(x^{(i)})|
$$
求解这个最优化问题，将所得的$\theta$代入$f_\theta$，模型就随之确定了。现在，对于一个新的、没见过的数据$x^{(n+1)}$，我们就认为其对应的$y^{(n+1)}=f_\theta(x^{(n+1)})$。

在这个过程中，过去的已知样本$(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})$被用于确定模型的参数（因而确定模型），这一过程被称为“训练模型”，这些数据被称为“训练数据”。接着之前城市的例子，假设我们知道武汉、重庆、西安这三座城市各自的人口、GDP、面积、零售业总额（分别是$x^{(1)},x^{(2)},x^{(3)}$，每个$x^{(i)}$是一个四维向量）以及房价（分别是$y^{(1)},y^{(2)},y^{(3)}$），并选择线性模型$y=f(x)=a_1x_1+...+a_4x_4$，就可以用这三个样本来求解线性模型的参数$a_1,a_2,a_3,a_4$。之后，假设又获知了成都的人口、GDP、面积、零售业总额，但不知道房价，便可以认为$f(x_{成都})$就是$y_{成都}$.



# 建模过程中的概念

损失：

训练集：

验证集：

测试集：



# 数据分析的一般流程

数据分析的大致过程是数据预处理→选择模型→训练模型→评价模型，如果模型比较好就可以使用，否则重新选择模型。有时我们对数据服从的模型没有头绪，可以先进行数据感知，查看数据的分布特点，再做进一步打算。

数据预处理，指的是讲原始数据处理成便于后续操作的过程。其中有的是为了让后续操作具有可能性，比如缺失值处理、特征降维、离散特征连续化等；有的则是为了让模型具有更好的效果，比如去噪、归一化等。

数据感知，即用各种指标反应数据的特点。通常包括画散点图查看样本分布、计算协方差矩阵、计算特征与标签的相关来反应特征重要程度等等。

选择模型顾名思义，就是选择合适的模型类型，并确定超参数。

训练模型，指的是根据已知的数据，求出模型的参数使得模型达到最有效果。这本质上是个最优化问题，可以使用多种算法来解。比如使用梯度下降法，就是每读取一个（或一些）样本，便可以计算一次损失，用损失对参数的导数更新参数，如此用掉所有样本，就得到了最终的参数。

评价模型，也可称为模型诊断，指将一些数据带入求解出的模型算出一些指标，用指标衡量这个模型的好坏。这一步通常和训练模型结合进行，先将数据划分为训练集与测试集两部分，用训练集的数据求解参数，再用测试集的数据带入模型计算评价指标。还有交叉验证等更加复杂的方案。

在训练神经网络时，情况要复杂一些。由于训练神经网络需要的数据量非常大，通常要将全部数据分为很多小部分，每一部分称为一个batch，每次只向神经网络中输入一个batch，这才能够运行。如果全部的数据还不够多，就将这些数据重复使用，每一次使用全部数据称为一个epoch。实际上模型运行的次数是epoch_n * batch_n 这么多次，每次计算了batch_size条样本。

