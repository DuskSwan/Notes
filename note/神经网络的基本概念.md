[toc]



# 基本概念

在统计学中，一个输入数据集由很多条（行）向量（variable）组成，每个向量的分量实际上是自由变量的观测值（observation），这样一条数据叫样本（sample）。

在机器学习中，将一条（行）向量称为样本（sample），或者实例（instance）、观测（observation）、输入向量（input vector）、特征向量（feature vector），其每个分量称为特征（feature）。

在训练模型时，训练集往往要反复使用多次，每一次用全部训练样本进行一次训练，就称一个纪元（epoch）。在每个纪元中，训练样本被分成多组，每次只用一组，一组就叫一个批次（batch），批次的容量（batch size）应该是训练样本总数除以批次数目。比如一共有$50000$条样本，共训练$10$纪，每一纪分$50$批训练，每一批次的容量都应该是$1000$。

特别的，批次数目仅为1（也即批次容量是全部训练样本数）时称为批次梯度下降（batch gradient descent, BGD）；批次容量仅为1（也即批次数目是全部训练样本数）时称为随机梯度下降（stochastic gradient descent, SGD）；批次数目与容量均不为1时称为微型批次梯度下降（mini-batch gradient descent）。

> 为什么要训练多个epoch和分batch？
>
> 我们使用一个迭代过程即梯度下降来优化模型参数，一个epoch意味着整个数据集完整地通过了一次模型。通常情况下，仅训练一个epoch是不够的，因为这样模型只看了一次所有的数据，很难达到最优。多个epoch可以确保模型有足够的机会（迭代次数）学习数据集中的各种特征和模式。
>
> 分batch则有硬件和软件两方面好处：
>
> 1. **内存效率**：通常，训练数据集非常大，一次性加载所有数据进行训练需要非常大的内存，这对于大多数硬件来说是不可行的。通过分批处理，每次只加载一个小批次的数据，可以有效减少内存的使用。
> 2. **加速收敛**：使用小批量数据可以使得梯度更新更频繁，加快收敛速度。与每次只用一个样本更新模型（随机梯度下降）相比，批处理可以减少训练过程中的方差，使训练更稳定。
> 3. **避免局部最小值和鞍点**：相对于使用整个数据集的梯度下降，批处理由于其固有的噪声性，可以帮助模型跳出局部最小值或鞍点，寻找到更好的全局最小值。

# 神经网络的训练过程

神经网络的训练过程是一个系统且复杂的过程，涉及多个关键步骤。以下是从数据准备到模型训练完毕的详细步骤：

1. 数据收集与预处理

- **数据收集**：首先需要收集足够的数据来训练模型。这些数据可以来自多种来源，如在线数据集、实验测量或实时系统。
- **数据清洗**：清除数据中的错误、重复值或无用信息。
- **数据预处理**：包括归一化（将数据缩放到一定的数值范围，常见的如0到1或-1到1）、标准化（将数据转换为均值为0，标准差为1的分布）、编码类别数据、处理缺失值等。

2. 数据分割

- **训练集、验证集与测试集**：数据被分为三部分。训练集用于模型训练，验证集用于调整模型参数和防止过拟合，测试集用于在训练完成后评估模型的性能。

3. 模型设计

- **选择网络架构**：根据问题的性质选择适当的神经网络类型，如全连接网络、卷积神经网络、循环神经网络等。
- **配置层和节点**：设计网络的层数和每层的节点数。

4. 批处理数据

- **批次大小选择**：确定每个批次包含的样本数量。批次大小会影响训练的效率和模型的最终性能。

5. 编译模型

- **选择优化器**：如SGD、Adam等，优化器会影响模型的训练速度和效果。
- **设定损失函数**：如均方误差、交叉熵等，损失函数用来计算预测值和真实值之间的差距。
- **评价指标**：如准确率、召回率等，用于在训练过程中评估模型的表现。

6. 模型训练

- **前向传播**：数据通过网络，从输入层传到输出层。
- **损失计算**：计算输出层的预测值和真实值之间的差异。
- **反向传播**：根据损失函数对网络中的权重进行调整。
- **权重更新**：使用优化器根据反向传播得到的梯度更新权重。

7. 迭代优化

- **多个周期**：整个数据集通常需要多次通过网络（称为epoch），每次迭代都会尝试减少损失函数的值。

8. 模型评估与调整

- **使用验证集**：定期使用验证集检查模型性能，避免过拟合。
- **超参数调整**：调整学习率、批次大小、层数、节点数等参数，以获得更好的训练结果。

9. 测试与部署

- **模型测试**：在独立的测试集上评估模型性能。
- **模型部署**：将训练好的模型部署到实际应用中。

# 反向传播算法/梯度传播的实现

### 思路

反向传播算法(Backpropagation，简称BP算法)是“误差反向传播”的简称，是适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。梯度下降法是训练神经网络的常用方法，许多的训练方法都是基于梯度下降法改良出来的，因此了解梯度下降法很重要。梯度下降法通过计算损失函数的梯度，并将这个梯度反馈给最优化函数来更新权重以最小化损失函数。

梯度方向是函数上升最快的方向，使自变量沿着负梯度方向移动，就可以令函数值减小。如果用损失函数来评价模型的优劣，损失函数值越小就说明模型越好，那么我们需要让模型参数移动到某个最优解，使得损失函数值最小。

为了获取最优的模型参数$w$，我们需要用最终的损失函数$loss$对$w$求导，求出的导数（梯度）$\nabla_{loss}$应该是与$w$同形状的向量（矩阵、张量），这之后更新$w$为$w-\alpha\nabla_{loss}$，以此迭代得到最优的$w$。

为了实现上述的梯度下降法，我们总是需要求出$loss$对$w$的导数。从$w$算到$loss$会经过很多步骤，可以用链式法则来递归或者迭代地计算出全部中间变量的梯度，进而获取最终结果$loss$关于最初变量$w$的梯度。这一过程称为反向传播算法。

为了理解反向传播的过程，考虑下面的例子：

![image-20240430092125063](D:\GithubRepos\notes_about_datascience\note\img\image-20240430092125063.png)

其中：

- 输入数据：i1=0.05，i2=0.10 
- 初始权重系数：w1=0.15，w2=0.20，w3=0.25，w4=0.30，w5=0.40，w6=0.45，- w7=0.50，w8=0.55
- 隐藏层神经元：h1，h2 - 输出层神经元：o1，o2 
- 偏置项(bias)：b1=0.35，b2=0.60
- 激活函数：sigmoid

给出输入数据(i1, i2)=(0.05和0.10)，我们希望输出尽可能与原始输出(o1,o2)=(0.01和0.99)接近。

第一次前向传播，得到
$$
\begin{align}
h &= (h_1,h_2)^T = Wi + b = 
\end{align}
$$






![](D:\GithubRepos\notes_about_datascience\note\img\计算图.png)

有了计算图，就容易根据链式法则确定$y$关于$x$的导数如何分步骤计算。

为了实现反向传播算法，我们首先按照计算图正序，从各个叶子结点算出全部节点的值，这一过程叫前向传播。之后，沿着计算图从上往下，依次算出每个节点关于其邻接节点的导数（梯度），这之后立刻根据链式法则更新最终结果$y$关于该节点的邻接节点的梯度。如此一路延伸，最终就可以算得$y$关于叶子结点的梯度。每个节点保存下来的梯度，都是$y$关于其值的梯度。这一过程叫反向传播。

> 可以这样想，每个变量都是一个类，它有着值和梯度两个属性，分别记录自身的值以及关于loss的梯度。比如上图中有$loss=(y-\hat y)^2$，那么σ(y)，那么首先可以计算出

在实际中，由于输入输出都是数组，也即函数都是向量值函数而非一元函数，所以求出来的导数会有雅可比矩阵，乃至更高维数的张量。这在编写程序上是很难实现的，为了解决这个问题，我们规定，只能直接实现标量对张量的求导。这样一来，标量对张量的导数，就是由对分量的导数所形成的一个同维的张量，这是容易做到的。

然而，在计算过程中，难免会有张量形式的中间变量，张量对张量求导是难以避免的。为了在这种情况下依然能求出最终的标量对初始张量的导数，实际程序中求张量对张量的导数时，需要给出一个与自变量同形状的张量$v$作为参数，输出的则是这个参数与“张量对张量导数”的乘积。具体细节可以参考以下的例子。

>设$l=f(y),y=g(x)$，其中$l$是标量，$y=(y_1,...,y_m)^T$是$m$维向量，$x=(x_1,...,x_n)^T$是$n$维向量。为求出$\frac{\part l}{\part x}$，首先应该求出$\frac{\part l}{\part y}$与$\frac{\part y}{\part x}$。
>
>理论上我们知道，求出来的结果应该是
>$$
>\frac{\part l}{\part y}=\left( \frac{\part l}{\part {y_1}},...,\frac{\part l}{\part {y_m}} \right)^T \\
>\frac{\part y}{\part {x}}=
>\begin{pmatrix}
>\frac{\part {y_1}}{\part {x_1}} & \cdots & \frac{\part {y_1}}{\part {x_n}}\\
>\vdots & \ddots &\vdots \\
>\frac{\part {y_m}}{\part {x_1}} & \cdots & \frac{\part {y_m}}{\part {x_n}}\\
>\end{pmatrix}
>$$
>所以
>$$
>\frac{\part l}{\part x}=\frac{\part l}{\part y}\frac{\part y}{\part x}=
>\begin{pmatrix}
>\frac{\part l}{\part {y_1}} \\ \vdots \\ \frac{\part l}{\part {y_m}}
>\end{pmatrix}
>\begin{pmatrix}
>\frac{\part {y_1}}{\part {x_1}} & \cdots & \frac{\part {y_1}}{\part {x_n}}\\
>\vdots & \ddots &\vdots \\
>\frac{\part {y_m}}{\part {x_1}} & \cdots & \frac{\part {y_m}}{\part {x_n}}\\
>\end{pmatrix}
>=\begin{pmatrix}
>\frac{\part {l}}{\part {y_1}}\frac{\part {y_1}}{\part {x_1}}+\cdots+\frac{\part {l}}{\part {y_m}}\frac{\part {y_m}}{\part {x_1}} \\
>\vdots \\
>\frac{\part {l}}{\part {y_1}}\frac{\part {y_1}}{\part {x_n}}+\cdots+\frac{\part {l}}{\part {y_m}}\frac{\part {y_m}}{\part {x_n}} \\
>\end{pmatrix}
>$$
>注意，在上式中，矩阵相乘的方向并不对，缺少了一些转置符号。但是由于更高张量的求导本身就是高维数组，其方向可以任意的人为规定，所以总能通过规定写法，来使得表达式中不需要转置。所以此处略过方向上的细节，会意即可。
>
>由此可见，如果我们不想写出高维的$\frac{\part y}{\part {x}}$，又要计算出$\frac{\part l}{\part {x}}$，只要把$\frac{\part l}{\part {y}}$传递给$y$，直接通过$\frac{\part l}{\part {y}}$和$y=g(x)$来计算出结果$\frac{\part l}{\part {x}}$即可。体现在程序上，就是将$\frac{\part l}{\part {y}}$作为一个参数，在计算$$\frac{\part y}{\part {x}}$$时传递给计算用的函数Func，让这个Func直接返回$\frac{\part l}{\part x}$。
>
>为了方便理解，可以这样看待：在计算张量$y$对张量$x$的梯度（导数）时，函数Func需要接收一个与$y$形状相同的参数（实际上是最终目标$l$关于$y$的导数），记为$v$，在计算出导数$J=\frac{\part y}{\part x}$后，Func实际返回的是$v$与$J$的乘积。
>
>更进一步，可以把$J$看作是$(\frac{\part {y_1}}{\part x},\frac{\part {y_2}}{\part x},...,\frac{\part {y_m}}{\part x})$，也即是一个与$v$、与$y$都同形状的“向量”，其每个“分量”都是标量对$x$求导，因而也是与$x$同形状的张量。所谓的“$v$与$J$的乘积”，实际上是二者的对应项相乘再求和，得到的还是与$x$同形状的张量。

### 例子

下面将通过一个具体例子来理解反向传播的具体流程。下图是一个神经网络的结构图，

![image-20240428142410472](D:\GithubRepos\notes_about_datascience\note\img\image-20240428142410472.png)

其中：

- 输入数据：i1=0.05，i2=0.10 
- 初始权重系数：w1=0.15，w2=0.20，w3=0.25，w4=0.30，w5=0.40，w6=0.45，- w7=0.50，w8=0.55
- 隐藏层神经元：h1，h2 - 输出层神经元：o1，o2 
- 偏置项(bias)：b1=0.35，b2=0.60
- 激活函数：sigmoid

给出输入数据(i1, i2)=(0.05和0.10)，我们希望输出尽可能与原始输出(o1,o2)=(0.01和0.99)接近。

