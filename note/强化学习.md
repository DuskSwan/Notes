强化学习任务可以这样描述：

我们的机器面对着$X$这一状态空间，每个$x\in X$是一个状态；可采取的全部行为构成空间$A$，每个$a\in A$是一个可采取的行为。在状态$x$处采取行为$a$，可能会产生多种结果，假设产生行为$y$的概率为$p$，那么函数$P: X\times A\times X\mapsto \mathbb{R}$就描述了所有可能的转移概率。为了评价这个行为的好坏，定义奖励函数$R:X\times A\times X\mapsto \mathbb{R}$来描述一次转移的得分。

这样的环境$E=(X,A,P,R)$下，我们希望机器给出一个策略 (policy) $\pi$。策略有两种形式，确定性策略是函数$\pi:X\mapsto A,\pi(x)=a$，表示遇到状态$x$就采用行为$a$；随机性策略则是函数$\pi:X\times A\mapsto \mathbb{R},\pi(x,a)=p$，表示在状态$x$下有$p$的概率选择行为$a$，这里还要求有$\sum_a\pi(x,a)=1$。

为了评价一个策略的好坏，考虑长期执行策略后的累计奖励。假设第$t$步获得的奖励为$r_t$，共执行了$T$步，那么期望$E(\frac1T\sum_{t=1}^Tr_t)$便可以作为一个评价指标。当然，还可以定义更多指标。