[toc]



# 思路

考虑这样的情况：每个特征都不是“本质”的，而是其他一些更底层的变量（因子）的线性组合。也正是因为如此，所以特征之间存在相关性。我们希望找出这些更底层的因子来描述数据。

具体来说，对于一个随机向量$x=(x_1,x_2,...,x_p')$，有均值$\mu=(\mu_1,\mu_2,...,\mu_p)$与协方差矩阵$\Sigma=(\sigma_{ij})$，我们会假定它的分量$x_i$由$m$个因子$f_1,f_2,...,f_m$的线性组合，以及自身的特殊因素$\varepsilon_i$来决定，即
$$
\begin{cases}
x_1&=\mu_1+a_{11}f_1+a_{12}f_2+\cdots+a_{1m}f_m+\varepsilon_1 \\
x_2&=\mu_2+a_{21}f_1+a_{22}f_2+\cdots+a_{2m}f_m+\varepsilon_2 \\
&\vdots& \\
x_p&=\mu_p+a_{p1}f_1+a_{p2}f_2+\cdots+a_{pm}f_m+\varepsilon_p \\
\end{cases}
$$
或者写成矩阵
$$
x=\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_p
\end{pmatrix}
=\begin{pmatrix}
\mu_1 \\ \mu_2 \\ \vdots \\ \mu_p
\end{pmatrix}+
\begin{pmatrix}
a_{11} & \cdots & a_{1m}  \\ 
a_{21} & \cdots & a_{2m}  \\ 
\vdots & \ddots & \vdots  \\ 
a_{p1} & \cdots & a_{pm}  \\ 
\end{pmatrix}
\begin{pmatrix}
f_1  \\ \vdots \\ f_m
\end{pmatrix}+
\begin{pmatrix}
\varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_p
\end{pmatrix}
=\mu+Af+\varepsilon
$$
其中的$f_i$称（公共）因子，$\varepsilon_i$称特殊因子，系数$a_{ij}$称为载荷。并且为了让因子之间的信息不重叠，还假定
$$
\begin{cases}
E(f)=0 \\
E(\varepsilon)=0 \\
V(f)=I \\
V(\varepsilon)=D=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2) \\
\operatorname{Cov}(f,\varepsilon)=0
\end{cases}
$$
建立在以上全部假定基础上的模型，称为正交因子模型。

在实践中，终极目的是针对每组样本$x_i$给出对应的$f$。这通常需要先算出载荷矩阵的$A$，根据载荷的大小，可以对因子做出实际意义上的解释（这一点与主成分分析类似）。

# 性质

1. （A的计算式）由$x=\mu+Af+\varepsilon$可得$\Sigma=V(x)=V(Af)+V(\varepsilon)=AV(f)A'+V(\varepsilon)=AA'+D$，所以只要能对$\Sigma$找到合适的分解，就能得到$A$。与主成分分析同理，为了消除变量尺度的影响，可以对$x$做标准化，标准化后的协方差矩阵就相当于原本的相关系数矩阵$R$。

1. （数据变化的影响）改变$x$的单位，也即将$x$变换成$x^*=Cx$，其中$C=\operatorname{diag}(c_1,c_2,\cdots,c_p),c_i>0$，并不妨害正交因子模型的成立。也即$x^*$依然满足全部假定。（这为上面说的标准化提供了可行性。）
1. （因子旋转的影响）对$\Sigma$的分解方法并不唯一，所有可行的分解之间只相差一个正交变换。假如已经得到一组可行的$A,f$，用正交矩阵$T$做变换$A^*=AT,f^*=T'f$，那么新得到的$A^*,f^*$依然满足正交因子模型的假定，且成立$\Sigma={A^*} {A^*}'+D $。这启示我们可以通过正交变换来选择性质更好的因子。
1. （载荷的意义）由$x=\mu+Af+\varepsilon$可得$\operatorname{Cov}(x,f)=A $，也即$\operatorname{Cov}(x_i,f_j)=a_{ij} $。特别地，如果$x$经过了标准化，就是$\rho(x_i,f_j)=a_{ij} $。
1. （载荷平方和）由$x_i = \mu_i+a_{i1}f_1+a_{i2}f_2+\cdots+a_{im}f_m+\varepsilon_i$可得$V(x_i)=\sigma_{ii}=a_{i1}^2+\cdots+a_{im}^2+\sigma_i^2$，进而有$ \sum\limits_{i=1}^p V(x_i)=\sum\limits_{i=1}^p a_{i1}^2+\cdots+\sum\limits_{i=1}^p a_{im}^2+\sum\limits_{i=1}^p \sigma_{i}^2$。记$h_i^2=\sum\limits_{j=1}^m a_{ij}^2$（行载荷平方和），$g_j^2=\sum\limits_{i=1}^p a_{ij}^2$（列载荷平方和），则$h_i^2$反映了公共因子对分量$x_i$的方差贡献，$g_j^2$反映了第$j$个公共因子$f_j$对所有分量的总方差贡献。

# 算法

为了求解因子模型，需要根据样本求出载荷矩阵$A$和特殊方差矩阵$D=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2)$。假设样本是$x_1,\cdots,x_n$，可知均值$\bar x$、协方差矩阵$S=\frac{1}{n-1}\sum\limits_{i=1}^n (x_i-\bar{x})(x_i-\bar{x})'$与相关系数矩阵$\hat R$。有以下方法估计$A$与$D$，以及计算因子得分$f$：

## 估计载荷-主成分法

已知协方差矩阵可以做分解$S=\lambda_1t_1t_1'+\lambda_2t_2t_2'+\cdots+\lambda_pt_pt_p'$，其中的$\lambda_i,t_i$是$S$的特征值（可以证明非负）与相应的单位正交特征向量。那么，令$ \hat A =(\sqrt{\lambda_1}t_1,\cdots,\sqrt{\lambda_m}t_m) $，就有$\hat A\hat A'=\lambda_1t_1t_1'+\cdots+\lambda_mt_mt_m'$，再令$ \hat D$是剩余部分$\lambda_{m+1}t_{m+1}t_{m+1}'+\cdots+\lambda_pt_pt_p'$的对角元所构成的对角阵，就有$S\approx\hat A\hat A'+\hat D$，二者只相差了$\lambda_{m+1}t_{m+1}t_{m+1}'+\cdots+\lambda_pt_pt_p'$的非对角元部分。$\hat A,\hat D$就是对$A,D$的估计。

与主成分分析类似，应该选择合适的$m$，使得贡献率$\sum\limits_{i=1}^m \lambda_i / \sum\limits_{i=1}^p \lambda_i$比较大。

## 估计载荷-迭代主因子法

该方法要求对数据经过了标准化，也即满足关系$R=AA'+D$。

如果我们已经有了对$D$的估计$\hat D$，就可以先得到$R^*=\hat R- \hat D=AA'$，再做分解$R^*=\lambda^*_1{t^*_1}{t^*_1}'+\cdots+\lambda^*_p{t^*_p}{t^*_p}'$。现在令$ \hat A =(\sqrt{\lambda_1^*}t_1^*,\cdots,\sqrt{\lambda_m^*}t_m^*) $，则有$R^*\approx \hat A\hat A'$。

没有先验的估计$ \hat D$时，可以用某个初值$D_0$作为估计，先得到初步的估计$\hat A$，再更新特殊方差为$\sigma_i^2=1-h_i^2=1-\sum\limits_{i=1}^m a_{ij}^2 $，进而得到更新的$D_1=\operatorname{diag}(\sigma_1^2,\sigma_2^2,\cdots,\sigma_p^2)$来作为下一步的$ \hat D $的估计。不断迭代来得到稳定的结果。

一般可以采取的$\sigma_i^2$的初始估计值有：（1）$\hat R^{-1} $的对角元$r^{ii}$（2）取$\sigma_i^2=1-\max\limits_{j\neq i} |r_{ij}|$（3）取$0$。（4）取$1$。

## 估计载荷-极大似然法

根据对$f,\varepsilon$的假设，可以知道$ f \sim N_m(0,I), \varepsilon\sim N_p(0,D) $，而$x$是$f$与$\varepsilon$的线性函数，于是也服从多元正态分布，即有$x\sim N_p(\mu,\Sigma)$。可以写出$x$的密度函数为$f(x,\mu,\Sigma)$，用$AA'-D$替换掉$\Sigma$，再把样本代入$x$，就得到了似然函数$L(\mu,A,D)$，

使得该函数取最大值的参数就是极大似然估计$(\hat\mu,\hat A,\hat D)$。

可以证明，$\hat\mu=\bar x$，而$\hat A,\hat D$满足方程组
$$
\begin{cases}
\hat\Sigma\hat D^{-1}\hat A=\hat A(I_m+\hat A'\hat D^{-1}\hat A) \\
\hat D=\operatorname{diag}(\hat\Sigma-\hat A\hat a')
\end{cases}
$$
其中$\hat\Sigma=\displaystyle \frac1n \sum_{i=1}^n(x_i-\bar x)(x_i-\bar x)' $。为了求出唯一解，还需要补充条件“$A'D^{-1}A$是对角阵”。现在，完全转变成为解方程问题，可以用数值方法求解。

## 载荷矩阵旋转

前面说过，对公因子向量$F$（相当于前述的$f$）做正交变换得到$Z=T'F$，则$Z$也会是一个可行的公因子向量，载荷矩阵则变成$AT$。所以考虑对求出的$A$做一系列正交变换，使得$AT$有更明显的意义，比如可解释性。

对因子模型$X=AF+\varepsilon$，载荷的行平方和记为$h_i^2=\sum\limits_{j=1}^m a_{ij}^2$，它也称为变量$x_i$的共同度。为了消除$a_{ij}$的符号不同的影响，以及各变量对公共因子依赖程度不同的影响，令$d_{ij}^2=\frac{a_{ij}^2}{h_i^2}$，$d_{ij}^2$就衡量了第$i$分量对第$j$因子的依赖。考虑第$j$列$(d_{1j}^2,\cdots,d_{pj}^2)'$的方差
$$
V_j = \frac1p \sum_{i=1}^p (d_{ij}^2-\bar{d_{j}^2})^2
$$
这个值越大，说明$d_{ij}^2$越分散，也就是说，$x$的不同分量对同一个因子$F_j$（$f_j$）的依赖程度差距很大，有的很依赖，有的不依赖，这有利于解释因子含义。所以我们的目标可以定为，旋转要使得方差之和（称为载荷矩阵$A$的方差）$\sum V_j$最大。

具体算法涉及到解析几何知识，目前只需要知道，这是可以通过多次旋转做到的。

## 因子得分-加权最小二乘法

得到载荷矩阵$A$只能满足解释的需求，为了实现降维，还需要计算出因子得分也即$f_1,f_2,\cdots,f_m$的值。

加权最小二乘法，其思路是选取合适的因子得分，使得下式
$$
\sum_{i=1}^p \frac{\left[x_i-(\mu_i+a_{i1}f_1+\cdots+a_{im}f_m)\right]^2}{\sigma_i^2}
$$
达到最小。这样求得的$\hat f=(\hat f_1,\cdots,\hat f_m)'$称为巴特莱特因子得分。

用数学分析的方法可以得到，上述问题的解为
$$
\hat f=(A'D^{-1}A)^{-1}A'D^{-1}(x-\mu)
$$
在实践中，用$ \bar x,\hat A,\hat D$作为$ \mu,A,D$的估计，代入求解。可以证明，该估计是无偏的。

## 因子得分-回归法/条件期望法

根据对$f,\varepsilon$的假设，可以知道$ \begin{pmatrix} f \\ \varepsilon \end{pmatrix} $服从多元正态分布$N(\bold{0}, \begin{pmatrix} I &\bold{0} \\ \bold{0} & D \end{pmatrix} )$。而$x$是$f$与$\varepsilon$的线性函数，所以$ \begin{pmatrix} f \\ x \end{pmatrix} $也服从多元正态分布，且可以求出分布为$N(\begin{pmatrix} \bold{0} \\ \bold{\mu} \end{pmatrix}, \begin{pmatrix} I & A' \\ A & \Sigma \end{pmatrix} )$。于是我们用$f$的条件数学期望
$$
\hat f=E(f|x)=A'\Sigma^{-1}(x-\mu)=(I+A'D^{-1}A)^{-1}A'D^{-1}(x-\mu)
$$
来作为$f$的估计。该得分也称之为汤姆森因子得分。

在实践中，用$ \bar x,\hat A,\hat D,S$作为$ \mu,A,D,\Sigma$的估计，代入求解。

该估计是有偏的。不过，其有效性严格大于加权最小二乘法。而且，在偏离正态假设的情形下依然可以使用。

# 评价

评价因子模型是否可用，有以下参考标准：

1. （残差矩阵）定义残差矩阵为$S-(\hat A \hat A'+\hat D)$（如果是经过了标准化，则用原数据的相关系数矩阵$\hat R$代替$S$），残差矩阵的元素越小，说明估计值$\hat A,\hat D$越精确，模型的效果也就越好。
1. （解释总方差的累计比例）在性质的载荷平方和部分提到过，$A$的列平方和$g_j^2$是$f_j$对所有分量的总方差贡献，那么$g_j^2$的和$\sum\limits_{j=1}^m g_j^2$也即全部载荷的平方和$\sum\sum a_{ij}^2$就是采用的全部$m$个因子所有分量的总方差贡献。所以可以用因子的总方差贡献除以分量方差之和，也即$\sum\limits_{j=1}^m g_j^2 / \sum\limits_{i=1}^p V(x_i) $ ，来衡量主成分发挥的作用。让该值比较大（达到$0.8$之类的）的$m$就是合适的因子个数。在实践中，用样本协方差矩阵的迹$\tr(S)$来代替$\sum\limits_{i=1}^p V(x_i)$计算。

# 代码

## Python

官方文档：

​	https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html



样例：

```python
#FactorAnalysis(n_components=None, *, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None, svd_method='randomized', iterated_power=3, rotation=None, random_state=0)

from sklearn.decomposition import FactorAnalysis 
FA=FactorAnalysis()
FA.fit(X)
X_transformed = FA.transform(X)
```



超参数：

​	n_components：所使用的因子维度$m$。

​	noise_variance_init：迭代计算的初始$D_0$，只需给出对角元即可。也即参数内容是长度为$p$的数组，代表每个分量的特殊方差估计值。默认全部为$1$。

​	rotation：旋转的方式，接受字符串‘varimax’或者‘quartimax’，默认值None。如果给了参数，会使用相应方法进行旋转。varimax指的是最大方差旋转法；quartimax可译为四次方最大值法，内容暂不明。



属性：

​	components_：载荷矩阵，每一列是一个分量所对应的载荷，也即$A'$。其形状为(n_components, n_features)的矩阵，对接到上述的理论则是$m\times p$。

​	noise_variance_：特殊方差。也即$D$的对角元组成的向量。



方法：

​	fit(X)：根据数据训练。X是二维数组类的变量。

​	transform(X)：对二维数组X应用训练过的模型，返回降维之后得到的由向量$f_i$作为行的二维数组。

​	score(X)：返回平均对数似然值。这是一个浮点数。

## R语言

样例：

```R
library(psych)
faml=fa(r=R, nfactors=4, residuals=TRUE, rotate="none", fm="ml") #极大似然法因子分析
faml$loadings
faml$communality

faml.varimax=fa(R, nfactors=4, rotate="varimax", scores=TRUE, fm="ml") #因子旋转
faml.varimax$loadings
```



参数：

​	r：协方差矩阵或者相关系数矩阵。

​	nfactorts：采用的因子数目。

​	rotate：旋转方式。有"none", "varimax", "quartimax", "bentlerT", "equamax", "varimin", "geominT"等等。

​	fm：计算方式。有"pa"（主成分法），"ml"（极大似然估计）等方法。



属性：

​	loadings：因子载荷矩阵。

​	communality：载荷的行平方和，也即共性方差。