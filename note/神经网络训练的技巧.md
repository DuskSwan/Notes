dropout：https://zhuanlan.zhihu.com/p/38200980

梯度爆炸问题：https://blog.csdn.net/qq_40765537/article/details/106063941

warm-up

# 残差连接

### 参考

https://blog.csdn.net/u014665013/article/details/81985082

### 思路

残差连接（Residual Connections）是深度学习中一种重要的结构创新，最著名的应用是在深度残差网络（Residual  Networks，简称ResNet）中。ResNet由微软研究院的Kaiming  He等人在2015年提出，该架构通过引入残差连接解决了深度神经网络训练过程中的退化问题，即随着网络深度的增加，网络的训练误差反而上升。

其核心思想是每一层都可以表示为学习残差函数$F(x)$，而不是学习一个直接的映射函数$H(x)$。在理想情况下，如果一个恒等映射是最优的，那么让层去逼近0的残差比去逼近恒等映射要容易得多。因此，ResNet让这些层去拟合残差$F(x)=H(x)−x$，而非直接去拟合输出$H(x)$。这种结构称为“残差块”，它允许信息通过“捷径”（shortcut connection）或“直连通道”（identity connection）直接传递，避免了深度网络中梯度消失的问题。

### 实现

假设我们想要的映射为$H(x)$，我们把网络层设计成学习一个残差映射$F(x):=H(x)−x$。因此，原始的映射就被重写为$F(x)+x$。以下面的结构为例，假设网络是一个简单的全连接层：

![image-20240401210021539](D:\GithubRepos\notes_about_datascience\note\img\image-20240401210021539.png)

可以看到一个弯弯的弧线，这个就是所谓的”shortcut connection“。实际的forward过程就是
$$
F(x) = W_2\sigma(W_1x+b_1)+b_2 \\
H(x) = \sigma(F(x) + x)
$$
上述例子中的$F(x)$与$x$是向量。但ResNet作为一种思想，可以用在各种网络层上，比如在卷积神经网络中，它们是特征图，$F(x)$与$x$可能是特征图等等。

当$F(x)$与$x$的形状不一致时，直接相加会遇到问题。在ResNet中，这个问题通过引入一个调整维度的操作来解决，这个操作通常在捷径连接（shortcut connection）中实现。

这个调整维度的操作通常有两种方式：

1. 卷积调整: 使用一个卷积层来改变$x$的维度（比如通道数、高度、宽度等），使其与$F(x)$的维度匹配。这个卷积层通常有一个$1 \times 1$的卷积核，步长和$F(x)$的计算过程中使用的步长相匹配，以确保空间维度的一致性。这种方式不仅能调整通道数，还能调整特征图的尺寸。但这样会增加参数，也会增加计算量。
2. 零填充调整: 如果只是通道数不一致，而空间维度（即高度和宽度）相同，另一种较为简单的方式是通过在通道维度上添加额外的零来增加$x$的通道数，使其与$F(x)$匹配。这种方式不引入额外的参数，但仅限于通道数的调整。

这两种方式中，卷积调整更为通用，可以处理通道数和空间维度的不一致问题，而零填充调整则更简单，适用于特定情况。在实际的ResNet实现中，通常采用卷积调整方法。



# Batch Normalization（BN层）

### 参考

具体讲了过程：https://blog.csdn.net/Superstar02/article/details/101537792

提供了意义层面的解释：https://blog.csdn.net/qq_40765537/article/details/105548340

### 思想

神经网络学习过程本质是为了学习数据分布，一旦训练数据与测试数据的分布不同，则网络的泛化能力也大大降低；其次，一旦每批训练数据的分布各不相同，那么网络就要在每次迭代都去学习适应不同的分布（新的数据分布），这样将会大大降低网络的训练速度。
神经网络训练，参数就会发生变化，除了输入层，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。作者将这种数据分布的改变称之为Internal Covariate Shift（内部协变量偏移）。
同时，之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛。所以受白化启发，batch normalization被提出。

白化是在输入层对数据进行变换使其服从标准正态分布。对深度神经网络来说，其中某个隐层的输出就是下一层的输入，那么能不能对每个隐层都做白化呢？BN确实就是这么做的，并且在模型训练中取得了良好的效果。

### 算法

一个样本是对一个随机向量的采样，随机向量的方差实际上是协方差矩阵。作者为了简化计算，并不对层输入和层输出的特征进行联合白化，仅独立的对每个分量特征进行标准化，这同样加快了收敛速度。

Batch Normalization通常在网络的全连接层或卷积层之后、激活函数之前执行。其步骤可以概括为以下几点：

给定一个batch的数据$B = {x_1, x_2, ..., x_m}$，其中每个$x_i$是一个$d$维的向量（而不是标量），代表一个样本。

1. **计算小批量的均值向量** $\mu_B$： $\mu_B=\frac1m\sum_{i=1}^mx_i$，这里$\mu_B$也是一个$d$维的向量，每个元素是对应维度上所有样本特征的均值。
2. **计算小批量的方差向量** $\sigma^2_B$： $\sigma_B^2 = \frac1m \sum_{i=1}^m (x_i-\mu_B)\odot(x_i-\mu_B)$，这里$\odot$表示Hadamard积（即元素对元素的乘积），结果$\sigma^2_B$是一个$d$维的向量，每个元素表示对应维度上的方差。这里使用Hadamard积实际计算出的是每个特征分量的方差。
3. **归一化** $x_i$： $\hat x_i^{(k)} = \frac{x_i^{(k)}-\mu_B^{(k)}}{\sqrt{(\sigma_B^2)^{(k)}+\epsilon}}$，这意味着在每个维度上独立进行归一化，其中$\epsilon$是一个很小的数，用来防止分母为零。结果$\hat{x}_i=(\hat x_i^{(1)}, \cdots, \hat x_1^{(d)})$是归一化后的向量。
4. **缩放和平移**： $y_i=γ⊙\hat x_i+β$ ，其中$\gamma$和$\beta$是可学习的参数向量，$y_i$是最终的输出向量。

解释一下第四步，如果仅对层的每个输入进行标准化可能会改变层所能表示的内容。作者举了一个例子来说明：对一个sigmoid的输入进行归一化会破坏其分布，本来学习后的特征数据分布在S激活函数的两侧，归一化处理后，数据变换成分布于S函数的中间部分，把学习到的特征分布损坏了。通过引入缩放和平移操作，可以确保即便经过归一化，数据分布也能够在激活函数的有效范围内变化，从而利用非线性特性。

简单来说，即使归一化改变了数据分布，缩放和平移步骤允许模型有能力恢复到任意的均值和方差，从而保留或增强网络的表达能力。在特殊情况下，如果数据不需要归一化处理，模型可以通过调整$\gamma$和$\beta$值来接近取消Batch  Normalization的效果（例如，如果$\gamma$接近$\sqrt{\sigma^2_B +  \epsilon}$，$\beta$接近$\mu_B$，那么$y_i$可以接近原始输入$x_i$）。

$\gamma$和$\beta$值是通过网络训练过程中的反向传播和梯度下降（或其他优化算法）直接学习得到的，而不是通过训练一个额外的网络获得的。它们作为模型的参数（类似于网络中的权重和偏置），在训练过程中与其他参数一起更新。



# Layer Normalization

### 参考

https://zhuanlan.zhihu.com/p/74516930

### 思路

> Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列  的数据，即我们的batchsize =  10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。
>
> 而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。
>
> 细心的你已经看出来，layer normalization  对所有的特征进行缩放，这显得很没道理。我们算出一行这【身高、体重、年龄】三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是BN则没有这个影响，因为BN是对一列进行缩放，一列的量纲单位都是相同的。
>
> 那么我们为什么还要使用LN呢？因为NLP领域中，LN更为合适。
>
> 如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的**第一个**词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是**针对每个位置**进行缩放，这**不符合NLP的规律**。
>
> 而LN则是针对一句话进行缩放的，且L**N一般用在第三维度**，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。