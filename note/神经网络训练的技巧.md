待补充内容

梯度爆炸问题：https://blog.csdn.net/qq_40765537/article/details/106063941

权重初始化：Glorot初始化（Xavier初始化），He初始化

梯度裁剪（gradient clipping）：设置梯度范围阈值，避免梯度过大导致的梯度爆炸。

标签平滑：将标签与一定的噪声混合，提高泛化能力。

混合精度训练：维持损失不变，训练速度提升。

多loss结合的权重设置：平衡各个loss幅值以及梯度



[toc]



# 结构设置

## dropout

### 参考

https://zhuanlan.zhihu.com/p/38200980

https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9

### 思路

dropout用于解决神经网络的过拟合问题。

什么是过拟合？深度神经网络有不同的架构，有时很浅，有时很深，试图对给定的数据集进行泛化。但是，在过于努力地从数据集中学习不同特征的过程中，它们有时会学习数据集中的统计噪声。这无疑会提高模型在训练数据集上的性能，但在新数据点（测试数据集）上却大为失败。这就是过拟合问题。为了解决这个问题，我们有各种正则化技术来惩罚网络的权重，但这还不够。

减少过拟合的最佳方法，是从所有可能的参数设置中获得最好的平均预测结果。但是这在计算上变得过于昂贵，并且对于实时推理/预测来说是不可行的。另一种方案是受到集成技术（例如 AdaBoost、XGBoost 和随机森林）的启发，使用不同架构的多个神经网络共同计算。但这需要训练和存储多个模型，随着时间的推移，随着网络的加深，这将成为一个巨大的挑战。

dropout则采用这样的思想：以 p 的丢弃概率被丢弃神经网络中的节点（输入层和隐藏层），被删除节点的所有前向和后向连接都被暂时删除，然后再进行一次前向传播和反向传播，传播过程中被删除节点的值不更新。这样一来模型不会太依赖某些局部的特征，也即一些关键的节点，而是所有节点得发挥不错的作用。

![image-20240515150608041](./img/image-20240515150608041.png)

在理解上，还可以将删除了一些节点的模型看作是弱分类器，用集成学习的视角来看，多个弱分类器共同给出结果，从而提高了泛化能力。

>为什么说Dropout可以解决过拟合？
>
>（1）取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用  “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。
>
>（2）减少神经元之间复杂的共适应关系：  因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。迫使网络去学习更加鲁棒的特征，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。
>

### 实现

考虑这样一个问题，在训练时使用dropout删除一些节点会提高泛化能力，但推理时应该不能删除节点了，否则不同的删除选择会带来不同的推理结果。换言之，推理时节点会全部保留，那此时训练和推理用的模型实际上并不相同，如何保证推理用的模型能给出正确的结果呢？

假设一层神经网络中有n个神经元，其中一个神经元的输出是x，输出期望也是x。加上dropout后，有p的概率这个神经元失活，那么这个神经元的输出期望就变成 $(1-p)x+p0=(1-p)x$ ，为了保证这个神经元在训练和测试阶段的输出**期望**不变。就有两种方式来解决：

- 第一，在训练的时候，让这个神经元的输出缩放为1/(1-p)倍，那么它的输出期望又变回了x，和不dropout的输出期望一致
- 第二，在测试的时候，让神经元的输出缩放为(1-p)倍，那么它的输出期望就变成了(1-p)x，和训练时的期望一致

在pytorch的实现中，采用的似乎是第一种方案。

### 代码

以下是使用torch.nn.Dropout来实现dropout的一个例子

```python

import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.layer1 = nn.Linear(10, 50)
        self.dropout = nn.Dropout(p=0.25)  # 25% 的概率丢弃
        self.layer2 = nn.Linear(50, 2)

    def forward(self, x):
        x = self.layer1(x)
        x = nn.functional.relu(x)
        x = self.dropout(x)  # 应用 dropout
        x = self.layer2(x)
        return x
```

在这个例子中，dropout只对上一层（layer1）生效。

> 既然dropout只对layer1生效，为什么不把 x = self.dropout(x)放在x = nn.functional.relu(x)之前，而是放在之后？
>
> 1. **保留激活特征的完整性**：
>    ReLU 激活函数的作用是引入非线性并将所有负值置为零，这有助于模型学习复杂的模式。在 ReLU 之后应用 `dropout`，意味着您是在非线性激活后的结果上进行随机的神经元丢弃。这样可以保持激活后的特征表示的完整性，然后再从这些特征中随机选择子集，这可能有助于网络在训练过程中更好地泛化。
> 2. **避免激活前的值被随机抑制**：
>    
>    如果在 ReLU 之前应用 `dropout`，那么负值和正值都有相同的机会被随机丢弃。这可能导致一些本应通过 ReLU 激活为零的负值被保留，而一些有益的正值被丢弃，从而扭曲了模型在该层的学习表达。
> 3. **实验结果和经验**：
>    
>    大多数研究和实际应用中发现，在 ReLU 之后使用 `dropout` 往往能获得更好的训练效果。这是因为激活后的特征通常更有信息量，对于后续层的学习更为重要。通过在激活后应用 `dropout`，模型被迫学习使用这些非线性激活后的部分特征来进行预测，从而可能增强模型对输入数据的理解和泛化能力。



## 残差连接

### 参考

https://blog.csdn.net/u014665013/article/details/81985082

### 思路

残差连接（Residual Connections）是深度学习中一种重要的结构创新，最著名的应用是在深度残差网络（Residual  Networks，简称ResNet）中。ResNet由微软研究院的Kaiming  He等人在2015年提出，该架构通过引入残差连接解决了深度神经网络训练过程中的退化问题，即随着网络深度的增加，网络的训练误差反而上升。

其核心思想是每一层都可以表示为学习残差函数$F(x)$，而不是学习一个直接的映射函数$H(x)$。考虑这样的情况，如果一个恒等映射是最优的，那么让层去逼近0的残差比去逼近恒等映射要容易得多。因此，ResNet让这些层去拟合残差$F(x)=H(x)−x$，而非直接去拟合输出$H(x)$。这种结构称为“残差块”，它允许信息通过“捷径”（shortcut connection）或“直连通道”（identity connection）直接传递，避免了深度网络中梯度消失的问题。

### 实现

假设我们想要的映射为$H(x)$，我们把网络层设计成学习一个残差映射$F(x):=H(x)−x$。因此，原始的映射就被重写为$F(x)+x$。以下面的结构为例，假设网络是一个简单的全连接层：

![image-20240401210021539](img/image-20240401210021539.png)

可以看到一个弯弯的弧线，这个就是所谓的”shortcut connection“。实际的forward过程就是
$$
F(x) = W_2\sigma(W_1x+b_1)+b_2 \\
H(x) = \sigma(F(x) + x)
$$
上述例子中的$F(x)$与$x$是向量。但残差连接作为一种思想，可以用在各种网络层上，比如在卷积神经网络中，它们是特征图，$F(x)$与$x$可能是特征图等等。

当$F(x)$与$x$的形状不一致时，直接相加会遇到问题。在ResNet中，这个问题通过引入一个调整维度的操作来解决，这个操作通常在捷径连接（shortcut connection）中实现。

这个调整维度的操作通常有两种方式：

1. 卷积调整: 使用一个卷积层来改变$x$的维度（比如通道数、高度、宽度等），使其与$F(x)$的维度匹配。这个卷积层通常有一个$1 \times 1$的卷积核，步长和$F(x)$的计算过程中使用的步长相匹配，以确保空间维度的一致性。这种方式不仅能调整通道数，还能调整特征图的尺寸。但这样会增加参数，也会增加计算量。
2. 零填充调整: 如果只是通道数不一致，而空间维度（即高度和宽度）相同，另一种较为简单的方式是通过在通道维度上添加额外的零来增加$x$的通道数，使其与$F(x)$匹配。这种方式不引入额外的参数，但仅限于通道数的调整。

这两种方式中，卷积调整更为通用，可以处理通道数和空间维度的不一致问题，而零填充调整则更简单，适用于特定情况。在实际的ResNet实现中，通常采用卷积调整方法。



## Batch Normalization（BN层）

### 参考

具体讲了过程：https://blog.csdn.net/Superstar02/article/details/101537792

提供了意义层面的解释：https://blog.csdn.net/qq_40765537/article/details/105548340

### 思想

神经网络学习过程本质是为了学习数据分布，一旦训练数据与测试数据的分布不同，则网络的泛化能力也大大降低；其次，一旦每批训练数据的分布各不相同，那么网络就要在每次迭代都去学习适应不同的分布（新的数据分布），这样将会大大降低网络的训练速度。
神经网络训练，参数就会发生变化，除了输入层，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。作者将这种数据分布的改变称之为Internal Covariate Shift（内部协变量偏移）。
同时，之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛。所以受白化启发，batch normalization被提出。

白化是在输入层对数据进行变换使其服从标准正态分布。对深度神经网络来说，其中某个隐层的输出就是下一层的输入，那么能不能对每个隐层都做白化呢？BN确实就是这么做的，并且在模型训练中取得了良好的效果。

### 算法

一个样本是对一个随机向量的采样，随机向量的方差实际上是协方差矩阵。作者为了简化计算，并不对层输入和层输出的特征进行联合白化，仅独立的对每个分量特征进行标准化，这同样加快了收敛速度。

Batch Normalization通常在网络的全连接层或卷积层之后、激活函数之前执行。其步骤可以概括为以下几点：

给定一个batch的数据$B = {x_1, x_2, ..., x_m}$，其中每个$x_i$是一个$d$维的向量（而不是标量），代表一个样本。

1. **计算小批量的均值向量** $\mu_B$： $\mu_B=\frac1m\sum_{i=1}^mx_i$，这里$\mu_B$也是一个$d$维的向量，每个元素是对应维度上所有样本特征的均值。
2. **计算小批量的方差向量** $\sigma^2_B$： $\sigma_B^2 = \frac1m \sum_{i=1}^m (x_i-\mu_B)\odot(x_i-\mu_B)$，这里$\odot$表示Hadamard积（即元素对元素的乘积），结果$\sigma^2_B$是一个$d$维的向量，每个元素表示对应维度上的方差。这里使用Hadamard积实际计算出的是每个特征分量的方差。
3. **归一化** $x_i$： $\hat x_i^{(k)} = \frac{x_i^{(k)}-\mu_B^{(k)}}{\sqrt{(\sigma_B^2)^{(k)}+\epsilon}}$，这意味着在每个维度上独立进行归一化，其中$\epsilon$是一个很小的数，用来防止分母为零。结果$\hat{x}_i=(\hat x_i^{(1)}, \cdots, \hat x_1^{(d)})$是归一化后的向量。
4. **缩放和平移**： $y_i=γ⊙\hat x_i+β$ ，其中$\gamma$和$\beta$是可学习的参数向量，$y_i$是最终的输出向量。

解释一下第四步，如果仅对层的每个输入进行标准化可能会改变层所能表示的内容。作者举了一个例子来说明：对一个sigmoid的输入进行归一化会破坏其分布，本来学习后的特征数据分布在S激活函数的两侧，归一化处理后，数据变换成分布于S函数的中间部分，把学习到的特征分布损坏了。通过引入缩放和平移操作，可以确保即便经过归一化，数据分布也能够在激活函数的有效范围内变化，从而利用非线性特性。

简单来说，即使归一化改变了数据分布，缩放和平移步骤允许模型有能力恢复到任意的均值和方差，从而保留或增强网络的表达能力。在特殊情况下，如果数据不需要归一化处理，模型可以通过调整$\gamma$和$\beta$值来接近取消Batch  Normalization的效果（例如，如果$\gamma$接近$\sqrt{\sigma^2_B +  \epsilon}$，$\beta$接近$\mu_B$，那么$y_i$可以接近原始输入$x_i$）。

$\gamma$和$\beta$值是通过网络训练过程中的反向传播和梯度下降（或其他优化算法）直接学习得到的，而不是通过训练一个额外的网络获得的。它们作为模型的参数（类似于网络中的权重和偏置），在训练过程中与其他参数一起更新。



## Layer Normalization

### 参考

https://zhuanlan.zhihu.com/p/74516930

### 思路

> Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列  的数据，即我们的batchsize =  10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。
>
> 而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。
>
> 细心的你已经看出来，layer normalization  对所有的特征进行缩放，这显得很没道理。我们算出一行这【身高、体重、年龄】三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是BN则没有这个影响，因为BN是对一列进行缩放，一列的量纲单位都是相同的。
>
> 那么我们为什么还要使用LN呢？因为NLP领域中，LN更为合适。
>
> 如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的**第一个**词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是**针对每个位置**进行缩放，这**不符合NLP的规律**。
>
> 而LN则是针对一句话进行缩放的，且**LN一般用在第三维度**，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。



# 训练技巧

## 正交初始化（Orthogonal Initialization）

网络参数的正交初始化（Orthogonal  Initialization）是一种在初始化神经网络权重时使用的技术，其核心思想是通过正交矩阵来初始化权重矩阵，从而帮助模型在训练初期有更好的收敛性。这种方法尤其适用于深层网络，可以有效减少训练过程中的梯度消失或爆炸问题。

实现正交初始化通常可以通过以下步骤进行：

1. **生成随机矩阵**：首先生成一个随机的权重矩阵 W。
2. **矩阵正交化**：使用数学方法（如Gram-Schmidt正交化过程）对矩阵 W 进行正交化处理，确保矩阵的行或列向量互相正交。
3. **归一化处理**：对正交化后的矩阵进行归一化处理，确保每个向量的范数为1。

在PyTorch等现代深度学习框架中，通常已经内置了正交初始化的函数（如`torch.nn.init.orthogonal_`），可以直接使用这些函数来初始化网络的权重。

正交初始化主要影响模型训练的初期阶段，而且权重矩阵在经过训练更新后，很可能不再保持正交性。然而，正交初始化还是有其重要价值和作用的。虽然权重会在训练过程中逐渐失去正交性，但初始的正交性对网络的早期行为影响巨大。良好的初始状态可以使网络更快地进入一个“好”的收敛区域，即使后期权重不再正交，早期的优势依然可以让网络比较稳定地继续训练。

那么最初的时候正交矩阵又为何有利于保证梯度稳定呢？从梯度传播的角度看，在神经网络尤其是深层网络和循环神经网络中，初始的权重如果能保持较好的独立性和均匀性，可以显著减少梯度消失或爆炸的风险，而正交矩阵的条件数（最大奇异值与最小奇异值之比）为1，这有助于梯度流通过网络时保持其规模，从而保持有效的学习速率。从信息熵的角度看，正交初始化帮助在参数空间中均匀地分布概率质量，这使得每个方向的学习初期都是相互独立的。这种均匀分布有助于搜索过程中避免某些方向上的过早饱和，提高了训练过程中的探索效率。

## 学习率策略与warm-up

### 参考

https://zhuanlan.zhihu.com/p/567977738

### 可变学习率

最初也是最普通的策略是使用一个固定的学习率，但不论出于理论还是出于经验，我们很容易发现，不同情况下应该使用不同的学习率。当loss变化比较陡峭、梯度值较大时，需要较小的学习率来避免参数反复横跳；而当loss变化比较平稳、梯度值较小时，需要较大的学习率来加速收敛。自然地，我们希望学习率可以在训练过程中发生变化。

常用的策略包括：

- **阶梯衰减 (Step Decay)**：在训练过程中，每隔一定的 epoch 数就将学习率按比例减少。例如，每 10 个 epoch 将学习率减半。

- **指数衰减 (Exponential Decay)**：使学习率按一个指数函数逐渐减小。公式为：`lr = lr_0 * gamma^epoch`，其中 `gamma` 是一个小于 1 的常数。

- **余弦退火 (Cosine Annealing)**：使用余弦函数在指定范围内调整学习率，使学习率在训练开始和结束时较小，中间时段较大。通常可以在训练末尾让学习率逐渐趋向于零。

- **周期性学习率 (Cyclic Learning Rate)**：让学习率在一定范围内周期性波动，帮助模型逃离局部极小值，常用于带有噪声的数据或复杂的训练任务中。

- **自适应学习率 (Adaptive Learning Rate)**：使用自适应优化器（如 Adam、RMSprop）自动调整每个参数的学习率。

### Warmup

Warmup是一种用于在训练开始阶段逐步增加学习率的策略，初期的学习率设置较低可以避免初始参数因为更新过大导致的不稳定。因此被翻译成模型的”预热“。常见的 warmup 方法有：

- **线性 Warmup**：在最初的几个 epoch 中，学习率从非常小的值线性增长到目标学习率。通常的 warmup 过程是 5-10 个 epoch。
- **指数 Warmup**：学习率从一个小值以指数方式逐步增加到目标学习率。

Warmup 常和其他学习率调整策略结合使用。例如，首先在训练的前几个 epoch 使用 warmup，然后切换到余弦退火或者阶梯衰减等策略。这种组合可以在训练初期稳定模型参数，随后更好地加速收敛。

### 代码

以学习率线性 warmup，然后在 warmup 结束后使用学习率指数衰减的方法为例。以下是代码示例：

```python
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR

# 示例模型
model = torch.nn.Linear(10, 1)

# 优化器
optimizer = optim.Adam(model.parameters(), lr=0.001)  # 初始学习率设为很低

# 线性 Warmup + 指数衰减函数
def lr_lambda(epoch):
    # warmup_epochs 为 warmup 的 epoch 数目
    warmup_epochs = 5
    decay_rate = 0.9

    if epoch < warmup_epochs:
        # 在 warmup 阶段，学习率线性增长
        return float(epoch) / float(max(1, warmup_epochs))
    else:
        # 在 warmup 结束后，学习率进行指数衰减
        return decay_rate ** (epoch - warmup_epochs)

# 创建学习率调度器
scheduler = LambdaLR(optimizer, lr_lambda)

# 模拟训练过程
for epoch in range(20):
    # 假设训练代码在这里，比如前向传播，损失计算，反向传播，优化器更新等
    # optimizer.zero_grad()
    # loss.backward()
    # optimizer.step()

    # 每个 epoch 结束后更新学习率
    scheduler.step()

    # 打印当前学习率
    current_lr = scheduler.get_last_lr()[0]
    print(f"Epoch {epoch+1}, Learning Rate: {current_lr}")

```



# Loss工程

## L范数正则化

### 概念

首先明确一下”正则化“究竟指什么：在我的理解中，”正则化“这一操作最初指的是类似LASSO方法的操作，为惩罚函数添加一项与模型复杂度有关的项，以限制模型不能太复杂。”在损失函数中添加惩罚项以限制模型复杂度“可看作是正则化方法的狭义定义。随着深度学习的发展，学术界开始以更广泛的视角来看待正则化，将任何能够提高模型泛化能力的方法都纳入正则化的范畴。这包括但不限于dropout、数据增强、早停（Early Stopping）、批归一化（Batch Normalization）。因此广义的正则化方法是任何能够防止过拟合、提高模型泛化能力的方法。

我们这里说的正则化，指的就是最初的概念，也即在损失函数中添加惩罚项以限制模型复杂度。这个惩罚项通常是模型所有权重绝对值（L1范数）之和或者平方（L2范数）之和，形如
$$
\text{Loss}_{\text{total}} = \text{Loss}_{\text{original}} + \lambda \sum_{i} |w_i| \\
\text{Loss}_{\text{total}} = \text{Loss}_{\text{original}} + \lambda \sum_{i} w_i^2
$$
其中，$\lambda$ 是正则化系数，控制正则化的强度，$w_i$ 是模型的权重参数。

在实际应用中，可能并非所有层都需要添加正则化，可针对容易过拟合的层（如全连接层）添加。
