（本文内容在AI帮助下完成）

参考：

提供图片：https://zhuanlan.zhihu.com/p/133207206

提供了比较清晰的思路描述：https://keras-cn.readthedocs.io/en/latest/legacy/blog/autoencoder/

提供多种多样的AE，同时VAE的结构图不错：https://github.com/zangzelin/Auto-encoder-AE-SAE-DAE-CAE-DAE-with-keras-in-Mnist-and-report/blob/master/Report.md#25-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8variational-auto-encoders-vaekingma-2014

# 自编码器AE（Auto Encoder）

自编码器是一种通过无监督学习来寻找数据压缩表示的神经网络。其目标是学习一种能够表示输入数据的高效编码，同时这种编码能够被用来重构回与原始数据相似的数据。

自编码器的核心目标是**数据表示的学习**。这意味着网络试图找到一种方法，以较小的数据形式（编码）准确地表示输入数据。这个过程中，自编码器学习到的编码通常比输入数据的维度要小，这有助于提取数据的内在特征，并且有助于数据的压缩和去噪。配合适当的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。通过这种方式，自编码器可以被用于：

- **数据降维**：转换高维数据到低维空间，方便数据可视化和处理。
- **特征提取**：学习数据的有用特征表示，为其他机器学习任务服务。
- **去噪**：学习从损坏的输入数据中重构原始数据。
- **数据生成**：在某些情况下，自编码器可以生成新的数据实例，特别是变分自编码器（VAE）。

自编码器由两部分组成：**编码器**和**解码器**。

- **编码器**：这部分的任务是将输入数据转换成一个压缩的内部表示，即编码。编码器将高维的输入数据映射到一个低维的隐藏层。
- **解码器**：解码器的任务是将编码重新转换（或重构）成原始输入数据。理想情况下，解码后的输出与原始输入非常相似。

以一个最简单的自编码器为例，其编码和解码器都是线性层。实际中的编码/解码器结构可能复杂得多。

![image-20240330155542319](D:\GithubRepos\notes_about_datascience\note\img\image-20240330155542319.png)

将一个样本的输入记为$x$，隐藏层/编码记为$h$（维数通常小于输入），输出记为$y$。使用输出$y$（即重构的输入）与原始输入$x$来计算损失（loss），但我们真正感兴趣的是隐藏层（编码层）$h$中的表示，它提供了输入数据的一个压缩表示。

自编码器的训练过程目的是使输出$y$尽可能接近输入$x$，从而保证隐藏层$h$中的压缩表示保留了输入数据的关键信息。换句话说，我们通过最小化$x$和$y$之间的差异来间接地优化$h$，使得$h$成为一个有效的数据表示。

常用的损失函数包括均方误差MSE和二元交叉熵BCE。

均方误差是衡量重构值与原始值之间差异的常见方法，特别适合实值数据。其公式表示为：

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2
$$

其中，$x_i$ 是原始数据点，$\hat{x}_i$ 是对应的重构数据点，而 $n$ 是数据点的总数。

特别地，当输入数据被归一化到 $[0, 1]$ 区间内时，二元交叉熵成为一个适合的选择。它是这样计算的：

$$
BCE = -\frac{1}{n} \sum_{i=1}^{n} [x_i \log(\hat{x}_i) + (1 - x_i) \log(1 - \hat{x}_i)]
$$

在这里，$x_i$ 同样代表原始数据点，$\hat{x}_i$ 是重构后的数据点，$n$ 是数据点的总数。

# 变分自编码器VAE（Variational autoencoder）

变分自编码器（VAE）是一种生成模型，生成模型的目标就是学习得到一个分布P(X)，使得该分布和数据的真是分布P(X)很接近，这样一来，我们就可以根据得到的P(X)来生成该数据集中到数据，也就是达到了生成数据的目的，这就是生成模型的最终目标。

VAE通过自编码器的框架实现，但在标准自编码器（AE）的基础上引入了概率分布的概念。VAE的目的不仅仅是学习输入数据的有效表示，还在于能够生成新的数据点，这些数据点与训练数据具有相同的分布。

### 思路

变分自编码器（VAE）是一种基于概率生成模型的深度学习技术，其核心思想是通过学习输入数据的潜在（隐）变量的分布来生成新的数据点。VAE的实现依赖于隐变量模型和变分推断的框架。

VAE假设存在一个隐变量空间（latent space），其中的隐变量$Z$能够生成观察到的数据点，并且$Z$可以从某个简单的分布（例如高斯分布）中采样得到。模型的目标是学习这个潜在空间的结构，以便我们可以从中采样并通过解码器生成新的数据点。

VAE的网络结构如下，我们考虑最简单的情况，$Z$服从的是正态分布。

![image-20240330162223594](D:\GithubRepos\notes_about_datascience\note\img\image-20240330162223594.png)

过程是：

1. 编码：首先将输入数据 $x$ 映射到潜在变量 $z$ 的分布上，通常是高斯分布的参数（均值$\mu$和方差$\sigma^2$）。
2. 采样：从这个分布中采样潜在变量 $z$。由于直接从分布中采样的操作无法进行梯度传递，VAE采用了“重参数化技巧”来解决这个问题。
3. 解码：然后将潜在变量 $z$ 映射回数据空间，尝试重构输入数据 $x$。


### 模型搭建与训练

第一部分是编码器网络。输入是数据点 $x$，输出是描述潜在变量 $z$ 分布的参数。对于高斯分布，输出是均值 $\mu$ 和对数方差 $\log(\sigma^2)$）。
$$
q_\phi(z|x) = \mathcal{N}(z; \mu(x), \sigma^2(x))
$$

其中，$\phi$ 是编码器网络的参数。上式想要表达编码器相当于一个后验的$z$的分布，但从数学上来看，它极为不严谨，等号左边是一个函数或者说概率，右边却是一个分布，这实在是难以理解。我认为$q_\phi(x) = (\mu(x), \sigma^2(x))$是更好的表达。编码器网络并不需要很复杂，在原论文中，由多个全连接层构成。

接下来要用到重参数化技巧。为了能够通过反向传播训练网络，VAE使用了重参数化技巧。具体来说，我们不直接从$\mathcal{N}(\mu(x), \sigma^2(x))$中采样出$z$，而是考虑从从标准正态分布 $\mathcal{N}(0,1)$ 中采样出$\epsilon$，然后计算

$$
z = \mu + \sigma \odot \epsilon
$$

这样得到的$z$就服从$\mathcal{N}(\mu, \sigma^2)$。而且这样的$z$关于编码器网络的结果$\mu$与$\sigma^2$可导。

得到潜变量$z$后，输入解码器网络。输入是潜在变量 $z$，输出是重构的数据 $\hat{x}$。目标是最小化重构误差，即输入 $x$ 和重构 $\hat{x}$ 之间的差异。原论文中解码器同样采用了全连接层的结构。

接下来计算损失，损失包括两个部分，一个是重构损失函数，该函数要求解码出来的样本与输入的样本相似（与之前的自编码器相同），第二项损失函数是学习到的隐分布与先验分布的KL距离，作为一个正则项，它对学习符合要求的隐空间和防止过拟合有帮助。

重构损失（Reconstruction Loss）衡量的是重构数据与原始数据之间的相似度。它可以通过不同的方式计算，常见的有均方误差（MSE）和二元交叉熵（Binary Cross-Entropy，BCE）。和AE是一样的，不再赘述。

KL散度（KL Divergence）则衡量编码的潜在变量分布$q_\phi(z|x)$与先验分布$p(z)$之间的差异。在VAE中，先验分布通常假设为标准正态分布$\mathcal{N}(0, I)$。其计算公式为

$$
L_{\text{KL}} = KL[q_\phi(z|x) \,||\, p(z)] = \frac{1}{2} \sum_{j=1}^{J} (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)
$$
其中，$\mu_j$ 和 $\sigma_j^2$ 分别是编码得到的潜在变量 $z$ 的分布参数，$J$ 是潜在空间的维度。

VAE的总损失是重构损失和KL散度的组合，$L_{\text{total}} = L_{\text{recon}} + \beta L_{\text{KL}}$。这里的 $\beta$ 是一个超参数，用于平衡重构损失和KL散度。在一些变种中，$\beta$ 可以用来控制潜在空间的紧凑性和重构质量之间的权衡。

通过最小化总损失，VAE能够学习到能够有效重构原始数据同时保证潜在空间分布接近先验分布的参数。
