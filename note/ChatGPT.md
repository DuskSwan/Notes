ChatGPT是使用了GPT（Generative Pre-trained Transformer，生成式预训练Transformer）的Chat模型。GPT是一种基于Transformer的自回归语言模型，即模型能够迭代地根据已经生成的单词来逐个预测后面的单词。事实上GPT和BERT一样，可以通过接不同的下游功能头来完成不同的任务。

对于对话任务，自回归语言模型是这样的：我们希望给该模型输入一个句子（作为一个矩阵），然后它会根据输入预测一个单词A，再根据输入和单词A预测单词B，再根据输入和单词A,B预测单词C……知道预测出终止符为止。结果序列A,B,C……可以看作是对输入的回应，这就完成了对话。



# GPT1

### 结构

GPT的基础是Transformer的解码器，它直接接受输入然后进行预测。其结构如下：

![image-20240415202052762](D:\GithubRepos\notes_about_datascience\note\img\image-20240415202052762.png)

它堆叠了12个Transformer解码器来提升性能，每一个单独部分的实现参加Transformer部分的笔记。解码器的最终输出同样可以接不同的任务头来完成不同的任务，在对话任务中，使用的是一个由层归一化、线性层、softmax函数组成的预测器（回归器）。

## 训练

GPT的训练方式被称为生成式预训练，目标是根据上文$C=\{x_1,...,x_{t-1}\}$，预测出下一个单词$x_t$。要实现这一步，本质上是需要学习出分布$P(x_t|C)$。假如设定$x_0=$`<sos>`为起始符，那么一个句子$x=\{x_1,...,x_n,...\}$的分布$P(x)=\prod P(x_t|C)$都可以求出。

可以看出，GPT的训练样本应该是词序列。在GPT的预训练过程中，首先从一个广泛的无标注文本数据库中抽取大量的句子。这些句子被转换成词序列，通常，这一步涉及文本清理（如去除特殊字符、统一格式等）和分词（tokenization），可能使用子词单元（subword units）来减少词汇表的大小和处理未知词。

假如一个词序列的长度设为L，模型会逐步使用第1个词到第K−1个词依次作为输入，对应地预测第2个词到第K个词作为输出。通过这种方式，模型学习如何根据前面的词来预测下一个词。这个训练使用交叉熵损失来优化模型参数，使其在实际应用中能够更好地进行文本生成或完成其他语言处理任务。

> 关于训练时的数据组织，事实上，虽然GPT和类似模型可以可以接受不同长度的输入，但也可以在一次训练中处理多条数据，即一次处理一个batch的数据。这通常通过填充和截断技术以及现代深度学习框架（如TensorFlow和PyTorch）支持的动态计算图实现的。每个batch中可以包含多个不同长度的句子，通过填充使它们长度一致。

当然，GPT也可以在不同任务上进行微调。

### 与BERT比较

GPT（如OpenAI的GPT系列）和BERT（由Google开发）都是现代自然语言处理领域的重要模型，它们都使用了深度学习和注意力机制来处理文本数据。尽管它们在表面上看似类似——都可以通过训练来生成针对输入文本的表示向量，然后使用这些向量完成各种下游任务（如对话生成、文本分类、相似度计算等），但是它们在设计哲学、预训练任务、适用场景等方面存在显著差异。

BERT的核心特点是其双向性，即模型在处理每个输入词时会考虑到整个句子的上下文（both left and right context）。这是通过预训练任务"Masked Language Model"（MLM）实现的，即在训练过程中随机遮挡输入句子中的某些词，然后让模型预测这些被遮挡的词。因为BERT能够捕捉到词在整个句子中的双向关系，它在需要深层次语义理解的任务上表现较好，如问答系统、文本分类、命名实体识别等。

与BERT不同，GPT是单向的，即它只考虑左侧的上下文（或右侧，取决于具体实现）。这意味着在生成某个词时，它只能看到之前的词。GPT通过一个不同的预训练任务"Autoregressive Language Model"（ALM）进行训练，模型需要预测给定文本之后的下一个词。GPT在文本生成任务上表现出色，如聊天机器人、内容生成等，因为它的结构天然适合于生成流畅的、连贯的文本。