[toc]



（本文内容在ChatGPT的帮助下完成）

ChatGPT是由OpenAI开发的基于GPT（Generative Pre-trained  Transformer）架构的语言处理模型的一个特定实现，它专门被优化和调整用于生成连贯和有意义的对话文本。它利用了GPT这样的模型作为基础，通过在大量对话数据上进行细致的微调，使其更适合在聊天机器人、对话系统等应用中使用。

GPT是一种基于Transformer的自回归语言模型，即模型能够迭代地根据已经生成的单词来逐个预测后面的单词。

对于对话任务，自回归语言模型是这样的：我们希望给该模型输入一个句子（作为一个矩阵），然后它会根据输入预测一个单词A，再根据输入和单词A预测单词B，再根据输入和单词A,B预测单词C……知道预测出终止符为止。结果序列A,B,C……可以看作是对输入的回应，这就完成了对话。



# GPT1

## 结构

GPT的基础是Transformer的解码器，它直接接受输入然后进行预测。其结构如下：

![image-20240415202052762](D:\GithubRepos\notes_about_datascience\note\img\image-20240415202052762.png)

它堆叠了12个Transformer解码器来提升性能，每一个单独部分的实现参加Transformer部分的笔记。解码器的最终输出同样可以接不同的任务头来完成不同的任务，在对话任务中，使用的是一个由层归一化、线性层、softmax函数组成的预测器（回归器）。

## 训练

### 可逆分词方法

在GPT系列模型的训练中，一般使用的是一种称为Byte Pair Encoding (BPE) 的算法进行词汇的分割处理。这种算法在GPT的前身Transformer模型中就开始使用，并在后续的多个版本中继续使用。其核心思路是将常见的、连续出现的字符序列合并为一个单元，这样做可以有效减少整个文本中的字符总数，同时也可以保持文本的易处理性和可读性。

在解码时，BPE分词后的文本可以完全还原，因此被称为可逆分词方法。

以下是BPE算法的详细过程：

1. 准备阶段

- 收集文本数据：首先需要有一个大型的文本数据集，这将作为词汇分割的基础。
- 确定词汇表大小：设定一个目标词汇表的大小，这个大小直接影响到算法的细致程度和最终模型的参数数量。

2. 初始化词汇表

- 分割文本：将所有文本分割成字符级别，包括空格和特殊符号，这些单独的字符将作为初始的词汇表。
- 统计频率：计算每个字符在整个数据集中出现的频率。

3. BPE算法主体

- 合并循环：算法会进行多轮合并操作，每一轮选择出现次数最多的相邻字符对进行合并。
  - 选择最频繁的相邻字符对，例如`('e', 's')`。
  - 将选中的字符对合并为一个新的单元，如`'es'`，并更新文本数据中的相应部分。
  - 更新词汇表和频率统计，将新的单元加入词汇表。（原本的也不删除。）
- 迭代过程：这一过程迭代进行，每次迭代词汇表增加新的合并单元，直到词汇表达到预定的大小。

4. 使用词汇表

- 编码新文本：使用最终的词汇表来将任何文本编码为一系列词汇单元。文本中的词汇通过查找最长的可能单元在词汇表中进行匹配，以此方式编码整个文本。

### 预训练

GPT的训练方式被称为生成式预训练，目标是根据上文$C=\{x_1,...,x_{t-1}\}$，预测出下一个单词$x_t$。要实现这一步，本质上是需要学习出分布$P(x_t|C)$。假如设定$x_0=$`<sos>`为起始符，那么一个句子$x=\{x_1,...,x_n,...\}$的分布$P(x)=\prod P(x_t|C)$都可以求出。

可以看出，GPT的训练样本应该是词序列。在GPT的预训练过程中，首先从一个广泛的无标注文本数据库中抽取大量的句子。这些句子被转换成词序列，通常，这一步涉及文本清理（如去除特殊字符、统一格式等）和分词（tokenization），可能使用子词单元（subword units）来减少词汇表的大小和处理未知词。

假如一个词序列的长度设为L，模型会逐步使用第1个词到第K−1个词依次作为输入，对应地预测第2个词到第K个词作为输出。通过这种方式，模型学习如何根据前面的词来预测下一个词。这个训练使用交叉熵损失来优化模型参数，使其在实际应用中能够更好地进行文本生成或完成其他语言处理任务。

> 关于训练时的数据组织，事实上，虽然GPT和类似模型可以可以接受不同长度的输入，但也可以在一次训练中处理多条数据，即一次处理一个batch的数据。这通常通过填充和截断技术以及现代深度学习框架（如TensorFlow和PyTorch）支持的动态计算图实现的。每个batch中可以包含多个不同长度的句子，通过填充使它们长度一致。

当然，GPT也可以在不同任务上进行微调。

## 与BERT比较

GPT（如OpenAI的GPT系列）和BERT（由Google开发）都是现代自然语言处理领域的重要模型，它们都使用了深度学习和注意力机制来处理文本数据。尽管它们在表面上看似类似——都可以通过训练来生成针对输入文本的表示向量，然后使用这些向量完成各种下游任务（如对话生成、文本分类、相似度计算等），但是它们在设计哲学、预训练任务、适用场景等方面存在显著差异。

BERT的核心特点是其双向性，即模型在处理每个输入词时会考虑到整个句子的上下文（both left and right context）。这是通过预训练任务"Masked Language Model"（MLM）实现的，即在训练过程中随机遮挡输入句子中的某些词，然后让模型预测这些被遮挡的词。因为BERT能够捕捉到词在整个句子中的双向关系，它在需要深层次语义理解的任务上表现较好，如问答系统、文本分类、命名实体识别等。

与BERT不同，GPT是单向的，即它只考虑左侧的上下文（或右侧，取决于具体实现）。这意味着在生成某个词时，它只能看到之前的词。GPT通过一个不同的预训练任务"Autoregressive Language Model"（ALM）进行训练，模型需要预测给定文本之后的下一个词。GPT在文本生成任务上表现出色，如聊天机器人、内容生成等，因为它的结构天然适合于生成流畅的、连贯的文本。

# GPT2

## 结构

GPT2在结构上并没有很大的改进，相比GPT1的主要变化是在解码器中将Layer Normalization移到了注意力和前馈神经网络之前，在最终的自注意力块之后又增加了额外的层归一化。（没有找到合适的图( ╯□╰ )）

同时，OpenAI尝试了将编码器堆叠不同次数，给出了不同规模的GPT2。

好吧，结构上的改进实在乏善可陈，GPT2最显著的不同在于解决问题的思路改变，以及由此带来的训练方式改变。GPT2做到了在零样本（Zero-shot）情况下执行下游任务，而无需任何参数或者架构的修改。

为了支持多个零样本任务，GPT-2需要在预训练阶段学习尽可能丰富的数据，因此OpenAI自建了高质量的WebText数据集，只保留了人工过滤过的网页，最终包含4500万个链接。

## 训练技巧

### 正交初始化

网络参数的正交初始化（Orthogonal  Initialization）是一种在初始化神经网络权重时使用的技术，其核心思想是通过正交矩阵来初始化权重矩阵，从而帮助模型在训练初期有更好的收敛性。这种方法尤其适用于深层网络，可以有效减少训练过程中的梯度消失或爆炸问题。

### BBPE

在GPT-2模型中，采用的是一种改进版本的Byte Pair Encoding（BPE）算法，通常称之为Byte-Level BPE（BBPE）。这种方法是对原始BPE的一种优化，使得它更适合处理多种语言，特别是那些使用非拉丁字母脚本的语言。

传统的BPE算法通常在字符级别进行操作，这意味着它会将文本分割成字符（如拉丁字母、汉字等）进行分析和合并。而BBPE则在字节级别上进行操作，处理的基本单元是字节（Byte）。这样做有以下几个优势：

- 语言无关性：字节是所有文本文件存储和传输的基本单位，不依赖于任何特定的语言或字符集。因此，BBPE算法具有很强的语言通用性，可以轻松处理包括中文、阿拉伯文、希伯来文等各种使用非拉丁字母的语言。
- 一致的处理方式：由于所有的文本都被视为字节序列，这消除了处理不同字符系统时的不一致性，简化了算法的实现。

在流程上，BBPE与BPE基本相同，只是在分割文本时将所有文本内容转换成字节序列。在这一步，文本中的每个字符（无论它是拉丁字母、汉字还是其他）都被转换成对应的字节表示（通常是UTF-8编码）。后续以字节为最小单位来进行合并。

## 无监督多任务学习

GPT1的应用过程可以描述为，针对不同任务拼接不同的下游功能头，微调训练功能头后靠功能头完成特定的任务。而GPT2希望取消微调，直接用预训练好的模型完成多种任务。它的输入是一个词序列，输出也是一个词序列。

要实现这一希望，需要将任务（问题）和文本一起输入给模型，基本的想法是通过特定的输入格式来“引导”模型理解并执行给定的任务。对于每个任务，定义一个或多个自然语言的提示，这些提示明确指示模型需要执行的任务。输入序列由提示、分隔符和目标文本组成。常用的分隔符包括特殊符号如“`[SEP]`”或简单的符号如冒号`:`。

例如可以输入`情感分析: 这是一个极好的电影。 <回答> 正面`

在这个例子中，`<回答>`是一个特殊的标记，用于指示模型此处应该输出其对前面文本的情感分析结果。这种格式帮助模型学习到在接到“情感分析:”的命令后，应该怎样处理随后的文本，并在看到`<回答>`标记后输出一个情感类别。

GPT2将自回归式地对每一个样本（句子）依次预测其每个词。最开始，输入到模型的是一个起始标记（在实际操作中，通常会预先定义一个或多个固定的任务描述作为训练样本的一部分），在这里是“情感分析:”；接下来，基于已知的“情感分析:”，模型将预测序列中的下一个词，即“这”；这个过程持续进行，每次生成一个新词，直到整个输入序列预测完成。

> 在自回归预测过程中，会把问题本身也预测出来。预测问题（或任务提示）本身在一些情况下看似没有直接的实用价值，因为这部分文本通常是已知的。然而，这样做有几个潜在的好处和原因：
>
> 1. **学习任务上下文**：
>    - 通过预测整个输入序列（包括任务提示），模型不仅学习如何生成文本，还学习了文本与特定任务之间的关联。这有助于模型在处理真实任务时，更好地理解任务需求和上下文。
> 2. **增强模型的泛化能力**：
>    - 在训练过程中，如果模型能够自行生成任务描述，这表明它已经学会了任务的内部逻辑和相关语言模式，这可以增强其泛化能力，使其在面对未见过的类似任务时表现更好。
> 3. **统一的训练过程**：
>    - 让模型预测整个序列，包括任务提示和答案，可以简化训练流程。这样的统一性使得训练过程更加标准化，便于管理和优化。
>
> 综上所述，虽然预测任务提示本身在某些情况下可能看起来不是直接必需的，但它对于模型的学习过程和最终的功能实现都是有益的，特别是在提升模型对任务的理解和执行能力方面。

# GPT3

## 结构

GPT3的结构与GPT2相似，不同之处在于GPT3堆叠的编码器不相同了，而是一种名为稀疏注意力（Sparse Attention）的新技术。

（网络结构图还是没找到）

GPT-3提供了多个版本，参数从2.5亿到1750亿不等。在最大的模型（如1750亿参数模型）中，OpenAI实验了多种类型的稀疏自注意力机制，这些机制在不同层中有不同的应用：

1. **局部窗口稀疏注意力** - 这种类型的注意力可能被用在模型的初级层（接近输入的层），使模型可以聚焦于输入中的局部特征，类似于我们在图像处理中看到的卷积操作。
2. **全局稀疏注意力** - 在更高层次上，模型可能使用全局稀疏注意力来捕获文本中的长距离依赖。这种注意力模式允许模型在整个文本输入中选择性地关注信息，这对于理解复杂的语言结构和含义至关重要。

在设计大规模的Transformer模型时，通常会使用一个混合策略，结合局部和全局的稀疏模式，以及可能的全注意力层。这种混合使用策略帮助模型在保持处理速度的同时，也能有效地处理各种语言理解和生成任务。



# 参考

ChatGPT4

《ChatGPT原理与架构：大模型的预训练、迁移和中间件编程》程戈（←←这书不行，有不少错误）