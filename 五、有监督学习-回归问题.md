回归描述的是这样的是任务：目标特征是连续性的变量，我们希望从数据中得到它与其他变量的函数关系，由此关系对目标特征进行预测。针对这样的问题有一些常用的”函数关系“，就称之为回归模型了。

## 线性回归/最小二乘法

假设数据之间满足关系
$$
y_i=(1\ x_i')\beta=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2} +\cdots+ \beta_p x_{ip}+\varepsilon_i
$$
其中$\varepsilon_i$服从某个均值为零、方差恒定的正态分布。

现在有样本$x_1=(x_{11},x_{12},\cdots,x_{1p})',...,x_n=(x_{n1},x_{n2},\cdots,x_{np})'$和对应的$y_1,\cdots,y_n$（为了区别于上述的假设总体，样本数据应该带有上标，如$\hat{x}$，但为了方便此处略去），我们希望计算出系数$\beta$的估计量。

将样本数据记为
$$
X=\begin{pmatrix}
x_{1}' \\ x_{2}' \\ \vdots \\ x_{n}' 
\end{pmatrix}
=\begin{pmatrix}
x_{11} & x_{12} & \cdots &x_{1p} \\
x_{21} & x_{22} & \cdots &x_{2p} \\
\vdots & \vdots & \ddots &\vdots \\
x_{n1} & x_{n2} & \cdots &x_{np} \\
\end{pmatrix},
y=\begin{pmatrix}
y_{1} \\ y_{2} \\ \vdots \\ y_{n}
\end{pmatrix}
$$
可以证明，统计量
$$
\hat{\beta}=\begin{pmatrix}
\beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{n}
\end{pmatrix}= (X_1^TX_1)^{-1}X_1^Ty \ ,
X_1=\begin{pmatrix}
1 & 1\ \ \ \cdots & 1 \\
 & & &\\
 &\ X & & \\
 & & &\\
\end{pmatrix}
$$
是$\beta$的无偏估计（若使用$X$代替$X_1$，则不会有$\beta_0$一项），而且是一致最小方差无偏估计（也即是使得$E(\beta-\hat{\theta})^2$最小的$\hat{\theta}$）。于是我们就用$\hat{\beta}$代替$\beta$来进行预测。（也可以理解为，我们以$\|y-Xw\|_2^2$为目标函数，求解使得其最小的$w$，同样得到$\hat{\beta}$）



## 线性回归正则化

### 岭回归

其假设与最小二乘法完全相同。但我们不希望系数$\beta$​太大，所以将惩罚函数增加了一项$\lambda\|w\|_2^2$，也即用优化问题
$$
\min \limits_{w}\|y-Xw\|_2^2+\lambda\|w\|_2^2
$$

的解来作为$\hat{\beta}$。用数学方法可以解出$\hat\beta=(X^TX+\lambda I)^{-1}X^Ty$，现在它不再是无偏估计了。

虽然思路上的目标是减小系数，但是岭回归主要的好处体现在其他方面：

1. 当数据存在共线性时，$X^TX$会接近奇异，计算逆矩阵时会有很大误差。而岭回归由于$\lambda I$一项的存在，不会出现此问题。
1. 当数据存在共线性时，$X^TX$会接近奇异，因此系数受数据影响很大。通过$\hat\beta$在不同$\lambda$下的变化曲线（岭迹图），可以看出系数（分量）对特征的敏感度。波动较大的分量对应的特征更可能存在共线性，波动较小说明影响力稳定。也就是说，可以用岭回归来筛选变量。（相应的，为了消除数据尺度所带来的系数差异过大问题，岭回归之前通常需要归一化。）

参数$\lambda$是自己指定的，为了选择合适的$\lambda$，也通常先画岭迹图，选一个使得系数稳定的$\lambda$。

### Lasso

其假设与最小二乘法完全相同。但我们不希望系数$\beta$​太大，所以将惩罚函数增加了一项$\lambda\|w\|_1$，也即用优化问题
$$
\min \limits_{w}\|y-Xw\|_2^2+\lambda\|w\|_1
$$

的解来作为$\hat{\beta}$，它显然不是无偏估计。这个问题并没有解析解，只能用数值方法求解。

在思路上Lasso与岭回归相似，只是换了一个范数，但它比岭回归多了一些好处：

虽然思路上的目标是减小系数，但是岭回归主要的好处体现在其他方面：

1. 系数受$1-$范数限制，此时很多特征的系数会成为$0$，也即选入自变量的特征较少。
1. 同样可以画岭迹图。现在，随着$\lambda$的增大，各个特征的系数会逐渐变成零，而越晚变成零，说明该特征对目标变量的影响越强。同样可以用岭回归来筛选变量。（也同样需要归一化。）


解即为岭回归所得到的的$β$，而参数$\lambda$是自己指定的，为了选择合适的$\lambda$，也通常先画岭迹图，根据系数的稳定性或者变量个数来选择$\lambda$。

### 特征网络正则化

最小化目标函数变为

$$
\operatorname*{min}\limits_{\beta}\ \|y-\beta X\|_2^2+\lambda_1\|\beta\|_1+\lambda_2\|\beta\|_2^2
$$


## 非线性回归

### 样条回归

以线性样条、多项式样条、B样条等为基来线性组合。

### 径向基函数网络

先选定一个函数，它有参数a，以数据x与中心向量c之间的距离r=||x-c||为自变量，且关于中心向量径向对称，称为径向基函数。径向基函数网络得到的模型是k个选定的径向基函数的线性组合。求解时先选定k个中心向量c_i，再用最小二乘法求解系数。